57f8139449dd9286c2203ec2eca118a550638a7c

diff --git a/architecture.py b/architecture.py
index 4fc3793..b22af61 100644
--- a/architecture.py
+++ b/architecture.py
@@ -4,7 +4,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-from transformer import TransformerEncoderLayer
+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
 
 from absl import flags
 FLAGS = flags.FLAGS
@@ -52,7 +52,9 @@ class Model(nn.Module):
         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
 
         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
 
         self.has_aux_out = num_aux_outs is not None
@@ -76,7 +78,8 @@ class Model(nn.Module):
         x = x_raw
 
         x = x.transpose(0,1) # put time first
-        x = self.transformer(x)
+        x = self.transformerEncoder(x)
+        x = self.transformerDecoder(x) #TODO I need the target EMG
         x = x.transpose(0,1)
 
         if self.has_aux_out:
diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
index 571de9d..8563980 100644
--- a/models/recognition_model/log.txt
+++ b/models/recognition_model/log.txt
@@ -1,5 +1,2 @@
+57f8139449dd9286c2203ec2eca118a550638a7c
 
-
-['recognition_model.py', '--output_directory', './models/recognition_model/']
-output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-train / dev split: 8055 200
diff --git a/output/log.txt b/output/log.txt
index ae42364..1d2cd8e 100644
--- a/output/log.txt
+++ b/output/log.txt
@@ -1,3 +1,13 @@
+57f8139449dd9286c2203ec2eca118a550638a7c
 
+diff --git a/output/log.txt b/output/log.txt
+index ae42364..8563980 100644
+--- a/output/log.txt
++++ b/output/log.txt
+@@ -1,3 +1,2 @@
++57f8139449dd9286c2203ec2eca118a550638a7c
+ 
+-
+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
 
 ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
diff --git a/transformer.py b/transformer.py
index 6743588..ac131be 100644
--- a/transformer.py
+++ b/transformer.py
@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
         Shape:
             see the docs in Transformer class.
         """
-        src2 = self.self_attn(src)
+        src2 = self.self_attn(src, src, src)
         src = src + self.dropout1(src2)
         src = self.norm1(src)
         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
         src = self.norm2(src)
         return src
 
+class TransformerDecoderLayer(nn.Module):
+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
+    This standard decoder layer is based on the paper "Attention Is All You Need".
+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
+    in a different way during application.
+
+    Args:
+        d_model: the number of expected features in the input (required).
+        nhead: the number of heads in the multiheadattention models (required).
+        dim_feedforward: the dimension of the feedforward network model (default=2048).
+        dropout: the dropout value (default=0.1).
+        activation: the activation function of the intermediate layer, can be a string
+            ("relu" or "gelu") or a unary callable. Default: relu
+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
+        batch_first: If ``True``, then the input and output tensors are provided
+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
+            attention and feedforward operations, respectively. Otherwise it's done after.
+            Default: ``False`` (after).
+
+    Examples::
+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
+        >>> memory = torch.rand(10, 32, 512)
+        >>> tgt = torch.rand(20, 32, 512)
+        >>> out = decoder_layer(tgt, memory)
+    """
+    # Adapted from pytorch source
+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
+        super(TransformerDecoderLayer, self).__init__()
+        #Attention Mechanism
+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
+        # Implementation of Feedforward model
+        self.linear1 = nn.Linear(d_model, dim_feedforward)
+        self.dropout = nn.Dropout(dropout)
+        self.linear2 = nn.Linear(dim_feedforward, d_model)
+        #Normalization Layer and Dropout Layer
+        self.norm1 = nn.LayerNorm(d_model)
+        self.norm2 = nn.LayerNorm(d_model)
+        self.norm3 = nn.LayerNorm(d_model)
+        self.dropout1 = nn.Dropout(dropout)
+        self.dropout2 = nn.Dropout(dropout)
+        self.dropout3 = nn.Dropout(dropout)
+        #Activation Function
+        self.activation = nn.ReLU()
+    
+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+        r"""Pass the input through the encoder layer.
+
+        Args:
+            tgt: the sequence to the decoder layer (required).
+            memory: the sequence from the last layer of the encoder (required).
+            tgt_mask: the mask for the tgt sequence (optional).
+            memory_mask: the mask for the memory sequence (optional).
+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
+
+        Shape:
+            see the docs in Transformer class.
+        """
+        tgt2 = self.self_attn(tgt, tgt, tgt)
+        tgt = tgt + self.dropout1(tgt2)
+        tgt = self.norm1(tgt)
+
+        tgt2=self.multihead_attn(tgt, memory, memory)
+        tgt = tgt + self.dropout1(tgt2)
+        tgt = self.norm1(tgt)
+
+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
+        tgt = tgt + self.dropout2(tgt2)
+        tgt = self.norm2(tgt)
+        return tgt
+    
+
 class MultiHeadAttention(nn.Module):
   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
     super().__init__()
@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
     else:
         self.relative_positional = None
 
-  def forward(self, x):
+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
     """Runs the multi-head self-attention layer.
 
     Args:
@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
       A single tensor containing the output from this layer
     """
 
-    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
 
     if self.relative_positional is not None:

['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
train / dev split: 8055 200
