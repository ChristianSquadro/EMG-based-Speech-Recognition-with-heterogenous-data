b7bef3ea94b6a529a3383f118641f3bf5c2c6264

diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
index 1bfdd27..49ed015 100644
--- a/models/recognition_model/log.txt
+++ b/models/recognition_model/log.txt
@@ -1,2 +1,2 @@
-train / dev split: 8055 200
-"RUCASTLE'S"
+b7bef3ea94b6a529a3383f118641f3bf5c2c6264
+
diff --git a/speech_recognition/architecture.py b/speech_recognition/architecture.py
index 255d38a..4fc3793 100644
--- a/speech_recognition/architecture.py
+++ b/speech_recognition/architecture.py
@@ -4,15 +4,13 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 
-from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-from data_utils import decollate_tensor
+from transformer import TransformerEncoderLayer
+
 from absl import flags
 FLAGS = flags.FLAGS
 flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
-flags.DEFINE_integer('emg_features_size', 112, 'number of emg features')
 flags.DEFINE_integer('num_layers', 6, 'number of layers')
 flags.DEFINE_float('dropout', .2, 'dropout')
-flags.DEFINE_integer('pad', 0, 'Padding value according to the position on phoneme inventory')
 
 class ResBlock(nn.Module):
     def __init__(self, num_ins, num_outs, stride=1):
@@ -43,133 +41,46 @@ class ResBlock(nn.Module):
         return F.relu(x + res)
 
 class Model(nn.Module):
-    def __init__(self, num_features, num_outs_enc, num_outs_dec, device):
+    def __init__(self, num_features, num_outs, num_aux_outs=None):
         super().__init__()
 
-        '''
         self.conv_blocks = nn.Sequential(
             ResBlock(8, FLAGS.model_size, 2),
             ResBlock(FLAGS.model_size, FLAGS.model_size, 2),
             ResBlock(FLAGS.model_size, FLAGS.model_size, 2),
         )
         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-        '''
-        self.emg_projection = nn.Linear(FLAGS.emg_features_size, FLAGS.model_size)
-        
-        self.embedding_tgt = nn.Embedding(num_outs_dec + 2 , FLAGS.model_size, padding_idx=FLAGS.pad) # We need to take in consideration the embedding of <S> and <PAD> without predicting them
-        self.pos_decoder = PositionalEncoding(FLAGS.model_size)
 
         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-        self.w_out = nn.Linear(FLAGS.model_size, num_outs_dec)
-        self.w_aux = nn.Linear(FLAGS.model_size, num_outs_enc)
-        
-        self.device=device
-        
-        self.tgt_key_padding_mask=None
-        self.src_key_padding_mask=None
-        self.memory_key_padding_mask=None
-        self.tgt_mask=None
-
-    def create_tgt_padding_mask(self, tgt):
-        # input tgt of shape ()
-        tgt_padding_mask = tgt == FLAGS.pad
-        return tgt_padding_mask
-    
-    def create_src_padding_mask(self, src):
-        # input tgt of shape ()
-        src_padding_mask = src == FLAGS.pad
-        return src_padding_mask
-    
-    def forward(self, x_raw= None, y= None, mode = 'default', part = None, memory=None):
+        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
+        self.w_out = nn.Linear(FLAGS.model_size, num_outs)
+
+        self.has_aux_out = num_aux_outs is not None
+        if self.has_aux_out:
+            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
+
+    def forward(self, x_feat, x_raw, session_ids):
         # x shape is (batch, time, electrode)
-        # y shape is (batch, sequence_length)
-        if mode == "default":
-            return self.forward_training(x_raw=x_raw, y=y)
-        elif mode == "beam_search":
-            if part == 'encoder':
-                return self.forward_beam_search(part=part, x_raw=x_raw)
-            elif part == 'decoder':
-                return self.forward_beam_search(part=part, y=y, memory=memory)
-      
-    def forward_training (self, x_raw= None, y= None) :
-        '''
+
         if self.training:
             r = random.randrange(8)
             if r > 0:
-                x_raw_clone = x_raw.clone()
-                x_raw_clone[:,:-r,:] = x_raw[:,r:,:] # shift left r
-                x_raw_clone[:,-r:,:] = 0 #TODO should i change it with padding value???
-                x_raw = x_raw_clone
+                x_raw[:,:-r,:] = x_raw[:,r:,:] # shift left r
+                x_raw[:,-r:,:] = 0
 
         x_raw = x_raw.transpose(1,2) # put channel before time for conv
         x_raw = self.conv_blocks(x_raw)
         x_raw = x_raw.transpose(1,2)
         x_raw = self.w_raw_in(x_raw)
+
         x = x_raw
-        '''
-        
-        #Padding Target Mask and attention mask
-        self.tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-        self.src_key_padding_mask = self.create_src_padding_mask(x_raw[:,:,0]).to(self.device)
-        self.memory_key_padding_mask = self.src_key_padding_mask
-        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-
-        #Projection from emg input to the expected number of hidden dimension
-        x=self.emg_projection(x_raw)
-
-        #Embedding and positional encoding of tgt
-        tgt=self.embedding_tgt(y)
-        tgt=self.pos_decoder(tgt)
-        
-        x = x.transpose(0,1) # put time first
-        tgt = tgt.transpose(0,1) # put sequence_length first
-        x_encoder = self.transformerEncoder(x, src_key_padding_mask= self.src_key_padding_mask)
-        x_decoder = self.transformerDecoder(tgt, x_encoder, memory_key_padding_mask= self.memory_key_padding_mask, tgt_key_padding_mask=self.tgt_key_padding_mask, tgt_mask=self.tgt_mask)
 
-        x_encoder = x_encoder.transpose(0,1)
-        x_decoder = x_decoder.transpose(0,1)
+        x = x.transpose(0,1) # put time first
+        x = self.transformer(x)
+        x = x.transpose(0,1)
 
-        
-        return self.w_aux(x_encoder), self.w_out(x_decoder)
-        
-    def forward_beam_search(self, part , x_raw=None, y=None, memory=None):
-        # x shape is (batch, time, electrode)
-        # y shape is (batch, sequence_length)
-
-        if part == 'encoder':
-            if self.training:
-                r = random.randrange(8)
-                if r > 0:
-                    x_raw_clone = x_raw.clone()
-                    x_raw_clone[:,:-r,:] = x_raw[:,r:,:] # shift left r
-                    x_raw_clone[:,-r:,:] = 0
-                    x_raw = x_raw_clone
-
-            x_raw = x_raw.transpose(1,2) # put channel before time for conv
-            x_raw = self.conv_blocks(x_raw)
-            x_raw = x_raw.transpose(1,2)
-            x_raw = self.w_raw_in(x_raw)
-            x = x_raw
-
-            
-            x = x.transpose(0,1) # put time first
-            x_encoder = self.transformerEncoder(x)
-            x_encoder = x_encoder.transpose(0,1)
-            
-            return x_encoder
-            
-        elif part == 'decoder':
-            #Embedding and positional encoding of tgt
-            tgt=self.embedding_tgt(y)
-            tgt=self.pos_decoder(tgt)
-            
-            tgt = tgt.transpose(0,1) # put sequence_length first
-            memory = memory.transpose(0,1) # put sequence_length first
-            x_decoder = self.transformerDecoder(tgt, memory)
-            x_decoder = x_decoder.transpose(0,1)
-            
-            return self.w_out(x_decoder)
+        if self.has_aux_out:
+            return self.w_out(x), self.w_aux(x)
+        else:
+            return self.w_out(x)
 
diff --git a/speech_recognition/data_utils.py b/speech_recognition/data_utils.py
index f4da932..3157ae2 100644
--- a/speech_recognition/data_utils.py
+++ b/speech_recognition/data_utils.py
@@ -1,12 +1,10 @@
-import math
 import string
-import logging
 
 import numpy as np
 import librosa
 import soundfile as sf
+from textgrids import TextGrid
 import jiwer
-from num2words import num2words
 from unidecode import unidecode
 
 import torch
@@ -16,8 +14,7 @@ from absl import flags
 FLAGS = flags.FLAGS
 flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')
 
-phoneme_inventory = ['AA', 'AE', 'AH', 'AO','AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IX','IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH','</S>','<S>','<PAD>']
-pron_dct= { line.split()[0] : line.split()[1:] for line in open('descriptions/dgaddy-lexicon.txt') if line.split() != [] }
+phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']
 
 def normalize_volume(audio):
     rms = librosa.feature.rms(audio)
@@ -47,7 +44,7 @@ def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin,
 
     global mel_basis, hann_window
     if fmax not in mel_basis:
-        mel = librosa.filters.mel(sr=sampling_rate,n_fft=n_fft,n_mels=num_mels, fmin=fmin, fmax=fmax)
+        mel = librosa.filters.mel(sampling_rate, n_fft, num_mels, fmin, fmax)
         mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)
         hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)
 
@@ -85,6 +82,7 @@ def load_audio(filename, start=None, end=None, max_frames=None, renormalize_volu
         mspec = mspec[:max_frames,:]
     return mspec
 
+
 def double_average(x):
     assert len(x.shape) == 1
     f = np.ones(9)/9.0
@@ -163,7 +161,7 @@ def combine_fixed_length(tensor_list, length):
     if total_length % length != 0:
         pad_length = length - (total_length % length)
         tensor_list = list(tensor_list) # copy
-        tensor_list.append(torch.full((pad_length,*tensor_list[0].size()[1:]),FLAGS.pad, dtype=tensor_list[0].dtype, device=tensor_list[0].device))
+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
         total_length += pad_length
     tensor = torch.cat(tensor_list, 0)
     n = total_length // length
@@ -223,42 +221,30 @@ def print_confusion(confusion_mat, n=10):
         p2s = phoneme_inventory[p2]
         print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')
 
-def read_phonemes(sentence):
-    
-    #Premanipulation to avoid issues with num2words
-    pre_sentence=jiwer.SubstituteRegexes({r"_": r" ", r"£": r"pound "})(sentence)
-    
-    #Transform digits into words
-    digits=[]
-    new_sentence=""
-    for unit in pre_sentence:
-        if unit.isdigit():
-            digits.append(unit)
-        elif digits:
-            new_sentence += num2words(int(''.join(digits))) + ' ' + unit
-            digits=[]  
-        else:
-            new_sentence += unit     
-            
-    #String manipulation before being proccesed by the dictionary     
-    new_sentence=jiwer.Compose([jiwer.SubstituteRegexes({r"—": r" ", r"-": r" ",r"’s": r"'s",r"'seventy": r"seventy",r"[.!?,\“\”;:‘’\[\]\(\)\/]": r""}), jiwer.ToUpperCase()])(new_sentence).split()
-    
-    #Transform the words into sequences of phones
-    phones = []
-    for n in new_sentence:
-        try:
-            p = pron_dct[n]
-            phones.append(p)
-        except KeyError as e:
-            #logging.warning('Dictionary error for the word %s in the phrase: %s', e, sentence)
-            logging.warning(e)
-    return [phone for word_phone in phones for phone in word_phone] + ['</S>'] #The model should learn the end token but the start token is manually injected during beam search
+def read_phonemes(textgrid_fname, max_len=None):
+    tg = TextGrid(textgrid_fname)
+    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)
+    phone_ids[:] = -1
+    phone_ids[-1] = phoneme_inventory.index('sil') # make sure list is long enough to cover full length of original sequence
+    for interval in tg['phones']:
+        phone = interval.text.lower()
+        if phone in ['', 'sp', 'spn']:
+            phone = 'sil'
+        if phone[-1] in string.digits:
+            phone = phone[:-1]
+        ph_id = phoneme_inventory.index(phone)
+        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id
+    assert (phone_ids >= 0).all(), 'missing aligned phones'
+
+    if max_len is not None:
+        phone_ids = phone_ids[:max_len]
+        assert phone_ids.shape[0] == max_len
+    return phone_ids
 
 class TextTransform(object):
     def __init__(self):
         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-        self.chars = "*" + string.ascii_lowercase+string.digits+ ' ' #TODO remove *
-        self.vocabulary_size=len(self.chars)
+        self.chars = string.ascii_lowercase+string.digits+' '
 
     def clean_text(self, text):
         text = unidecode(text)
@@ -271,16 +257,3 @@ class TextTransform(object):
 
     def int_to_text(self, ints):
         return ''.join(self.chars[i] for i in ints)
-    
-class PhoneTransform(object):
-    def __init__(self):
-        self.phoneme_inventory = phoneme_inventory
-        self.vocabulary_size=len(self.phoneme_inventory)
-
-    def phone_to_int(self, phone):
-        phone= phone
-        return [self.phoneme_inventory.index(c) for c in phone]
-
-    def int_to_phone(self, ints):
-        return ''.join(self.phoneme_inventory[i] for i in ints)
-    
diff --git a/speech_recognition/read_emg.py b/speech_recognition/read_emg.py
index 7ff309c..664aa94 100644
--- a/speech_recognition/read_emg.py
+++ b/speech_recognition/read_emg.py
@@ -1,8 +1,9 @@
 import re
 import os
 import numpy as np
-from typing import List
+import matplotlib.pyplot as plt
 import random
+from collections import defaultdict
 import scipy
 import json
 import copy
@@ -10,7 +11,6 @@ import sys
 import pickle
 import string
 import logging
-from scipy.stats import lognorm
 from functools import lru_cache
 from copy import copy
 
@@ -19,7 +19,7 @@ import soundfile as sf
 
 import torch
 
-from data_utils import load_audio, get_emg_features, FeatureNormalizer, phoneme_inventory, read_phonemes, TextTransform, PhoneTransform
+from data_utils import load_audio, get_emg_features, FeatureNormalizer, phoneme_inventory, read_phonemes, TextTransform
 
 from absl import flags
 FLAGS = flags.FLAGS
@@ -54,7 +54,7 @@ def apply_to_all(function, signal_array, *args, **kwargs):
         results.append(function(signal_array[:,i], *args, **kwargs))
     return np.stack(results, 1)
 
-def load_utterance(base_dir, index, limit_length=False, debug=False):
+def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):
     index = int(index)
     raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))
     before = os.path.join(base_dir, f'{index-1}_emg.npy')
@@ -72,8 +72,8 @@ def load_utterance(base_dir, index, limit_length=False, debug=False):
     x = apply_to_all(notch_harmonics, x, 60, 1000)
     x = apply_to_all(remove_drift, x, 1000)
     x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]
-    emg_orig = apply_to_all(subsample, x, 600.00, 1000)
-    x = apply_to_all(subsample, x, 450.00, 1000)
+    emg_orig = apply_to_all(subsample, x, 689.06, 1000)
+    x = apply_to_all(subsample, x, 516.79, 1000)
     emg = x
 
     for c in FLAGS.remove_channels:
@@ -96,9 +96,12 @@ def load_utterance(base_dir, index, limit_length=False, debug=False):
         info = json.load(f)
 
     sess = os.path.basename(base_dir)
-    
-    phonemes=read_phonemes(info['text'])
-    
+    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'
+    if os.path.exists(tg_fname):
+        phonemes = read_phonemes(tg_fname, mfccs.shape[0])
+    else:
+        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')
+
     return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)
 
 class EMGDirectory(object):
@@ -141,207 +144,6 @@ class SizeAwareSampler(torch.utils.data.Sampler):
             batch_length += length
         # dropping last incomplete batch
 
-class DynamicBatchSampler(torch.utils.data.Sampler):
-    def __init__(
-        self,
-        dataset,
-        max_batch_length: int,
-        num_buckets: int = None,
-        shuffle: bool = True,
-        batch_ordering: str = "random",
-        max_batch_ex: int = None,
-        bucket_boundaries: List[int] = [],
-        seed: int = 42,
-        epoch: int = 0,
-        drop_last: bool = False,
-        verbose: bool = False,
-    ):
-        self._dataset = dataset
-        self._ex_lengths = {}
-        self.verbose = verbose
-        self.lengths_list=[]
-        
-        indices = list(range(len(dataset)))
-        for idx in indices:
-            directory_info, file_idx = dataset.example_indices[idx]
-            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:
-                info = json.load(f)
-            self.lengths_list.append(sum([emg_len for emg_len, _, _ in info['chunks']]))
-
-        if self.lengths_list is not None:
-            # take length of examples from this argument and bypass length_key
-            for indx in range(len(self.lengths_list)):
-                self._ex_lengths[str(indx)] = self.lengths_list[indx]
-
-        if len(bucket_boundaries) > 0:
-            if not all([x >= 0 for x in bucket_boundaries]):
-                raise ValueError(
-                    "All elements in bucket boundaries should be non-negative (>= 0)."
-                )
-            if not len(set(bucket_boundaries)) == len(bucket_boundaries):
-                raise ValueError(
-                    "Bucket_boundaries should not contain duplicates."
-                )
-            np.testing.assert_array_equal(
-                np.array(bucket_boundaries),
-                np.array(sorted(bucket_boundaries)),
-                err_msg="The arg bucket_boundaries should be an ascending sorted list of non negative values values!",
-            )
-            self._bucket_boundaries = np.array(sorted(bucket_boundaries))
-        else:
-            # use num_buckets
-            self._bucket_boundaries = np.array(
-                self._get_boundaries_through_warping(
-                    max_batch_length=max_batch_length,
-                    num_quantiles=num_buckets,
-                )
-            )
-
-        self._max_batch_length = max_batch_length
-        self._shuffle_ex = shuffle
-        self._batch_ordering = batch_ordering
-        self._seed = seed
-        self._drop_last = drop_last
-        if max_batch_ex is None:
-            max_batch_ex = np.inf
-        self._max_batch_ex = max_batch_ex
-        # Calculate bucket lengths - how often does one bucket boundary fit into max_batch_length?
-        self._bucket_lens = [
-            max(1, int(max_batch_length / self._bucket_boundaries[i]))
-            for i in range(len(self._bucket_boundaries))
-        ] + [1]
-        self._epoch = epoch
-        self._generate_batches()
-
-    def get_durations(self, batch):
-        """Gets durations of the elements in the batch."""
-        return [self._ex_lengths[str(idx)] for idx in batch]
-
-    def _get_boundaries_through_warping(
-        self, max_batch_length: int, num_quantiles: int,
-    ) -> List[int]:
-
-        # NOTE: the following lines do not cover that there is only one example in the dataset
-        # warp frames (duration) distribution of train data
-        # linspace set-up
-        num_boundaries = num_quantiles + 1
-        # create latent linearly equal spaced buckets
-        latent_boundaries = np.linspace(
-            1 / num_boundaries, num_quantiles / num_boundaries, num_quantiles,
-        )
-        # get quantiles using lognormal distribution
-        quantiles = lognorm.ppf(latent_boundaries, 1)
-        # scale up to to max_batch_length
-        bucket_boundaries = quantiles * max_batch_length / quantiles[-1]
-        
-        return list(sorted(bucket_boundaries))
-
-    def _permute_batches(self):
-
-        if self._batch_ordering == "random":
-            # deterministically shuffle based on epoch and seed
-            g = torch.Generator()
-            g.manual_seed(self._seed + self._epoch)
-            sampler = torch.randperm(
-                len(self._batches), generator=g
-            ).tolist()  # type: ignore
-            tmp = []
-            for idx in sampler:
-                tmp.append(self._batches[idx])
-            self._batches = tmp
-
-        elif self._batch_ordering == "ascending":
-            self._batches = sorted(
-                self._batches,
-                key=lambda x: max([self._ex_lengths[str(idx)] for idx in x]),
-            )
-        elif self._batch_ordering == "descending":
-            self._batches = sorted(
-                self._batches,
-                key=lambda x: max([self._ex_lengths[str(idx)] for idx in x]),
-                reverse=True,
-            )
-        else:
-            raise NotImplementedError
-
-    def _generate_batches(self):
-        if self._shuffle_ex:
-            # deterministically shuffle based on epoch and seed
-            g = torch.Generator()
-            g.manual_seed(self._seed + self._epoch)
-            sampler = torch.randperm(len(self._dataset), generator=g).tolist()  # type: ignore
-        else:
-            # take examples as they are: e.g. they have been sorted
-            sampler = range(len(self._dataset))  # type: ignore
-
-        self._batches = []
-        bucket_batches = [[] for i in self._bucket_lens]
-
-        stats_tracker = [
-            {"min": np.inf, "max": -np.inf, "tot": 0, "n_ex": 0}
-            for i in self._bucket_lens
-        ]
-
-        for idx in sampler:
-            directory_info, file_idx = self._dataset.example_indices[idx]
-            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:
-                info = json.load(f)
-            if not np.any([l in string.ascii_letters for l in info['text']]):
-                continue
-            # length of pre-sampled audio
-            item_len = self._ex_lengths[str(idx)]
-            # bucket to fill up most padding
-            bucket_id = np.searchsorted(self._bucket_boundaries, item_len)
-            # fill audio's duration into that bucket
-            bucket_batches[bucket_id].append(idx)
-
-            stats_tracker[bucket_id]["min"] = min(
-                stats_tracker[bucket_id]["min"], item_len
-            )
-            stats_tracker[bucket_id]["max"] = max(
-                stats_tracker[bucket_id]["max"], item_len
-            )
-            stats_tracker[bucket_id]["tot"] += item_len
-            stats_tracker[bucket_id]["n_ex"] += 1
-            # track #samples - why not duration/#frames; rounded up?
-            # keep track of durations, if necessary
-
-            if (
-                len(bucket_batches[bucket_id]) >= self._bucket_lens[bucket_id]
-                or len(bucket_batches[bucket_id]) >= self._max_batch_ex
-            ):
-                self._batches.append(bucket_batches[bucket_id])
-                bucket_batches[bucket_id] = []
-                # keep track of durations
-
-        # Dump remaining batches
-        if not self._drop_last:
-            for batch in bucket_batches:
-                if batch:
-                    self._batches.append(batch)
-
-        self._permute_batches()  # possibly reorder batches
-
-    def __iter__(self):
-        for batch in self._batches:
-            yield batch
-        if self._shuffle_ex:  # re-generate examples if ex_ordering == "random"
-            self._generate_batches()
-        if self._batch_ordering == "random":
-            # we randomly permute the batches only --> faster
-            self._permute_batches()
-
-    def set_epoch(self, epoch):
-        """
-        You can also just access self.epoch, but we maintain this interface
-        to mirror torch.utils.data.distributed.DistributedSampler
-        """
-        self._epoch = epoch
-        self._generate_batches()
-
-    def __len__(self):
-        return len(self._batches)
-
 class EMGDataset(torch.utils.data.Dataset):
     def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):
 
@@ -405,7 +207,6 @@ class EMGDataset(torch.utils.data.Dataset):
         self.num_sessions = len(directories)
 
         self.text_transform = TextTransform()
-        self.phone_transform = PhoneTransform()
 
     def silent_subset(self):
         result = copy(self)
@@ -427,7 +228,7 @@ class EMGDataset(torch.utils.data.Dataset):
     @lru_cache(maxsize=None)
     def __getitem__(self, i):
         directory_info, idx = self.example_indices[i]
-        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length)
+        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)
         raw_emg = raw_emg / 20
         raw_emg = 50*np.tanh(raw_emg/50.)
 
@@ -439,16 +240,13 @@ class EMGDataset(torch.utils.data.Dataset):
         session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)
         audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'
 
-        #self.text_transform.add_new_words(text)
-        words= [word for word in text]
         text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)
 
-
-        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'words': words,'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}
+        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}
 
         if directory_info.silent:
             voiced_directory, voiced_idx = self.voiced_data_locations[book_location]
-            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False)
+            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)
 
             if not self.no_normalizers:
                 voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)
@@ -459,8 +257,7 @@ class EMGDataset(torch.utils.data.Dataset):
             result['parallel_voiced_emg'] = torch.from_numpy(voiced_emg).pin_memory()
 
             audio_file = f'{voiced_directory.directory}/{voiced_idx}_audio_clean.flac'
-        
-        phonemes=np.array(self.phone_transform.phone_to_int(phonemes), dtype=np.int64)
+
         result['phonemes'] = torch.from_numpy(phonemes).pin_memory() # either from this example if vocalized or aligned example if silent
         result['audio_file'] = audio_file
 
@@ -482,7 +279,6 @@ class EMGDataset(torch.utils.data.Dataset):
                 audio_feature_lengths.append(ex['audio_features'].shape[0])
                 parallel_emg.append(np.zeros(1))
         phonemes = [ex['phonemes'] for ex in batch]
-        phonemes_lengths = [ex['phonemes'].shape[0] for ex in batch]
         emg = [ex['emg'] for ex in batch]
         raw_emg = [ex['raw_emg'] for ex in batch]
         session_ids = [ex['session_ids'] for ex in batch]
@@ -497,7 +293,6 @@ class EMGDataset(torch.utils.data.Dataset):
                   'raw_emg':raw_emg,
                   'parallel_voiced_emg':parallel_emg,
                   'phonemes':phonemes,
-                  'phonemes_lengths': phonemes_lengths,
                   'session_ids':session_ids,
                   'lengths':lengths,
                   'silent':silent,
diff --git a/speech_recognition/recognition_model.py b/speech_recognition/recognition_model.py
index 7a0f4ea..b914784 100644
--- a/speech_recognition/recognition_model.py
+++ b/speech_recognition/recognition_model.py
@@ -1,19 +1,19 @@
-import datetime
 import os
 import sys
 import numpy as np
 import logging
+import subprocess
 import jiwer
-import PrefixTree
-from torch.utils.tensorboard import SummaryWriter
+import random
 
 import torch
 from torch import nn
 import torch.nn.functional as F
 
-from read_emg import EMGDataset, DynamicBatchSampler
+from read_emg import EMGDataset, SizeAwareSampler
 from architecture import Model
-from BeamSearch import run_single_bs
+from data_utils import combine_fixed_length, decollate_tensor
+from transformer import TransformerEncoderLayer
 
 from absl import flags
 FLAGS = flags.FLAGS
@@ -24,63 +24,27 @@ flags.DEFINE_float('learning_rate', 3e-4, 'learning rate')
 flags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')
 flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')
 flags.DEFINE_string('start_training_from', None, 'start training from this model')
-flags.DEFINE_float('l2', 0., 'weight decay')
-flags.DEFINE_float('alpha_loss', 0.3, 'parameter alpha for the two losses')
-flags.DEFINE_float('report_every', 10, "Reporting parameter of the loss plot")
+flags.DEFINE_float('l2', 0, 'weight decay')
 flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')
-flags.DEFINE_string('phonesSet', "descriptions/phonesSet", 'the set of all phones in the lexicon')
-flags.DEFINE_string('vocabulary', "descriptions/vocabulary", 'the set of all words in the lexicon')
-flags.DEFINE_string('dict', "descriptions/dgaddy-lexicon.txt", 'the pronunciation dictionary')
-flags.DEFINE_string('lang_model', "descriptions/lm.binary", 'the language model')
-
-def test(model, testset, device, tree, language_model):
-    model.eval()
-    dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
-    n_phones = len(testset.phone_transform.phoneme_inventory)
-    references = []
-    predictions = []
-    batch_idx = 0
-     
-    with torch.no_grad():
-        for example in dataloader:
-            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True, padding_value= FLAGS.pad).to(device)
-            tgt = nn.utils.rnn.pad_sequence(example['phonemes'], batch_first=True, padding_value= FLAGS.pad).to(device)
-
-            pred=run_single_bs(model,X_raw,tgt,n_phones,tree,language_model,device)
- 
-            pred_text = testset.text_transform.clean_text(' '.join(pred[2]))
-            target_text = testset.text_transform.clean_text(example['text'][0])
-            references.append(target_text)
-            predictions.append(pred_text)
-
-        batch_idx += 1
-        
-    model.train()
-    #remove empty strings because I had an error in the calculation of WER function
-    #predictions = [predictions[i] for i in range(len(predictions)) if len(references[i]) > 0]
-    #references = [references[i] for i in range(len(references)) if len(references[i]) > 0]
-    return jiwer.wer(references, predictions)
-
-def train_model(trainset, devset, device, writer, tree, language_model, n_epochs=200, report_every=5):
-    #Define Dataloader
-    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0,collate_fn=EMGDataset.collate_raw, batch_sampler= DynamicBatchSampler(trainset, 10000, 64, shuffle=True, batch_ordering='random'))
-    dataloader_evaluation = torch.utils.data.DataLoader(devset, pin_memory=(device=='cuda'), num_workers=0,collate_fn=EMGDataset.collate_raw, batch_sampler= DynamicBatchSampler(devset, 128000, 64, shuffle=True, batch_ordering='random'))
-
-    #Define model and loss function
-    n_phones = len(devset.phone_transform.phoneme_inventory) - 2 #we should remove from prediction the <S> and <PAD>
-    model = Model(devset.num_features, n_phones + 1, n_phones, device) #plus 1 for the blank symbol of CTC loss in the encoder
-    model=nn.DataParallel(model, device_ids=[0,1]).to(device)
-    loss_fn=nn.CrossEntropyLoss(ignore_index=FLAGS.pad)
+
+def test(model, testset, device):
+   return
+
+
+def train_model(trainset, devset, device, n_epochs=200):
+    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
+
+
+    n_chars = len(devset.text_transform.chars)
+    model = Model(devset.num_features, n_chars+1).to(device)
 
     if FLAGS.start_training_from is not None:
         state_dict = torch.load(FLAGS.start_training_from)
         model.load_state_dict(state_dict, strict=False)
 
-    #Define optimizer and scheduler for the learning rate
     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
 
-
     def set_lr(new_lr):
         for param_group in optim.param_groups:
             param_group['lr'] = new_lr
@@ -92,108 +56,34 @@ def train_model(trainset, devset, device, writer, tree, language_model, n_epochs
             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
 
     batch_idx = 0
-    train_loss= 0
-    eval_loss = 0
-    run_steps=0
     optim.zero_grad()
     for epoch_idx in range(n_epochs):
         losses = []
-        for example in dataloader_training:
-            #Model train mode enabled and schedule_lr to change learning rate during the warmup phase
-            model.train()
+        for example in dataloader:
             schedule_lr(batch_idx)
-            
-            #Preprosessing of the input and target for the model
-            X_raw=nn.utils.rnn.pad_sequence(example['emg'], batch_first=True, padding_value= FLAGS.pad).to(device)
-            y = nn.utils.rnn.pad_sequence(example['phonemes'], batch_first=True,  padding_value= FLAGS.pad).to(device)
-
-            #Shifting target for input decoder and loss
-            tgt= y[:,:-1]
-            target= y[:,1:]
-
-            #Prediction
-            out_enc, out_dec = model(x_raw=X_raw, y=tgt)
-
-            #Decoder Loss
-            out_dec=out_dec.permute(0,2,1)
-            loss_dec = loss_fn(out_dec, target)
-
-            #Encoder Loss
-            out_enc = F.log_softmax(out_enc, 2)
-            out_enc = out_enc.transpose(1,0)
-            y = nn.utils.rnn.pad_sequence(example['phonemes'], batch_first=True).to(device)
-            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['phonemes_lengths'], blank = len(devset.phone_transform.phoneme_inventory)-2) 
-
-            #Combination the two losses
-            loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
+
+            X = combine_fixed_length(example['emg'], 200).to(device)
+            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
+            sess = combine_fixed_length(example['session_ids'], 200).to(device)
+
+            pred = model(X, X_raw, sess)
+            pred = F.log_softmax(pred, 2)
+
+            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
+            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
             losses.append(loss.item())
-            train_loss += loss.item()
 
-            #Gradient Update
             loss.backward()
             if (batch_idx+1) % 2 == 0:
                 optim.step()
                 optim.zero_grad()
 
-
-            #Increment counter batch and counter for steps of losses   
             batch_idx += 1
-            run_steps += 1
-
-            if batch_idx % report_every == 0:     
-                #Evaluation
-                model.eval()
-                
-                #Print training loss
-                writer.add_scalar('Loss/Training', round(train_loss / run_steps,3), batch_idx)
-                writer.flush()
-                train_loss= 0
-                run_steps = 0
-                
-                with torch.no_grad():
-                    for idx, example in enumerate(dataloader_evaluation):
-                        #Collect the data
-                        X_raw=nn.utils.rnn.pad_sequence(example['emg'], batch_first=True,  padding_value= FLAGS.pad).to(device)
-                        y = nn.utils.rnn.pad_sequence(example['phonemes'], batch_first=True, padding_value=FLAGS.pad).to(device)
-                    
-                        #Shifting target for input decoder and loss
-                        tgt= y[:,:-1]
-                        target= y[:,1:]
-                        
-                        #Forward Model
-                        out_enc, out_dec = model(x_raw=X_raw, y=tgt)
-
-                        #Decoder Loss
-                        out_dec=out_dec.permute(0,2,1)
-                        loss = loss_fn(out_dec, target)
-                        loss_dec = loss_fn(out_dec, target)
-
-                        #Encoder Loss
-                        out_enc = F.log_softmax(out_enc, 2)
-                        out_enc = out_enc.transpose(1,0)
-                        loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['phonemes_lengths'], blank = len(devset.phone_transform.phoneme_inventory)-2) 
-
-                        #Combination the two losses
-                        loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
-                        eval_loss += loss.item()
-                        run_steps += 1
-                        
-                        #just to block processing all validation batches
-                        if idx == 3:
-                            break
-                            
-                #Writing on tensorboard
-                writer.add_scalar('Loss/Evaluation', round (eval_loss / run_steps, 3), batch_idx)
-                writer.flush()
-                eval_loss= 0
-                run_steps=0
-                
-        #Change learning rate
-        lr_sched.step()
-    
-        #Logging
         train_loss = np.mean(losses)
-        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f}')
+        val = test(model, devset, device)
+        lr_sched.step()
+        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
 
     model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters
@@ -214,22 +104,19 @@ def main():
             logging.StreamHandler()
             ], level=logging.INFO, format="%(message)s")
 
-  #  logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
-  #  logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
-  #  logging.info(sys.argv)
+    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
+    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
+
+    logging.info(sys.argv)
 
-    tree = PrefixTree.init_tree(FLAGS.phonesSet,FLAGS.vocabulary,FLAGS.dict)
-    language_model = PrefixTree.init_language_model(FLAGS.lang_model)
     trainset = EMGDataset(dev=False,test=False)
     devset = EMGDataset(dev=True)
-  #  logging.info('output example: %s', devset.example_indices[0])
+    logging.info('output example: %s', devset.example_indices[0])
     logging.info('train / dev split: %d %d',len(trainset),len(devset))
 
     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-    log_dir="logs/run/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
-    writer = SummaryWriter(log_dir=log_dir)
 
-    model = train_model(trainset, devset ,device, writer, tree, language_model)
+    model = train_model(trainset, devset, device)
 
 if __name__ == '__main__':
     FLAGS(sys.argv)
diff --git a/speech_recognition/transformer.py b/speech_recognition/transformer.py
index 817ccaf..6743588 100644
--- a/speech_recognition/transformer.py
+++ b/speech_recognition/transformer.py
@@ -1,4 +1,3 @@
-import math
 from typing import Optional
 
 import torch
@@ -52,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
         Shape:
             see the docs in Transformer class.
         """
-        src2 = self.self_attn(src, src, src, src_key_padding_mask=src_key_padding_mask)
+        src2 = self.self_attn(src)
         src = src + self.dropout1(src2)
         src = self.norm1(src)
         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
@@ -60,77 +59,6 @@ class TransformerEncoderLayer(nn.Module):
         src = self.norm2(src)
         return src
 
-class TransformerDecoderLayer(nn.Module):
-    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-    This standard decoder layer is based on the paper "Attention Is All You Need".
-    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-    in a different way during application.
-
-    Args:
-        d_model: the number of expected features in the input (required).
-        nhead: the number of heads in the multiheadattention models (required).
-        dim_feedforward: the dimension of the feedforward network model (default=2048).
-        dropout: the dropout value (default=0.1).
-        activation: the activation function of the intermediate layer, can be a string
-            ("relu" or "gelu") or a unary callable. Default: relu
-        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-        batch_first: If ``True``, then the input and output tensors are provided
-            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-            attention and feedforward operations, respectively. Otherwise it's done after.
-            Default: ``False`` (after).
-        """
-    # Adapted from pytorch source
-    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-        super(TransformerDecoderLayer, self).__init__()
-        #Attention Mechanism
-        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-        # Implementation of Feedforward model
-        self.linear1 = nn.Linear(d_model, dim_feedforward)
-        self.dropout = nn.Dropout(dropout)
-        self.linear2 = nn.Linear(dim_feedforward, d_model)
-        #Normalization Layer and Dropout Layer
-        self.norm1 = nn.LayerNorm(d_model)
-        self.norm2 = nn.LayerNorm(d_model)
-        self.norm3 = nn.LayerNorm(d_model)
-        self.dropout1 = nn.Dropout(dropout)
-        self.dropout2 = nn.Dropout(dropout)
-        self.dropout3 = nn.Dropout(dropout)
-        #Activation Function
-        self.activation = nn.ReLU()
-    
-    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-        r"""Pass the input through the encoder layer.
-
-        Args:
-            tgt: the sequence to the decoder layer (required).
-            memory: the sequence from the last layer of the encoder (required).
-            tgt_mask: the mask for the tgt sequence (optional).
-            memory_mask: the mask for the memory sequence (optional).
-            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-
-        Shape:
-            see the docs in Transformer class.
-        """
-        tgt2 = self.self_attn(tgt, tgt, tgt,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
-        tgt = tgt + self.dropout1(tgt2)
-        tgt = self.norm1(tgt)
-
-        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask=memory_key_padding_mask)
-        tgt = tgt + self.dropout1(tgt2)
-        tgt = self.norm1(tgt)
-
-        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-        tgt = tgt + self.dropout2(tgt2)
-        tgt = self.norm2(tgt)
-        return tgt
-    
-
 class MultiHeadAttention(nn.Module):
   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
     super().__init__()
@@ -156,7 +84,7 @@ class MultiHeadAttention(nn.Module):
     else:
         self.relative_positional = None
 
-  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, tgt_key_padding_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None):
+  def forward(self, x):
     """Runs the multi-head self-attention layer.
 
     Args:
@@ -165,32 +93,11 @@ class MultiHeadAttention(nn.Module):
       A single tensor containing the output from this layer
     """
 
-    #Computes projections
-    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-     
-    # Compute scaled dot-product attention
+    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
+    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
+    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
 
-    # Apply att_mask to the attention weights if provided
-    if tgt_mask is not None:
-        logits = logits.masked_fill(tgt_mask == float('-inf'), -1e8)
-    
-    
-    #Apply padding_mask to the attention weights if provided
-    if tgt_key_padding_mask is not None:
-       logits = logits.masked_fill(tgt_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
-
-    #Apply padding_mask to the attention weights if provided
-    if src_key_padding_mask is not None:
-       logits = logits.masked_fill(src_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
-
-    #Apply padding_mask to the attention weights if provided
-    if memory_key_padding_mask is not None:
-       logits = logits.masked_fill(memory_key_padding_mask.unsqueeze(1).unsqueeze(2), -1e8)
-    
-    
     if self.relative_positional is not None:
         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
         l,b,h,d = q_pos.size()
@@ -396,39 +303,3 @@ class LearnedRelativePositionalEmbedding(nn.Module):
             x = x.transpose(0, 1)
             x = x.contiguous().view(bsz_heads, length+1, length)
             return x[:, 1:, :]
-        
-
-########
-# Taken from:
-# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-# or also here:
-# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-class PositionalEncoding(nn.Module):
-
-    def __init__(self, d_model, dropout=0.0, max_len=5000):
-        super(PositionalEncoding, self).__init__()
-        self.dropout = nn.Dropout(p=dropout)
-        self.max_len = max_len
-
-        pe = torch.zeros(max_len, d_model)
-        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-                             * (-math.log(10000.0) / d_model))
-        pe[:, 0::2] = torch.sin(position * div_term)
-        pe[:, 1::2] = torch.cos(position * div_term)
-        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-        self.register_buffer('pe', pe)  # Will not be trained.
-
-    def forward(self, x):
-        """Inputs of forward function
-        Args:
-            x: the sequence fed to the positional encoder model (required).
-        Shape:
-            x: [sequence length, batch size, embed dim]
-            output: [sequence length, batch size, embed dim]
-        """
-        assert x.size(0) < self.max_len, (
-            f"Too long sequence length: increase `max_len` of pos encoding")
-        # shape of x (len, B, dim)
-        x = x + self.pe[:x.size(0), :]
-        return self.dropout(x)

['/home/christian/storage/EMG-christian/silent_speech-main/speech_recognition/recognition_model.py', '--output_directory', './models/recognition_model/']
