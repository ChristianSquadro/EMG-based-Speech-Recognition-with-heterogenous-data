2fa943cd85263a152b6be80d502eda27932ebb27

diff --git a/architecture.py b/architecture.py
index a8c70f3..2413a8a 100644
--- a/architecture.py
+++ b/architecture.py
@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
         return F.relu(x + res)
 
 class Model(nn.Module):
-    def __init__(self, num_features, num_outs, num_aux_outs=None):
+    def __init__(self, num_features, num_outs, has_aux_loss=False):
         super().__init__()
 
         self.conv_blocks = nn.Sequential(
@@ -59,9 +59,7 @@ class Model(nn.Module):
         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
 
-        self.has_aux_out = num_aux_outs is not None
-        if self.has_aux_out:
-            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
+        self.has_aux_loss = has_aux_loss
 
     def forward(self, x_feat, x_raw, y,session_ids):
         # x shape is (batch, time, electrode)
@@ -82,12 +80,14 @@ class Model(nn.Module):
 
         x = x.transpose(0,1) # put time first
         tgt = tgt.transpose(0,1) # put channel after
-        x = self.transformerEncoder(x)
-        x = self.transformerDecoder(tgt, x)
-        x = x.transpose(0,1)
+        x_encoder = self.transformerEncoder(x)
+        x_decoder = self.transformerDecoder(tgt, x_encoder)
 
-        if self.has_aux_out:
-            return self.w_out(x), self.w_aux(x)
+        x_encoder = x_encoder.transpose(0,1)
+        x_decoder = x_decoder.transpose(0,1)
+
+        if self.has_aux_loss:
+            return self.w_out(x_encoder), self.w_out(x_decoder)
         else:
             return self.w_out(x)
 
diff --git a/data_utils.py b/data_utils.py
index e2632e8..8b05213 100644
--- a/data_utils.py
+++ b/data_utils.py
@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
 
 def combine_fixed_length_tgt(tensor_list, n_batch):
     total_length = sum(t.size(0) for t in tensor_list)
+    tensor_list = list(tensor_list) # copy
     if total_length % n_batch != 0:
         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-        tensor_list = list(tensor_list) # copy
         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
         total_length += pad_length
     tensor = torch.cat(tensor_list, 0)
diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
index b8f7791..617dd85 100644
--- a/models/recognition_model/log.txt
+++ b/models/recognition_model/log.txt
@@ -1,480 +1,2 @@
-902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
+2fa943cd85263a152b6be80d502eda27932ebb27
 
-diff --git a/architecture.py b/architecture.py
-index d6e99b4..a8c70f3 100644
---- a/architecture.py
-+++ b/architecture.py
-@@ -54,7 +54,7 @@ class Model(nn.Module):
-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
- 
-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-diff --git a/data_utils.py b/data_utils.py
-index e4ac852..e2632e8 100644
---- a/data_utils.py
-+++ b/data_utils.py
-@@ -1,3 +1,4 @@
-+import math
- import string
- 
- import numpy as np
-@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-     n = total_length // length
-     return tensor.view(n, length, *tensor.size()[1:])
- 
-+def combine_fixed_length_tgt(tensor_list, n_batch):
-+    total_length = sum(t.size(0) for t in tensor_list)
-+    if total_length % n_batch != 0:
-+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-+        tensor_list = list(tensor_list) # copy
-+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-+        total_length += pad_length
-+    tensor = torch.cat(tensor_list, 0)
-+    length = total_length // n_batch
-+    return tensor.view(n_batch, length, *tensor.size()[1:])
-+
- def decollate_tensor(tensor, lengths):
-     b, s, d = tensor.size()
-     tensor = tensor.view(b*s, d)
-diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-index e890f0f..1ee3421 100644
---- a/models/recognition_model/log.txt
-+++ b/models/recognition_model/log.txt
-@@ -1,265 +1,2 @@
--031b80598b18e602b7f2b8d237d6b2f8d1246c05
-+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
- 
--diff --git a/architecture.py b/architecture.py
--index b22af61..d6e99b4 100644
----- a/architecture.py
--+++ b/architecture.py
--@@ -51,6 +51,8 @@ class Model(nn.Module):
--         )
--         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-- 
--+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--+
--         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--@@ -61,7 +63,7 @@ class Model(nn.Module):
--         if self.has_aux_out:
--             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-- 
---    def forward(self, x_feat, x_raw, session_ids):
--+    def forward(self, x_feat, x_raw, y,session_ids):
--         # x shape is (batch, time, electrode)
-- 
--         if self.training:
--@@ -76,10 +78,12 @@ class Model(nn.Module):
--         x_raw = self.w_raw_in(x_raw)
-- 
--         x = x_raw
--+        tgt=self.embedding_tgt(y)
-- 
--         x = x.transpose(0,1) # put time first
--+        tgt = tgt.transpose(0,1) # put channel after
--         x = self.transformerEncoder(x)
---        x = self.transformerDecoder(x) #TODO I need the target EMG
--+        x = self.transformerDecoder(tgt, x)
--         x = x.transpose(0,1)
-- 
--         if self.has_aux_out:
--diff --git a/data_utils.py b/data_utils.py
--index 11d4805..e4ac852 100644
----- a/data_utils.py
--+++ b/data_utils.py
--@@ -244,6 +244,7 @@ class TextTransform(object):
--     def __init__(self):
--         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--         self.chars = string.ascii_lowercase+string.digits+' '
--+        self.vocabulary_size=len(self.chars)
-- 
--     def clean_text(self, text):
--         text = unidecode(text)
--diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--index fbc0abb..400061a 100644
----- a/models/recognition_model/log.txt
--+++ b/models/recognition_model/log.txt
--@@ -1,188 +1,2 @@
---57f8139449dd9286c2203ec2eca118a550638a7c
--+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-- 
---diff --git a/architecture.py b/architecture.py
---index 4fc3793..b22af61 100644
------ a/architecture.py
---+++ b/architecture.py
---@@ -4,7 +4,7 @@ import torch
--- from torch import nn
--- import torch.nn.functional as F
--- 
----from transformer import TransformerEncoderLayer
---+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--- 
--- from absl import flags
--- FLAGS = flags.FLAGS
---@@ -52,7 +52,9 @@ class Model(nn.Module):
---         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--- 
---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
---+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
---+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--- 
---         self.has_aux_out = num_aux_outs is not None
---@@ -76,7 +78,8 @@ class Model(nn.Module):
---         x = x_raw
--- 
---         x = x.transpose(0,1) # put time first
----        x = self.transformer(x)
---+        x = self.transformerEncoder(x)
---+        x = self.transformerDecoder(x) #TODO I need the target EMG
---         x = x.transpose(0,1)
--- 
---         if self.has_aux_out:
---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
---index 571de9d..8563980 100644
------ a/models/recognition_model/log.txt
---+++ b/models/recognition_model/log.txt
---@@ -1,5 +1,2 @@
---+57f8139449dd9286c2203ec2eca118a550638a7c
--- 
----
----['recognition_model.py', '--output_directory', './models/recognition_model/']
----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----train / dev split: 8055 200
---diff --git a/output/log.txt b/output/log.txt
---index ae42364..1d2cd8e 100644
------ a/output/log.txt
---+++ b/output/log.txt
---@@ -1,3 +1,13 @@
---+57f8139449dd9286c2203ec2eca118a550638a7c
--- 
---+diff --git a/output/log.txt b/output/log.txt
---+index ae42364..8563980 100644
---+--- a/output/log.txt
---++++ b/output/log.txt
---+@@ -1,3 +1,2 @@
---++57f8139449dd9286c2203ec2eca118a550638a7c
---+ 
---+-
---+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--- 
--- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
---diff --git a/transformer.py b/transformer.py
---index 6743588..ac131be 100644
------ a/transformer.py
---+++ b/transformer.py
---@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
---         Shape:
---             see the docs in Transformer class.
---         """
----        src2 = self.self_attn(src)
---+        src2 = self.self_attn(src, src, src)
---         src = src + self.dropout1(src2)
---         src = self.norm1(src)
---         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
---@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
---         src = self.norm2(src)
---         return src
--- 
---+class TransformerDecoderLayer(nn.Module):
---+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
---+    This standard decoder layer is based on the paper "Attention Is All You Need".
---+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
---+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
---+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
---+    in a different way during application.
---+
---+    Args:
---+        d_model: the number of expected features in the input (required).
---+        nhead: the number of heads in the multiheadattention models (required).
---+        dim_feedforward: the dimension of the feedforward network model (default=2048).
---+        dropout: the dropout value (default=0.1).
---+        activation: the activation function of the intermediate layer, can be a string
---+            ("relu" or "gelu") or a unary callable. Default: relu
---+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
---+        batch_first: If ``True``, then the input and output tensors are provided
---+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
---+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
---+            attention and feedforward operations, respectively. Otherwise it's done after.
---+            Default: ``False`` (after).
---+
---+    Examples::
---+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
---+        >>> memory = torch.rand(10, 32, 512)
---+        >>> tgt = torch.rand(20, 32, 512)
---+        >>> out = decoder_layer(tgt, memory)
---+    """
---+    # Adapted from pytorch source
---+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
---+        super(TransformerDecoderLayer, self).__init__()
---+        #Attention Mechanism
---+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
---+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
---+        # Implementation of Feedforward model
---+        self.linear1 = nn.Linear(d_model, dim_feedforward)
---+        self.dropout = nn.Dropout(dropout)
---+        self.linear2 = nn.Linear(dim_feedforward, d_model)
---+        #Normalization Layer and Dropout Layer
---+        self.norm1 = nn.LayerNorm(d_model)
---+        self.norm2 = nn.LayerNorm(d_model)
---+        self.norm3 = nn.LayerNorm(d_model)
---+        self.dropout1 = nn.Dropout(dropout)
---+        self.dropout2 = nn.Dropout(dropout)
---+        self.dropout3 = nn.Dropout(dropout)
---+        #Activation Function
---+        self.activation = nn.ReLU()
---+    
---+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
---+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
---+        r"""Pass the input through the encoder layer.
---+
---+        Args:
---+            tgt: the sequence to the decoder layer (required).
---+            memory: the sequence from the last layer of the encoder (required).
---+            tgt_mask: the mask for the tgt sequence (optional).
---+            memory_mask: the mask for the memory sequence (optional).
---+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
---+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
---+
---+        Shape:
---+            see the docs in Transformer class.
---+        """
---+        tgt2 = self.self_attn(tgt, tgt, tgt)
---+        tgt = tgt + self.dropout1(tgt2)
---+        tgt = self.norm1(tgt)
---+
---+        tgt2=self.multihead_attn(tgt, memory, memory)
---+        tgt = tgt + self.dropout1(tgt2)
---+        tgt = self.norm1(tgt)
---+
---+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
---+        tgt = tgt + self.dropout2(tgt2)
---+        tgt = self.norm2(tgt)
---+        return tgt
---+    
---+
--- class MultiHeadAttention(nn.Module):
---   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
---     super().__init__()
---@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
---     else:
---         self.relative_positional = None
--- 
----  def forward(self, x):
---+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
---     """Runs the multi-head self-attention layer.
--- 
---     Args:
---@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
---       A single tensor containing the output from this layer
---     """
--- 
----    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
----    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
----    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
---+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
---+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
---+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
---     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--- 
---     if self.relative_positional is not None:
---
---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
---train / dev split: 8055 200
--diff --git a/recognition_model.py b/recognition_model.py
--index dea6d47..a46dff0 100644
----- a/recognition_model.py
--+++ b/recognition_model.py
--@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-- 
--             X = combine_fixed_length(example['emg'], 200).to(device)
--             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-- 
---            pred = model(X, X_raw, sess)
--+            pred = model(X, X_raw, tgt, sess)
--             pred = F.log_softmax(pred, 2)
-- 
--             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--
--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--train / dev split: 8055 200
-diff --git a/recognition_model.py b/recognition_model.py
-index a46dff0..8fd300c 100644
---- a/recognition_model.py
-+++ b/recognition_model.py
-@@ -6,6 +6,7 @@ import subprocess
- from ctcdecode import CTCBeamDecoder
- import jiwer
- import random
-+from torch.utils.tensorboard import SummaryWriter
- 
- import torch
- from torch import nn
-@@ -13,7 +14,7 @@ import torch.nn.functional as F
- 
- from read_emg import EMGDataset, SizeAwareSampler
- from architecture import Model
--from data_utils import combine_fixed_length, decollate_tensor
-+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
- from transformer import TransformerEncoderLayer
- 
- from absl import flags
-@@ -62,17 +63,21 @@ def test(model, testset, device):
-     return jiwer.wer(references, predictions)
- 
- 
--def train_model(trainset, devset, device, n_epochs=200):
--    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--
-+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-+    #Define Dataloader
-+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
- 
-+    #Define model and loss function
-     n_chars = len(devset.text_transform.chars)
-     model = Model(devset.num_features, n_chars+1).to(device)
-+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
- 
-     if FLAGS.start_training_from is not None:
-         state_dict = torch.load(FLAGS.start_training_from)
-         model.load_state_dict(state_dict, strict=False)
- 
-+    #Define optimizer and scheduler for the learning rate
-     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
- 
-@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
- 
-     batch_idx = 0
-+    train_loss= 0
-+    eval_loss = 0
-     optim.zero_grad()
-     for epoch_idx in range(n_epochs):
-+        model.train()
-         losses = []
--        for example in dataloader:
-+        for example in dataloader_training:
-             schedule_lr(batch_idx)
- 
-+            #Preprosessing of the input and target for the model
-             X = combine_fixed_length(example['emg'], 200).to(device)
-             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
- 
-+            #Shifting target for input decoder and loss
-+            tgt= y[:,:-1]
-+            target= y[:,1:]
-+
-+            #Prediction
-             pred = model(X, X_raw, tgt, sess)
--            pred = F.log_softmax(pred, 2)
- 
--            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+            #Primary Loss
-+            pred=pred.permute(0,2,1)
-+            loss = loss_fn(pred, target)
-+
-+            #Auxiliary Loss
-+            #pred = F.log_softmax(pred, 2)
-+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-             losses.append(loss.item())
-+            train_loss += loss.item()
- 
-             loss.backward()
-             if (batch_idx+1) % 2 == 0:
-                 optim.step()
-                 optim.zero_grad()
- 
-+            #Report plots in tensorboard
-+            if batch_idx % report_every == report_every - 2:     
-+                #Evaluation
-+                model.eval()
-+                with torch.no_grad():
-+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-+                        X_raw = example['raw_emg'].to(device)
-+                        sess = example['session_ids'].to(device)
-+                        y = example['text_int'].to(device)
-+
-+                        #Shifting target for input decoder and loss
-+                        tgt= y[:,:-1]
-+                        target= y[:,1:]
-+
-+                        #Prediction without the 197-th batch because of missing label
-+                        if idx != 197:
-+                            pred = model(X, X_raw, tgt, sess)
-+                            #Primary Loss
-+                            pred=pred.permute(0,2,1)
-+                            loss = loss_fn(pred, target)
-+                            eval_loss += loss.item()
-+
-+                #Writing on tensorboard
-+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-+                train_loss= 0
-+                eval_loss= 0
-+
-+            #Increment counter        
-             batch_idx += 1
--        train_loss = np.mean(losses)
-+
-+        #Testing and change learning rate
-         val = test(model, devset, device)
-+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-         lr_sched.step()
-+    
-+        #Logging
-+        train_loss = np.mean(losses)
-         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
- 
-@@ -148,8 +201,9 @@ def main():
-     logging.info('train / dev split: %d %d',len(trainset),len(devset))
- 
-     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-+    writer = SummaryWriter(log_dir="./content/runs")
- 
--    model = train_model(trainset, devset, device)
-+    model = train_model(trainset, devset ,device, writer)
- 
- if __name__ == '__main__':
-     FLAGS(sys.argv)
-diff --git a/transformer.py b/transformer.py
-index ac131be..51e1f2e 100644
---- a/transformer.py
-+++ b/transformer.py
-@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-     self.d_qkv = d_qkv
- 
-+    #self.kdim = kdim if kdim is not None else embed_dim
-+    #self.vdim = vdim if vdim is not None else embed_dim
-+
-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-
-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-train / dev split: 8055 200
diff --git a/recognition_model.py b/recognition_model.py
index fde5a40..6d5143b 100644
--- a/recognition_model.py
+++ b/recognition_model.py
@@ -63,14 +63,14 @@ def test(model, testset, device):
     return jiwer.wer(references, predictions)
 
 
-def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
     #Define Dataloader
     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
 
     #Define model and loss function
     n_chars = len(devset.text_transform.chars)
-    model = Model(devset.num_features, n_chars+1).to(device)
+    model = Model(devset.num_features, n_chars+1, True).to(device)
     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
 
     if FLAGS.start_training_from is not None:
@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
             target= y[:,1:]
 
             #Prediction
-            pred = model(X, X_raw, tgt, sess)
+            out_enc, out_dec = model(X, X_raw, tgt, sess)
 
             #Primary Loss
-            pred=pred.permute(0,2,1)
-            loss = loss_fn(pred, target)
+            out_dec=out_dec.permute(0,2,1)
+            loss_dec = loss_fn(out_dec, target)
 
             #Auxiliary Loss
-            #pred = F.log_softmax(pred, 2)
-            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
+            out_enc = F.log_softmax(out_enc, 2)
+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
+
+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
             losses.append(loss.item())
             train_loss += loss.item()
 
@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
             if (batch_idx+1) % 2 == 0:
                 optim.step()
                 optim.zero_grad()
-
-            if batch_idx % report_every == report_every - 2:     
+            
+            if False:
+            #if batch_idx % report_every == report_every - 2:     
                 #Evaluation
                 model.eval()
                 with torch.no_grad():
                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-                        X_raw = example['raw_emg'].to(device)
-                        sess = example['session_ids'].to(device)
-                        y = example['text_int'].to(device)
+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
 
                         #Shifting target for input decoder and loss
                         tgt= y[:,:-1]
                         target= y[:,1:]
 
+                        print(idx)
+
                         #Prediction without the 197-th batch because of missing label
-                        if idx != 197:
+                        if idx != 181:
                             pred = model(X, X_raw, tgt, sess)
                             #Primary Loss
                             pred=pred.permute(0,2,1)
@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
 
             #Increment counter        
             batch_idx += 1
+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
 
         #Testing and change learning rate
         val = test(model, devset, device)

['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
train / dev split: 8055 200
