f6cac23b8fedc0c25322157824ddc23cc56529a6

diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
index 34df652..f2d4c4b 100644
--- a/models/recognition_model/log.txt
+++ b/models/recognition_model/log.txt
@@ -1,11621 +1,2 @@
-5bcb1728eddeb24d9db0848ff16d7060a8224f0f
+f6cac23b8fedc0c25322157824ddc23cc56529a6
 
-diff --git a/.gitignore b/.gitignore
-index 1aa8ca1..8451f93 100644
---- a/.gitignore
-+++ b/.gitignore
-@@ -9,5 +9,4 @@ models/recognition_model/model.pt
- hifi_gan
- content/runs
- models/recognition_model/log.txt
--descriptions
- text_alignments
-diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-index 0400a21..3c8d0ba 100644
---- a/models/recognition_model/log.txt
-+++ b/models/recognition_model/log.txt
-@@ -1,11354 +1,2 @@
--1f0ae9fc61ec7f335ca23b30bb4f60f17e8c6af2
-+5bcb1728eddeb24d9db0848ff16d7060a8224f0f
- 
--diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--index bd4763c..7dfe75a 100644
----- a/models/recognition_model/log.txt
--+++ b/models/recognition_model/log.txt
--@@ -1,11168 +1,2 @@
---1c33db37c95ffc6cc95d06ca7008503994b2bbd2
--+1f0ae9fc61ec7f335ca23b30bb4f60f17e8c6af2
-- 
---diff --git a/align.py b/align.py
---deleted file mode 100644
---index 74437f6..0000000
------ a/align.py
---+++ /dev/null
---@@ -1,38 +0,0 @@
----import numpy as np
----import scipy
----import matplotlib.pyplot as plt
----from numba import jit
----
----import torch
----
----@jit
----def time_warp(costs):
----    dtw = np.zeros_like(costs)
----    dtw[0,1:] = np.inf
----    dtw[1:,0] = np.inf
----    eps = 1e-4
----    for i in range(1,costs.shape[0]):
----        for j in range(1,costs.shape[1]):
----            dtw[i,j] = costs[i,j] + min(dtw[i-1,j],dtw[i,j-1],dtw[i-1,j-1])
----    return dtw
----
----def align_from_distances(distance_matrix, debug=False):
----    # for each position in spectrum 1, returns best match position in spectrum2
----    # using monotonic alignment
----    dtw = time_warp(distance_matrix)
----
----    i = distance_matrix.shape[0]-1
----    j = distance_matrix.shape[1]-1
----    results = [0] * distance_matrix.shape[0]
----    while i > 0 and j > 0:
----        results[i] = j
----        i, j = min([(i-1,j),(i,j-1),(i-1,j-1)], key=lambda x: dtw[x[0],x[1]])
----
----    if debug:
----        visual = np.zeros_like(dtw)
----        visual[range(len(results)),results] = 1
----        plt.matshow(visual)
----        plt.show()
----
----    return results
----
---diff --git a/architecture.py b/architecture.py
---deleted file mode 100644
---index bbbeb61..0000000
------ a/architecture.py
---+++ /dev/null
---@@ -1,121 +0,0 @@
----import random
----
----import torch
----from torch import nn
----import torch.nn.functional as F
----
----from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
----from data_utils import decollate_tensor
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
----flags.DEFINE_integer('num_layers', 6, 'number of layers')
----flags.DEFINE_float('dropout', .2, 'dropout')
----
----class ResBlock(nn.Module):
----    def __init__(self, num_ins, num_outs, stride=1):
----        super().__init__()
----
----        self.conv1 = nn.Conv1d(num_ins, num_outs, 3, padding=1, stride=stride)
----        self.bn1 = nn.BatchNorm1d(num_outs)
----        self.conv2 = nn.Conv1d(num_outs, num_outs, 3, padding=1)
----        self.bn2 = nn.BatchNorm1d(num_outs)
----
----        if stride != 1 or num_ins != num_outs:
----            self.residual_path = nn.Conv1d(num_ins, num_outs, 1, stride=stride)
----            self.res_norm = nn.BatchNorm1d(num_outs)
----        else:
----            self.residual_path = None
----
----    def forward(self, x):
----        input_value = x
----
----        x = F.relu(self.bn1(self.conv1(x)))
----        x = self.bn2(self.conv2(x))
----
----        if self.residual_path is not None:
----            res = self.res_norm(self.residual_path(input_value))
----        else:
----            res = input_value
----
----        return F.relu(x + res)
----
----class Model(nn.Module):
----    def __init__(self, num_features, num_outs_enc, num_outs_dec, device , has_aux_loss=False):
----        super().__init__()
----
----        self.conv_blocks = nn.Sequential(
----            ResBlock(8, FLAGS.model_size, 2),
----            ResBlock(FLAGS.model_size, FLAGS.model_size, 2),
----            ResBlock(FLAGS.model_size, FLAGS.model_size, 2),
----        )
----        self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
----
----        self.embedding_tgt = nn.Embedding(num_outs_dec, FLAGS.model_size, padding_idx=0)
----        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
----
----        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
----        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
----        self.w_out = nn.Linear(FLAGS.model_size, num_outs_dec)
----
----        self.has_aux_loss = has_aux_loss
----        if self.has_aux_loss:
----            self.w_aux = nn.Linear(FLAGS.model_size, num_outs_enc)
----        self.device=device
----
----    def create_tgt_padding_mask(self, tgt):
----        # input tgt of shape ()
----        tgt_padding_mask = tgt == 0
----        return tgt_padding_mask
----    
----    def create_src_padding_mask(self, src):
----        # input tgt of shape ()
----        src_padding_mask = src == 0
----        return src_padding_mask
----    
----    def forward(self, x_raw, y, length_raw_signal):
----        # x shape is (batch, time, electrode)
----        # y shape is (batch, sequence_length)
----
----        if self.training:
----            r = random.randrange(8)
----            if r > 0:
----                x_raw_clone = x_raw.clone()
----                x_raw_clone[:,:-r,:] = x_raw[:,r:,:] # shift left r
----                x_raw_clone[:,-r:,:] = 0
----                x_raw = x_raw_clone
----
----        x_raw = x_raw.transpose(1,2) # put channel before time for conv
----        x_raw = self.conv_blocks(x_raw)
----        x_raw = x_raw.transpose(1,2)
----        x_raw = self.w_raw_in(x_raw)
----        x = x_raw
----
----        x=decollate_tensor(x, length_raw_signal)
----        x=nn.utils.rnn.pad_sequence(x, batch_first=True)
----
----        #Padding Target Mask and attention mask
----        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
----        src_key_padding_mask = self.create_src_padding_mask(x[:,:,0]).to(self.device)
----        memory_key_padding_mask = src_key_padding_mask
----        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
----
----        #Embedding and positional encoding of tgt
----        tgt=self.embedding_tgt(y)
----        tgt=self.pos_encoder(tgt)
----        
----        x = x.transpose(0,1) # put time first
----        tgt = tgt.transpose(0,1) # put sequence_length first
----        x_encoder = self.transformerEncoder(x, src_key_padding_mask=src_key_padding_mask)
----        x_decoder = self.transformerDecoder(tgt, x_encoder, memory_key_padding_mask=memory_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----
----        x_encoder = x_encoder.transpose(0,1)
----        x_decoder = x_decoder.transpose(0,1)
----
----        if self.has_aux_loss:
----            return self.w_aux(x_encoder), self.w_out(x_decoder)
----        else:
----            return self.w_out(x)
----
---diff --git a/asr_evaluation.py b/asr_evaluation.py
---deleted file mode 100644
---index 7c3083e..0000000
------ a/asr_evaluation.py
---+++ /dev/null
---@@ -1,31 +0,0 @@
----import os
----import logging
----
----import deepspeech
----import jiwer
----import soundfile as sf
----import numpy as np
----from unidecode import unidecode
----import librosa
----
----def evaluate(testset, audio_directory):
----    model = deepspeech.Model('deepspeech-0.7.0-models.pbmm')
----    model.enableExternalScorer('deepspeech-0.7.0-models.scorer')
----    predictions = []
----    targets = []
----    for i, datapoint in enumerate(testset):
----        audio, rate = sf.read(os.path.join(audio_directory,f'example_output_{i}.wav'))
----        if rate != 16000:
----            audio = librosa.resample(audio, rate, 16000)
----        assert model.sampleRate() == 16000, 'wrong sample rate'
----        audio_int16 = (audio*(2**15)).astype(np.int16)
----        text = model.stt(audio_int16)
----        predictions.append(text)
----        target_text = unidecode(datapoint['text'])
----        targets.append(target_text)
----    transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----    targets = transformation(targets)
----    predictions = transformation(predictions)
----    logging.info(f'targets: {targets}')
----    logging.info(f'predictions: {predictions}')
----    logging.info(f'wer: {jiwer.wer(targets, predictions)}')
---diff --git a/data_utils.py b/data_utils.py
---deleted file mode 100644
---index 6f7fe93..0000000
------ a/data_utils.py
---+++ /dev/null
---@@ -1,316 +0,0 @@
----import math
----import string
----
----import numpy as np
----import librosa
----import soundfile as sf
----from textgrids import TextGrid
----import jiwer
----from unidecode import unidecode
----
----import torch
----import matplotlib.pyplot as plt
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')
----
----phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']
----
----def normalize_volume(audio):
----    rms = librosa.feature.rms(audio)
----    max_rms = rms.max() + 0.01
----    target_rms = 0.2
----    audio = audio * (target_rms/max_rms)
----    max_val = np.abs(audio).max()
----    if max_val > 1.0: # this shouldn't happen too often with the target_rms of 0.2
----        audio = audio / max_val
----    return audio
----
----def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):
----    return torch.log(torch.clamp(x, min=clip_val) * C)
----
----def spectral_normalize_torch(magnitudes):
----    output = dynamic_range_compression_torch(magnitudes)
----    return output
----
----mel_basis = {}
----hann_window = {}
----
----def mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):
----    if torch.min(y) < -1.:
----        print('min value is ', torch.min(y))
----    if torch.max(y) > 1.:
----        print('max value is ', torch.max(y))
----
----    global mel_basis, hann_window
----    if fmax not in mel_basis:
----        mel = librosa.filters.mel(sr=sampling_rate,n_fft=n_fft,n_mels=num_mels, fmin=fmin, fmax=fmax)
----        mel_basis[str(fmax)+'_'+str(y.device)] = torch.from_numpy(mel).float().to(y.device)
----        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)
----
----    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')
----    y = y.squeeze(1)
----
----    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],
----                      center=center, pad_mode='reflect', normalized=False, onesided=True)
----
----    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))
----
----    spec = torch.matmul(mel_basis[str(fmax)+'_'+str(y.device)], spec)
----    spec = spectral_normalize_torch(spec)
----
----    return spec
----
----def load_audio(filename, start=None, end=None, max_frames=None, renormalize_volume=False):
----    audio, r = sf.read(filename)
----
----    if len(audio.shape) > 1:
----        audio = audio[:,0] # select first channel of stero audio
----    if start is not None or end is not None:
----        audio = audio[start:end]
----
----    if renormalize_volume:
----        audio = normalize_volume(audio)
----    if r == 16000:
----        audio = librosa.resample(y=audio, orig_sr=16000, target_sr=22050)
----    else:
----        assert r == 22050
----    audio = np.clip(audio, -1, 1) # because resampling sometimes pushes things out of range
----    pytorch_mspec = mel_spectrogram(torch.tensor(audio, dtype=torch.float32).unsqueeze(0), 1024, 80, 22050, 256, 1024, 0, 8000, center=False)
----    mspec = pytorch_mspec.squeeze(0).T.numpy()
----    if max_frames is not None and mspec.shape[0] > max_frames:
----        mspec = mspec[:max_frames,:]
----    return mspec
----
----def double_average(x):
----    assert len(x.shape) == 1
----    f = np.ones(9)/9.0
----    v = np.convolve(x, f, mode='same')
----    w = np.convolve(v, f, mode='same')
----    return w
----
----def get_emg_features(emg_data, debug=False):
----    xs = emg_data - emg_data.mean(axis=0, keepdims=True)
----    frame_features = []
----    for i in range(emg_data.shape[1]):
----        x = xs[:,i]
----        w = double_average(x)
----        p = x - w
----        r = np.abs(p)
----
----        w_h = librosa.util.frame(w, frame_length=16, hop_length=6).mean(axis=0)
----        p_w = librosa.feature.rms(y=w, frame_length=16, hop_length=6, center=False)
----        p_w = np.squeeze(p_w, 0)
----        p_r = librosa.feature.rms(y=r, frame_length=16, hop_length=6, center=False)
----        p_r = np.squeeze(p_r, 0)
----        z_p = librosa.feature.zero_crossing_rate(p, frame_length=16, hop_length=6, center=False)
----        z_p = np.squeeze(z_p, 0)
----        r_h = librosa.util.frame(r, frame_length=16, hop_length=6).mean(axis=0)
----
----        s = abs(librosa.stft(np.ascontiguousarray(x), n_fft=16, hop_length=6, center=False))
----        # s has feature dimension first and time second
----
----        if debug:
----            plt.subplot(7,1,1)
----            plt.plot(x)
----            plt.subplot(7,1,2)
----            plt.plot(w_h)
----            plt.subplot(7,1,3)
----            plt.plot(p_w)
----            plt.subplot(7,1,4)
----            plt.plot(p_r)
----            plt.subplot(7,1,5)
----            plt.plot(z_p)
----            plt.subplot(7,1,6)
----            plt.plot(r_h)
----
----            plt.subplot(7,1,7)
----            plt.imshow(s, origin='lower', aspect='auto', interpolation='nearest')
----
----            plt.show()
----
----        frame_features.append(np.stack([w_h, p_w, p_r, z_p, r_h], axis=1))
----        frame_features.append(s.T)
----
----    frame_features = np.concatenate(frame_features, axis=1)
----    return frame_features.astype(np.float32)
----
----class FeatureNormalizer(object):
----    def __init__(self, feature_samples, share_scale=False):
----        """ features_samples should be list of 2d matrices with dimension (time, feature) """
----        feature_samples = np.concatenate(feature_samples, axis=0)
----        self.feature_means = feature_samples.mean(axis=0, keepdims=True)
----        if share_scale:
----            self.feature_stddevs = feature_samples.std()
----        else:
----            self.feature_stddevs = feature_samples.std(axis=0, keepdims=True)
----
----    def normalize(self, sample):
----        sample -= self.feature_means
----        sample /= self.feature_stddevs
----        return sample
----
----    def inverse(self, sample):
----        sample = sample * self.feature_stddevs
----        sample = sample + self.feature_means
----        return sample
----
----def combine_fixed_length(tensor_list, length):
----    total_length = sum(t.size(0) for t in tensor_list)
----    if total_length % length != 0:
----        pad_length = length - (total_length % length)
----        tensor_list = list(tensor_list) # copy
----        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
----        total_length += pad_length
----    tensor = torch.cat(tensor_list, 0)
----    n = total_length // length
----    return tensor.view(n, length, *tensor.size()[1:])
----
----def decollate_tensor(tensor, lengths):
----    b, s, d = tensor.size()
----    tensor = tensor.view(b*s, d)
----    results = []
----    idx = 0
----    for length in lengths:
----        assert idx + length <= b * s
----        results.append(tensor[idx:idx+length])
----        idx += length
----    return results
----
----def splice_audio(chunks, overlap):
----    chunks = [c.copy() for c in chunks] # copy so we can modify in place
----
----    assert np.all([c.shape[0]>=overlap for c in chunks])
----
----    result_len = sum(c.shape[0] for c in chunks) - overlap*(len(chunks)-1)
----    result = np.zeros(result_len, dtype=chunks[0].dtype)
----
----    ramp_up = np.linspace(0,1,overlap)
----    ramp_down = np.linspace(1,0,overlap)
----
----    i = 0
----    for chunk in chunks:
----        l = chunk.shape[0]
----
----        # note: this will also fade the beginning and end of the result
----        chunk[:overlap] *= ramp_up
----        chunk[-overlap:] *= ramp_down
----
----        result[i:i+l] += chunk
----        i += l-overlap
----
----    return result
----
----def print_confusion(confusion_mat, n=10):
----    # axes are (pred, target)
----    target_counts = confusion_mat.sum(0) + 1e-4
----    aslist = []
----    for p1 in range(len(phoneme_inventory)):
----        for p2 in range(p1):
----            if p1 != p2:
----                aslist.append(((confusion_mat[p1,p2]+confusion_mat[p2,p1])/(target_counts[p1]+target_counts[p2]), p1, p2))
----    aslist.sort()
----    aslist = aslist[-n:]
----    max_val = aslist[-1][0]
----    min_val = aslist[0][0]
----    val_range = max_val - min_val
----    print('Common confusions (confusion, accuracy)')
----    for v, p1, p2 in aslist:
----        p1s = phoneme_inventory[p1]
----        p2s = phoneme_inventory[p2]
----        print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')
----
----def read_phonemes(textgrid_fname, max_len=None):
----    tg = TextGrid(textgrid_fname)
----    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)
----    phone_ids[:] = -1
----    phone_ids[-1] = phoneme_inventory.index('sil') # make sure list is long enough to cover full length of original sequence
----    for interval in tg['phones']:
----        phone = interval.text.lower()
----        if phone in ['', 'sp', 'spn']:
----            phone = 'sil'
----        if phone[-1] in string.digits:
----            phone = phone[:-1]
----        ph_id = phoneme_inventory.index(phone)
----        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id
----    assert (phone_ids >= 0).all(), 'missing aligned phones'
----
----    if max_len is not None:
----        phone_ids = phone_ids[:max_len]
----        assert phone_ids.shape[0] == max_len
----    return phone_ids
----
----class TextTransform(object):
----    '''
----    def __init__(self, pad_token="<pad>", blank_token='_', eos_token='<eos>', sos_token='<sos>'):
----        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----        self.id_to_string = {}
----        self.string_to_id = {}
----        
----        # add the default pad token
----        self.id_to_string[0] = pad_token
----        self.string_to_id[pad_token] = 0
----        
----        # add the default unknown token
----        self.id_to_string[1] = blank_token
----        self.string_to_id[blank_token] = 1
----        
----        # add the default unknown token
----        self.id_to_string[2] = eos_token
----        self.string_to_id[eos_token] = 2   
----
----        # add the default unknown token
----        self.id_to_string[3] = sos_token
----        self.string_to_id[sos_token] = 3
----
----        # shortcut access
----        self.pad_id = 0
----        self.blank_id = 1
----        self.eos_id = 2
----        self.sos_id = 3
----
----    def __len__(self):
----        return len('<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' ')
----    
----    def clean_text(self, text):
----        text = unidecode(text)
----        text = self.transformation(text)
----        return text
----    
----    def add_new_words(self, text):
----        text = self.clean_text(text)
----        for c in text:
----            if c not in self.string_to_id:
----                self.string_to_id[c] = len(self.string_to_id)
----                self.id_to_string[len(self.id_to_string)] = c
----    
----    def get_labels(self):
----        return  '<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' '
----            
----    def text_to_int(self, text):
----        text = self.clean_text(text)
----        return [self.string_to_id[c] for c in text]
----
----    def int_to_text(self, ints):
----        return ''.join(self.id_to_string[i] for i in ints)
----    '''
----    
----
----    def __init__(self):
----        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----        self.chars = "*" + string.ascii_lowercase+string.digits+ ' '
----        self.vocabulary_size=len(self.chars)
----
----    def clean_text(self, text):
----        text = unidecode(text)
----        text = self.transformation(text)
----        return text
----
----    def text_to_int(self, text):
----        text = self.clean_text(text)
----        return [self.chars.index(c) for c in text]
----
----    def int_to_text(self, ints):
----        return ''.join(self.chars[i] for i in ints)
----    
---diff --git a/evaluate.py b/evaluate.py
---deleted file mode 100644
---index e1fbb8f..0000000
------ a/evaluate.py
---+++ /dev/null
---@@ -1,66 +0,0 @@
----import sys
----import os
----import logging
----
----import torch
----from torch import nn
----
----from architecture import Model
----from transduction_model import test, save_output
----from read_emg import EMGDataset
----from asr_evaluation import evaluate
----from data_utils import phoneme_inventory, print_confusion
----from vocoder import Vocoder
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_list('models', [], 'identifiers of models to evaluate')
----flags.DEFINE_boolean('dev', False, 'evaluate dev insead of test')
----
----class EnsembleModel(nn.Module):
----    def __init__(self, models):
----        super().__init__()
----        self.models = nn.ModuleList(models)
----
----    def forward(self, x, x_raw, sess):
----        ys = []
----        ps = []
----        for model in self.models:
----            y, p = model(x, x_raw, sess)
----            ys.append(y)
----            ps.append(p)
----        return torch.stack(ys,0).mean(0), torch.stack(ps,0).mean(0)
----
----def main():
----    os.makedirs(FLAGS.output_directory, exist_ok=True)
----    logging.basicConfig(handlers=[
----            logging.FileHandler(os.path.join(FLAGS.output_directory, 'eval_log.txt'), 'w'),
----            logging.StreamHandler()
----            ], level=logging.INFO, format="%(message)s")
----
----    dev = FLAGS.dev
----    testset = EMGDataset(dev=dev, test=not dev)
----
----    device = 'cuda' if torch.cuda.is_available() else 'cpu'
----
----    models = []
----    for fname in FLAGS.models:
----        state_dict = torch.load(fname)
----        model = Model(testset.num_features, testset.num_speech_features, len(phoneme_inventory)).to(device)
----        model.load_state_dict(state_dict)
----        models.append(model)
----    ensemble = EnsembleModel(models)
----
----    _, _, confusion = test(ensemble, testset, device)
----    print_confusion(confusion)
----
----    vocoder = Vocoder()
----
----    for i, datapoint in enumerate(testset):
----        save_output(ensemble, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device, testset.mfcc_norm, vocoder)
----
----    evaluate(testset, FLAGS.output_directory)
----
----if __name__ == "__main__":
----    FLAGS(sys.argv)
----    main()
---diff --git a/make_vocoder_trainset.py b/make_vocoder_trainset.py
---deleted file mode 100644
---index 43dbe46..0000000
------ a/make_vocoder_trainset.py
---+++ /dev/null
---@@ -1,51 +0,0 @@
----import sys
----import os
----import shutil
----import numpy as np
----
----import soundfile as sf
----import librosa
----
----import torch
----from torch import nn
----
----from architecture import Model
----from transduction_model import get_aligned_prediction
----from read_emg import EMGDataset
----from data_utils import phoneme_inventory
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_string('model', None, 'checkpoint of model to run')
----
----def main():
----    trainset = EMGDataset(dev=False,test=False)
----    devset = EMGDataset(dev=True)
----
----    device = 'cuda' if torch.cuda.is_available() else 'cpu'
----
----    n_phones = len(phoneme_inventory)
----    model = Model(devset.num_features, devset.num_speech_features, n_phones, devset.num_sessions).to(device)
----    state_dict = torch.load(FLAGS.model)
----    model.load_state_dict(state_dict)
----
----    os.makedirs(os.path.join(FLAGS.output_directory, 'mels'), exist_ok=True)
----    os.makedirs(os.path.join(FLAGS.output_directory, 'wavs'), exist_ok=True)
----
----    for dataset, name_prefix in [(trainset, 'train'), (devset, 'dev')]:
----        with open(os.path.join(FLAGS.output_directory, f'{name_prefix}_filelist.txt'), 'w') as filelist:
----            for i, datapoint in enumerate(dataset):
----                spec = get_aligned_prediction(model, datapoint, device, dataset.mfcc_norm)
----                spec = spec.T[np.newaxis,:,:].detach().cpu().numpy()
----                np.save(os.path.join(FLAGS.output_directory, 'mels', f'{name_prefix}_output_{i}.npy'), spec)
----                audio, r = sf.read(datapoint['audio_file'])
----                if r != 22050:
----                    audio = librosa.resample(audio, r, 22050, res_type='kaiser_fast')
----                audio = np.clip(audio, -1, 1) # because resampling sometimes pushes things out of range
----                sf.write(os.path.join(FLAGS.output_directory, 'wavs', f'{name_prefix}_output_{i}.wav'), audio, 22050)
----                filelist.write(f'{name_prefix}_output_{i}\n')
----        
----
----if __name__ == "__main__":
----    FLAGS(sys.argv)
----    main()
---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
---index f125a9b..c4d1162 100644
------ a/models/recognition_model/log.txt
---+++ b/models/recognition_model/log.txt
---@@ -1,9183 +1,2 @@
----13cdf66caaf2329756214d61f3ea3e53a5a166e4
---+1c33db37c95ffc6cc95d06ca7008503994b2bbd2
--- 
----diff --git a/architecture.py b/architecture.py
----index 08f6772..bbbeb61 100644
------- a/architecture.py
----+++ b/architecture.py
----@@ -5,7 +5,7 @@ from torch import nn
---- import torch.nn.functional as F
---- 
---- from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-----
----+from data_utils import decollate_tensor
---- from absl import flags
---- FLAGS = flags.FLAGS
---- flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
----@@ -70,7 +70,12 @@ class Model(nn.Module):
----         tgt_padding_mask = tgt == 0
----         return tgt_padding_mask
----     
-----    def forward(self, x_raw, y):
----+    def create_src_padding_mask(self, src):
----+        # input tgt of shape ()
----+        src_padding_mask = src == 0
----+        return src_padding_mask
----+    
----+    def forward(self, x_raw, y, length_raw_signal):
----         # x shape is (batch, time, electrode)
----         # y shape is (batch, sequence_length)
---- 
----@@ -88,8 +93,13 @@ class Model(nn.Module):
----         x_raw = self.w_raw_in(x_raw)
----         x = x_raw
---- 
----+        x=decollate_tensor(x, length_raw_signal)
----+        x=nn.utils.rnn.pad_sequence(x, batch_first=True)
----+
----         #Padding Target Mask and attention mask
----         tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
----+        src_key_padding_mask = self.create_src_padding_mask(x[:,:,0]).to(self.device)
----+        memory_key_padding_mask = src_key_padding_mask
----         tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
---- 
----         #Embedding and positional encoding of tgt
----@@ -98,8 +108,8 @@ class Model(nn.Module):
----         
----         x = x.transpose(0,1) # put time first
----         tgt = tgt.transpose(0,1) # put sequence_length first
-----        x_encoder = self.transformerEncoder(x)
-----        x_decoder = self.transformerDecoder(tgt, x_encoder, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----+        x_encoder = self.transformerEncoder(x, src_key_padding_mask=src_key_padding_mask)
----+        x_decoder = self.transformerDecoder(tgt, x_encoder, memory_key_padding_mask=memory_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
---- 
----         x_encoder = x_encoder.transpose(0,1)
----         x_decoder = x_decoder.transpose(0,1)
----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
----index a8259db..f1b9524 100644
------- a/models/recognition_model/log.txt
----+++ b/models/recognition_model/log.txt
----@@ -1,5630 +1,2 @@
-----c0ff3e593059fc6ca881802c4ecda4e17816dc73
----+13cdf66caaf2329756214d61f3ea3e53a5a166e4
---- 
-----diff --git a/architecture.py b/architecture.py
-----index eab7ba3..08f6772 100644
-------- a/architecture.py
-----+++ b/architecture.py
-----@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----         return F.relu(x + res)
----- 
----- class Model(nn.Module):
------    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-----+    def __init__(self, num_features, num_outs_enc, num_outs_dec, device , has_aux_loss=False):
-----         super().__init__()
----- 
-----         self.conv_blocks = nn.Sequential(
-----@@ -51,18 +51,18 @@ class Model(nn.Module):
-----         )
-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
----- 
------        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)
-----+        self.embedding_tgt = nn.Embedding(num_outs_dec, FLAGS.model_size, padding_idx=0)
-----         self.pos_encoder = PositionalEncoding(FLAGS.model_size)
----- 
-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
------        self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----+        self.w_out = nn.Linear(FLAGS.model_size, num_outs_dec)
----- 
-----         self.has_aux_loss = has_aux_loss
-----         if self.has_aux_loss:
------            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-----+            self.w_aux = nn.Linear(FLAGS.model_size, num_outs_enc)
-----         self.device=device
----- 
-----     def create_tgt_padding_mask(self, tgt):
-----@@ -99,7 +99,7 @@ class Model(nn.Module):
-----         x = x.transpose(0,1) # put time first
-----         tgt = tgt.transpose(0,1) # put sequence_length first
-----         x_encoder = self.transformerEncoder(x)
------        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
-----+        x_decoder = self.transformerDecoder(tgt, x_encoder, tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----- 
-----         x_encoder = x_encoder.transpose(0,1)
-----         x_decoder = x_decoder.transpose(0,1)
-----diff --git a/content/runs/events.out.tfevents.1678814083.x05.818136.0 b/content/runs/events.out.tfevents.1678814083.x05.818136.0
-----deleted file mode 100644
-----index d2604ad..0000000
-----Binary files a/content/runs/events.out.tfevents.1678814083.x05.818136.0 and /dev/null differ
-----diff --git a/content/runs/events.out.tfevents.1678814434.x05.869347.0 b/content/runs/events.out.tfevents.1678814434.x05.869347.0
-----deleted file mode 100644
-----index cdaf50d..0000000
-----Binary files a/content/runs/events.out.tfevents.1678814434.x05.869347.0 and /dev/null differ
-----diff --git a/content/runs/events.out.tfevents.1678814613.x05.901212.0 b/content/runs/events.out.tfevents.1678814613.x05.901212.0
-----deleted file mode 100644
-----index 4aaabf9..0000000
-----Binary files a/content/runs/events.out.tfevents.1678814613.x05.901212.0 and /dev/null differ
-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----index 442a426..07d5cd7 100644
-------- a/models/recognition_model/log.txt
-----+++ b/models/recognition_model/log.txt
-----@@ -1,5467 +1,2 @@
------aa1dc781b41bdd7245f12ef208b68733fc671794
-----+c0ff3e593059fc6ca881802c4ecda4e17816dc73
----- 
------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
------index bae57f5..e974e75 100644
--------- a/models/recognition_model/log.txt
------+++ b/models/recognition_model/log.txt
------@@ -1,5336 +1,2 @@
-------8997bdb087f32f96fa632af6c28b9ffa3f442abf
------+aa1dc781b41bdd7245f12ef208b68733fc671794
------ 
-------diff --git a/architecture.py b/architecture.py
-------index 8671340..eab7ba3 100644
---------- a/architecture.py
-------+++ b/architecture.py
-------@@ -9,7 +9,7 @@ from transformer import TransformerEncoderLayer, TransformerDecoderLayer, Positi
------- from absl import flags
------- FLAGS = flags.FLAGS
------- flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
--------flags.DEFINE_integer('num_layers', 3, 'number of layers')
-------+flags.DEFINE_integer('num_layers', 6, 'number of layers')
------- flags.DEFINE_float('dropout', .2, 'dropout')
------- 
------- class ResBlock(nn.Module):
-------diff --git a/data_utils.py b/data_utils.py
-------index 0cddf1e..6f7fe93 100644
---------- a/data_utils.py
-------+++ b/data_utils.py
-------@@ -299,7 +299,7 @@ class TextTransform(object):
------- 
-------     def __init__(self):
-------         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--------        self.chars = "<pad>" + string.ascii_lowercase+string.digits+ ' '
-------+        self.chars = "*" + string.ascii_lowercase+string.digits+ ' '
-------         self.vocabulary_size=len(self.chars)
------- 
-------     def clean_text(self, text):
-------diff --git a/environment.yml b/environment.yml
-------deleted file mode 100644
-------index 8232df7..0000000
---------- a/environment.yml
-------+++ /dev/null
-------@@ -1,152 +0,0 @@
--------name: tf
--------channels:
--------  - pytorch
--------  - conda-forge
--------  - defaults
--------dependencies:
--------  - _libgcc_mutex=0.1=conda_forge
--------  - _openmp_mutex=4.5=2_kmp_llvm
--------  - absl-py=1.4.0=pyhd8ed1ab_0
--------  - aiohttp=3.8.4=py38h1de0b5d_0
--------  - aiosignal=1.3.1=pyhd8ed1ab_0
--------  - async-timeout=4.0.2=pyhd8ed1ab_0
--------  - attrs=22.2.0=pyh71513ae_0
--------  - blas=1.0=mkl
--------  - blinker=1.5=pyhd8ed1ab_0
--------  - brotlipy=0.7.0=py38h0a891b7_1005
--------  - bzip2=1.0.8=h7f98852_4
--------  - c-ares=1.18.1=h7f98852_0
--------  - ca-certificates=2022.12.7=ha878542_0
--------  - cachetools=5.3.0=pyhd8ed1ab_0
--------  - certifi=2022.12.7=pyhd8ed1ab_0
--------  - cffi=1.15.1=py38h4a40e3a_3
--------  - click=8.1.3=unix_pyhd8ed1ab_2
--------  - cryptography=39.0.2=py38h3d167d9_0
--------  - cudatoolkit=10.1.243=h6bb024c_0
--------  - frozenlist=1.3.3=py38h0a891b7_0
--------  - gettext=0.21.1=h27087fc_0
--------  - google-auth=2.16.2=pyh1a96a4e_0
--------  - google-auth-oauthlib=0.4.6=pyhd8ed1ab_0
--------  - grpcio=1.52.1=py38h8dc9893_1
--------  - icu=70.1=h27087fc_0
--------  - idna=3.4=pyhd8ed1ab_0
--------  - importlib-metadata=6.0.0=pyha770c72_0
--------  - ld_impl_linux-64=2.40=h41732ed_0
--------  - libabseil=20230125.0=cxx17_hcb278e6_1
--------  - libffi=3.4.2=h7f98852_5
--------  - libflac=1.4.2=h27087fc_0
--------  - libgcc-ng=12.2.0=h65d4601_19
--------  - libgrpc=1.52.1=hcf146ea_1
--------  - libhwloc=2.9.0=hd6dc26d_0
--------  - libiconv=1.17=h166bdaf_0
--------  - libnsl=2.0.0=h7f98852_0
--------  - libogg=1.3.4=h7f98852_1
--------  - libprotobuf=3.21.12=h3eb15da_0
--------  - libsndfile=1.0.28=he1b5a44_1000
--------  - libsqlite=3.40.0=h753d276_0
--------  - libstdcxx-ng=12.2.0=h46fd767_19
--------  - libuuid=2.32.1=h7f98852_1000
--------  - libuv=1.44.2=h166bdaf_0
--------  - libvorbis=1.3.7=h9c3ff4c_0
--------  - libxml2=2.10.3=h7463322_0
--------  - libzlib=1.2.13=h166bdaf_4
--------  - llvm-openmp=15.0.7=h0cdce71_0
--------  - markdown=3.4.1=pyhd8ed1ab_0
--------  - markupsafe=2.1.2=py38h1de0b5d_0
--------  - mkl=2021.4.0=h8d4b97c_729
--------  - mkl-service=2.4.0=py38h95df7f1_0
--------  - mkl_fft=1.3.1=py38hd3c417c_0
--------  - mkl_random=1.2.2=py38h1abd341_0
--------  - multidict=6.0.4=py38h1de0b5d_0
--------  - ncurses=6.3=h27087fc_1
--------  - ninja=1.11.1=h924138e_0
--------  - numpy=1.23.5=py38h14f4228_0
--------  - numpy-base=1.23.5=py38h31eccc5_0
--------  - oauthlib=3.2.2=pyhd8ed1ab_0
--------  - openssl=3.0.8=h0b41bf4_0
--------  - protobuf=4.21.12=py38h8dc9893_0
--------  - pyasn1=0.4.8=py_0
--------  - pyasn1-modules=0.2.7=py_0
--------  - pycparser=2.21=pyhd8ed1ab_0
--------  - pyjwt=2.6.0=pyhd8ed1ab_0
--------  - pyopenssl=23.0.0=pyhd8ed1ab_0
--------  - pysocks=1.7.1=pyha2e5f31_6
--------  - python=3.8.16=he550d4f_1_cpython
--------  - python_abi=3.8=3_cp38
--------  - pytorch=1.7.1=py3.8_cuda10.1.243_cudnn7.6.3_0
--------  - pyu2f=0.1.5=pyhd8ed1ab_0
--------  - re2=2023.02.02=hcb278e6_0
--------  - readline=8.1.2=h0f457ee_0
--------  - requests=2.28.2=pyhd8ed1ab_0
--------  - requests-oauthlib=1.3.1=pyhd8ed1ab_0
--------  - rsa=4.9=pyhd8ed1ab_0
--------  - six=1.16.0=pyh6c4a22f_0
--------  - tbb=2021.8.0=hf52228f_0
--------  - tensorboard=2.12.0=pyhd8ed1ab_0
--------  - tensorboard-data-server=0.7.0=py38h3d167d9_0
--------  - tensorboard-plugin-wit=1.8.1=pyhd8ed1ab_0
--------  - tk=8.6.12=h27826a3_0
--------  - torchaudio=0.7.2=py38
--------  - typing-extensions=4.5.0=hd8ed1ab_0
--------  - typing_extensions=4.5.0=pyha770c72_0
--------  - werkzeug=2.2.3=pyhd8ed1ab_0
--------  - wheel=0.38.4=pyhd8ed1ab_0
--------  - xz=5.2.6=h166bdaf_0
--------  - yarl=1.8.2=py38h0a891b7_0
--------  - zipp=3.15.0=pyhd8ed1ab_0
--------  - zlib=1.2.13=h166bdaf_4
--------  - pip:
--------      - appdirs==1.4.4
--------      - audioread==3.0.0
--------      - backoff==2.2.1
--------      - charset-normalizer==3.1.0
--------      - contourpy==1.0.7
--------      - ctcdecode==1.0.3
--------      - cycler==0.11.0
--------      - decorator==5.1.1
--------      - deepspeech==0.8.2
--------      - docker-pycreds==0.4.0
--------      - fonttools==4.39.0
--------      - gitdb==4.0.10
--------      - gitpython==3.1.31
--------      - gql==3.4.0
--------      - graphql-core==3.2.3
--------      - importlib-resources==5.12.0
--------      - jiwer==2.5.1
--------      - joblib==1.2.0
--------      - kiwisolver==1.4.4
--------      - lazy-loader==0.1
--------      - levenshtein==0.20.2
--------      - librosa==0.10.0
--------      - llvmlite==0.39.1
--------      - matplotlib==3.7.1
--------      - msgpack==1.0.4
--------      - numba==0.56.4
--------      - packaging==23.0
--------      - pandas==1.5.3
--------      - pathtools==0.1.2
--------      - pillow==9.4.0
--------      - pip==23.0.1
--------      - platformdirs==3.1.0
--------      - pooch==1.7.0
--------      - praat-textgrids==1.4.0
--------      - psutil==5.9.4
--------      - pyparsing==3.0.9
--------      - python-dateutil==2.8.2
--------      - pytz==2022.7.1
--------      - pyyaml==6.0
--------      - rapidfuzz==2.13.7
--------      - scikit-learn==1.2.1
--------      - scipy==1.10.1
--------      - sentry-sdk==1.17.0
--------      - setproctitle==1.3.2
--------      - setuptools==67.5.1
--------      - smmap==5.0.0
--------      - soundfile==0.12.1
--------      - soxr==0.3.4
--------      - threadpoolctl==3.1.0
--------      - torch-tb-profiler==0.4.1
--------      - unidecode==1.3.6
--------      - urllib3==1.26.14
--------      - wandb==0.14.0
--------prefix: /home/christian/anaconda3/envs/tf
-------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-------index 4e5a040..8d1dd40 100644
---------- a/models/recognition_model/log.txt
-------+++ b/models/recognition_model/log.txt
-------@@ -1,4984 +1,2 @@
--------5949d97bc1c69e68630acd5fe6c5973507aeda2f
-------+8997bdb087f32f96fa632af6c28b9ffa3f442abf
------- 
--------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------index 2eea771..208097f 100644
----------- a/models/recognition_model/log.txt
--------+++ b/models/recognition_model/log.txt
--------@@ -1,4959 +1,2 @@
---------29c44f0a9172221b31dc715f9f900609f525bddf
--------+5949d97bc1c69e68630acd5fe6c5973507aeda2f
-------- 
---------diff --git a/data_utils.py b/data_utils.py
---------index 33a96f7..0cddf1e 100644
------------ a/data_utils.py
---------+++ b/data_utils.py
---------@@ -299,7 +299,7 @@ class TextTransform(object):
--------- 
---------     def __init__(self):
---------         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----------        self.chars = string.ascii_lowercase+string.digits+' '
---------+        self.chars = "<pad>" + string.ascii_lowercase+string.digits+ ' '
---------         self.vocabulary_size=len(self.chars)
--------- 
---------     def clean_text(self, text):
---------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
---------index 39ae3f9..7d0df7b 100644
------------ a/models/recognition_model/log.txt
---------+++ b/models/recognition_model/log.txt
---------@@ -1,4863 +1,2 @@
----------897f691543960d43991a13846d6b09c92f89eb8c
---------+29c44f0a9172221b31dc715f9f900609f525bddf
--------- 
----------diff --git a/architecture.py b/architecture.py
----------index 91fd2da..8671340 100644
------------- a/architecture.py
----------+++ b/architecture.py
----------@@ -9,7 +9,7 @@ from transformer import TransformerEncoderLayer, TransformerDecoderLayer, Positi
---------- from absl import flags
---------- FLAGS = flags.FLAGS
---------- flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
-----------flags.DEFINE_integer('num_layers', 6, 'number of layers')
----------+flags.DEFINE_integer('num_layers', 3, 'number of layers')
---------- flags.DEFINE_float('dropout', .2, 'dropout')
---------- 
---------- class ResBlock(nn.Module):
----------@@ -54,8 +54,8 @@ class Model(nn.Module):
----------         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)
----------         self.pos_encoder = PositionalEncoding(FLAGS.model_size)
---------- 
-----------        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----------+        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----------+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----------         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
----------         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
----------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
----------@@ -65,12 +65,6 @@ class Model(nn.Module):
----------             self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
----------         self.device=device
---------- 
-----------
-----------    def create_src_padding_mask(self, src):
-----------        # input src of shape ()
-----------        src_padding_mask = src == 0
-----------        return src_padding_mask
-----------
----------     def create_tgt_padding_mask(self, tgt):
----------         # input tgt of shape ()
----------         tgt_padding_mask = tgt == 0
----------@@ -83,8 +77,10 @@ class Model(nn.Module):
----------         if self.training:
----------             r = random.randrange(8)
----------             if r > 0:
-----------                x_raw[:,:-r,:] = x_raw[:,r:,:] # shift left r
-----------                x_raw[:,-r:,:] = 0
----------+                x_raw_clone = x_raw.clone()
----------+                x_raw_clone[:,:-r,:] = x_raw[:,r:,:] # shift left r
----------+                x_raw_clone[:,-r:,:] = 0
----------+                x_raw = x_raw_clone
---------- 
----------         x_raw = x_raw.transpose(1,2) # put channel before time for conv
----------         x_raw = self.conv_blocks(x_raw)
----------@@ -92,15 +88,16 @@ class Model(nn.Module):
----------         x_raw = self.w_raw_in(x_raw)
----------         x = x_raw
---------- 
----------+        #Padding Target Mask and attention mask
----------+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
----------+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
----------+
----------         #Embedding and positional encoding of tgt
----------         tgt=self.embedding_tgt(y)
----------         tgt=self.pos_encoder(tgt)
-----------        #Padding Target
-----------        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).transpose(0,1).to(self.device)
-----------        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, tgt.shape[1]).to(self.device)
-----------
----------+        
----------         x = x.transpose(0,1) # put time first
-----------        tgt = tgt.transpose(0,1) # put channel after
----------+        tgt = tgt.transpose(0,1) # put sequence_length first
----------         x_encoder = self.transformerEncoder(x)
----------         x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
---------- 
----------diff --git a/data_utils.py b/data_utils.py
----------index 8b05213..33a96f7 100644
------------- a/data_utils.py
----------+++ b/data_utils.py
----------@@ -167,17 +167,6 @@ def combine_fixed_length(tensor_list, length):
----------     n = total_length // length
----------     return tensor.view(n, length, *tensor.size()[1:])
---------- 
-----------def combine_fixed_length_tgt(tensor_list, n_batch):
-----------    total_length = sum(t.size(0) for t in tensor_list)
-----------    tensor_list = list(tensor_list) # copy
-----------    if total_length % n_batch != 0:
-----------        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------        total_length += pad_length
-----------    tensor = torch.cat(tensor_list, 0)
-----------    length = total_length // n_batch
-----------    return tensor.view(n_batch, length, *tensor.size()[1:])
-----------
---------- def decollate_tensor(tensor, lengths):
----------     b, s, d = tensor.size()
----------     tensor = tensor.view(b*s, d)
----------@@ -253,6 +242,61 @@ def read_phonemes(textgrid_fname, max_len=None):
----------     return phone_ids
---------- 
---------- class TextTransform(object):
----------+    '''
----------+    def __init__(self, pad_token="<pad>", blank_token='_', eos_token='<eos>', sos_token='<sos>'):
----------+        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----------+        self.id_to_string = {}
----------+        self.string_to_id = {}
----------+        
----------+        # add the default pad token
----------+        self.id_to_string[0] = pad_token
----------+        self.string_to_id[pad_token] = 0
----------+        
----------+        # add the default unknown token
----------+        self.id_to_string[1] = blank_token
----------+        self.string_to_id[blank_token] = 1
----------+        
----------+        # add the default unknown token
----------+        self.id_to_string[2] = eos_token
----------+        self.string_to_id[eos_token] = 2   
----------+
----------+        # add the default unknown token
----------+        self.id_to_string[3] = sos_token
----------+        self.string_to_id[sos_token] = 3
----------+
----------+        # shortcut access
----------+        self.pad_id = 0
----------+        self.blank_id = 1
----------+        self.eos_id = 2
----------+        self.sos_id = 3
----------+
----------+    def __len__(self):
----------+        return len('<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' ')
----------+    
----------+    def clean_text(self, text):
----------+        text = unidecode(text)
----------+        text = self.transformation(text)
----------+        return text
----------+    
----------+    def add_new_words(self, text):
----------+        text = self.clean_text(text)
----------+        for c in text:
----------+            if c not in self.string_to_id:
----------+                self.string_to_id[c] = len(self.string_to_id)
----------+                self.id_to_string[len(self.id_to_string)] = c
----------+    
----------+    def get_labels(self):
----------+        return  '<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' '
----------+            
----------+    def text_to_int(self, text):
----------+        text = self.clean_text(text)
----------+        return [self.string_to_id[c] for c in text]
----------+
----------+    def int_to_text(self, ints):
----------+        return ''.join(self.id_to_string[i] for i in ints)
----------+    '''
----------+    
----------+
----------     def __init__(self):
----------         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
----------         self.chars = string.ascii_lowercase+string.digits+' '
----------@@ -269,3 +313,4 @@ class TextTransform(object):
---------- 
----------     def int_to_text(self, ints):
----------         return ''.join(self.chars[i] for i in ints)
----------+    
----------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
----------index 96f39d1..76c276d 100644
------------- a/models/recognition_model/log.txt
----------+++ b/models/recognition_model/log.txt
----------@@ -1,4433 +1,2 @@
-----------be71135adc89793578f304adb405cea80a5b2b9a
----------+897f691543960d43991a13846d6b09c92f89eb8c
---------- 
-----------diff --git a/architecture.py b/architecture.py
-----------index 94d0de0..91fd2da 100644
-------------- a/architecture.py
-----------+++ b/architecture.py
-----------@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------         return F.relu(x + res)
----------- 
----------- class Model(nn.Module):
------------    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------+    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-----------         super().__init__()
----------- 
-----------         self.conv_blocks = nn.Sequential(
-----------@@ -51,7 +51,7 @@ class Model(nn.Module):
-----------         )
-----------         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
----------- 
------------        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)
-----------         self.pos_encoder = PositionalEncoding(FLAGS.model_size)
----------- 
-----------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------@@ -61,25 +61,24 @@ class Model(nn.Module):
-----------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
----------- 
-----------         self.has_aux_loss = has_aux_loss
-----------+        if self.has_aux_loss:
-----------+            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-----------         self.device=device
----------- 
-----------+
-----------     def create_src_padding_mask(self, src):
-----------         # input src of shape ()
------------        src_padding_mask = src.transpose(1, 0) == 0
-----------+        src_padding_mask = src == 0
-----------         return src_padding_mask
----------- 
-----------     def create_tgt_padding_mask(self, tgt):
-----------         # input tgt of shape ()
------------        tgt_padding_mask = tgt.transpose(1, 0) == 0
-----------+        tgt_padding_mask = tgt == 0
-----------         return tgt_padding_mask
-----------     
------------    def forward(self, x_feat, x_raw, y, session_ids):
-----------+    def forward(self, x_raw, y):
-----------         # x shape is (batch, time, electrode)
-----------         # y shape is (batch, sequence_length)
------------        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
------------        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
------------        memory_key_padding_mask = src_key_padding_mask
------------        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
----------- 
-----------         if self.training:
-----------             r = random.randrange(8)
-----------@@ -96,17 +95,20 @@ class Model(nn.Module):
-----------         #Embedding and positional encoding of tgt
-----------         tgt=self.embedding_tgt(y)
-----------         tgt=self.pos_encoder(tgt)
-----------+        #Padding Target
-----------+        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).transpose(0,1).to(self.device)
-----------+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, tgt.shape[1]).to(self.device)
----------- 
-----------         x = x.transpose(0,1) # put time first
-----------         tgt = tgt.transpose(0,1) # put channel after
------------        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
------------        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-----------+        x_encoder = self.transformerEncoder(x)
-----------+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----------- 
-----------         x_encoder = x_encoder.transpose(0,1)
-----------         x_decoder = x_decoder.transpose(0,1)
----------- 
-----------         if self.has_aux_loss:
------------            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------+            return self.w_aux(x_encoder), self.w_out(x_decoder)
-----------         else:
-----------             return self.w_out(x)
----------- 
-----------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------index 2243f2d..342fccd 100644
-------------- a/models/recognition_model/log.txt
-----------+++ b/models/recognition_model/log.txt
-----------@@ -1,845 +1,2 @@
------------dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------+be71135adc89793578f304adb405cea80a5b2b9a
----------- 
------------diff --git a/architecture.py b/architecture.py
------------index 2413a8a..94d0de0 100644
--------------- a/architecture.py
------------+++ b/architecture.py
------------@@ -4,7 +4,7 @@ import torch
------------ from torch import nn
------------ import torch.nn.functional as F
------------ 
-------------from transformer import TransformerEncoderLayer, TransformerDecoderLayer
------------+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
------------ 
------------ from absl import flags
------------ FLAGS = flags.FLAGS
------------@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
------------         return F.relu(x + res)
------------ 
------------ class Model(nn.Module):
-------------    def __init__(self, num_features, num_outs, has_aux_loss=False):
------------+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
------------         super().__init__()
------------ 
------------         self.conv_blocks = nn.Sequential(
------------@@ -52,6 +52,7 @@ class Model(nn.Module):
------------         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
------------ 
------------         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
------------+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
------------ 
------------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
------------         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
------------@@ -60,9 +61,25 @@ class Model(nn.Module):
------------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
------------ 
------------         self.has_aux_loss = has_aux_loss
-------------
-------------    def forward(self, x_feat, x_raw, y,session_ids):
------------+        self.device=device
------------+
------------+    def create_src_padding_mask(self, src):
------------+        # input src of shape ()
------------+        src_padding_mask = src.transpose(1, 0) == 0
------------+        return src_padding_mask
------------+
------------+    def create_tgt_padding_mask(self, tgt):
------------+        # input tgt of shape ()
------------+        tgt_padding_mask = tgt.transpose(1, 0) == 0
------------+        return tgt_padding_mask
------------+    
------------+    def forward(self, x_feat, x_raw, y, session_ids):
------------         # x shape is (batch, time, electrode)
------------+        # y shape is (batch, sequence_length)
------------+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
------------+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
------------+        memory_key_padding_mask = src_key_padding_mask
------------+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
------------ 
------------         if self.training:
------------             r = random.randrange(8)
------------@@ -74,14 +91,16 @@ class Model(nn.Module):
------------         x_raw = self.conv_blocks(x_raw)
------------         x_raw = x_raw.transpose(1,2)
------------         x_raw = self.w_raw_in(x_raw)
-------------
------------         x = x_raw
------------+
------------+        #Embedding and positional encoding of tgt
------------         tgt=self.embedding_tgt(y)
------------+        tgt=self.pos_encoder(tgt)
------------ 
------------         x = x.transpose(0,1) # put time first
------------         tgt = tgt.transpose(0,1) # put channel after
-------------        x_encoder = self.transformerEncoder(x)
-------------        x_decoder = self.transformerDecoder(tgt, x_encoder)
------------+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
------------+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
------------ 
------------         x_encoder = x_encoder.transpose(0,1)
------------         x_decoder = x_decoder.transpose(0,1)
------------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
------------index 53839a0..1343d7d 100644
--------------- a/models/recognition_model/log.txt
------------+++ b/models/recognition_model/log.txt
------------@@ -1,639 +1,2 @@
-------------2fa943cd85263a152b6be80d502eda27932ebb27
------------+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
------------ 
-------------diff --git a/architecture.py b/architecture.py
-------------index a8c70f3..2413a8a 100644
---------------- a/architecture.py
-------------+++ b/architecture.py
-------------@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-------------         return F.relu(x + res)
------------- 
------------- class Model(nn.Module):
--------------    def __init__(self, num_features, num_outs, num_aux_outs=None):
-------------+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-------------         super().__init__()
------------- 
-------------         self.conv_blocks = nn.Sequential(
-------------@@ -59,9 +59,7 @@ class Model(nn.Module):
-------------         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-------------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
------------- 
--------------        self.has_aux_out = num_aux_outs is not None
--------------        if self.has_aux_out:
--------------            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-------------+        self.has_aux_loss = has_aux_loss
------------- 
-------------     def forward(self, x_feat, x_raw, y,session_ids):
-------------         # x shape is (batch, time, electrode)
-------------@@ -82,12 +80,14 @@ class Model(nn.Module):
------------- 
-------------         x = x.transpose(0,1) # put time first
-------------         tgt = tgt.transpose(0,1) # put channel after
--------------        x = self.transformerEncoder(x)
--------------        x = self.transformerDecoder(tgt, x)
--------------        x = x.transpose(0,1)
-------------+        x_encoder = self.transformerEncoder(x)
-------------+        x_decoder = self.transformerDecoder(tgt, x_encoder)
------------- 
--------------        if self.has_aux_out:
--------------            return self.w_out(x), self.w_aux(x)
-------------+        x_encoder = x_encoder.transpose(0,1)
-------------+        x_decoder = x_decoder.transpose(0,1)
-------------+
-------------+        if self.has_aux_loss:
-------------+            return self.w_out(x_encoder), self.w_out(x_decoder)
-------------         else:
-------------             return self.w_out(x)
------------- 
-------------diff --git a/data_utils.py b/data_utils.py
-------------index e2632e8..8b05213 100644
---------------- a/data_utils.py
-------------+++ b/data_utils.py
-------------@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
------------- 
------------- def combine_fixed_length_tgt(tensor_list, n_batch):
-------------     total_length = sum(t.size(0) for t in tensor_list)
-------------+    tensor_list = list(tensor_list) # copy
-------------     if total_length % n_batch != 0:
-------------         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------------        tensor_list = list(tensor_list) # copy
-------------         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-------------         total_length += pad_length
-------------     tensor = torch.cat(tensor_list, 0)
-------------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-------------index b8f7791..617dd85 100644
---------------- a/models/recognition_model/log.txt
-------------+++ b/models/recognition_model/log.txt
-------------@@ -1,480 +1,2 @@
--------------902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-------------+2fa943cd85263a152b6be80d502eda27932ebb27
------------- 
--------------diff --git a/architecture.py b/architecture.py
--------------index d6e99b4..a8c70f3 100644
----------------- a/architecture.py
--------------+++ b/architecture.py
--------------@@ -54,7 +54,7 @@ class Model(nn.Module):
--------------         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-------------- 
--------------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
---------------        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------------+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------------         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------------         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------------diff --git a/data_utils.py b/data_utils.py
--------------index e4ac852..e2632e8 100644
----------------- a/data_utils.py
--------------+++ b/data_utils.py
--------------@@ -1,3 +1,4 @@
--------------+import math
-------------- import string
-------------- 
-------------- import numpy as np
--------------@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
--------------     n = total_length // length
--------------     return tensor.view(n, length, *tensor.size()[1:])
-------------- 
--------------+def combine_fixed_length_tgt(tensor_list, n_batch):
--------------+    total_length = sum(t.size(0) for t in tensor_list)
--------------+    if total_length % n_batch != 0:
--------------+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------------+        tensor_list = list(tensor_list) # copy
--------------+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------------+        total_length += pad_length
--------------+    tensor = torch.cat(tensor_list, 0)
--------------+    length = total_length // n_batch
--------------+    return tensor.view(n_batch, length, *tensor.size()[1:])
--------------+
-------------- def decollate_tensor(tensor, lengths):
--------------     b, s, d = tensor.size()
--------------     tensor = tensor.view(b*s, d)
--------------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------------index e890f0f..1ee3421 100644
----------------- a/models/recognition_model/log.txt
--------------+++ b/models/recognition_model/log.txt
--------------@@ -1,265 +1,2 @@
---------------031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------------+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-------------- 
---------------diff --git a/architecture.py b/architecture.py
---------------index b22af61..d6e99b4 100644
------------------ a/architecture.py
---------------+++ b/architecture.py
---------------@@ -51,6 +51,8 @@ class Model(nn.Module):
---------------         )
---------------         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------------- 
---------------+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
---------------+
---------------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
---------------         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
---------------         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
---------------@@ -61,7 +63,7 @@ class Model(nn.Module):
---------------         if self.has_aux_out:
---------------             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------------- 
----------------    def forward(self, x_feat, x_raw, session_ids):
---------------+    def forward(self, x_feat, x_raw, y,session_ids):
---------------         # x shape is (batch, time, electrode)
--------------- 
---------------         if self.training:
---------------@@ -76,10 +78,12 @@ class Model(nn.Module):
---------------         x_raw = self.w_raw_in(x_raw)
--------------- 
---------------         x = x_raw
---------------+        tgt=self.embedding_tgt(y)
--------------- 
---------------         x = x.transpose(0,1) # put time first
---------------+        tgt = tgt.transpose(0,1) # put channel after
---------------         x = self.transformerEncoder(x)
----------------        x = self.transformerDecoder(x) #TODO I need the target EMG
---------------+        x = self.transformerDecoder(tgt, x)
---------------         x = x.transpose(0,1)
--------------- 
---------------         if self.has_aux_out:
---------------diff --git a/data_utils.py b/data_utils.py
---------------index 11d4805..e4ac852 100644
------------------ a/data_utils.py
---------------+++ b/data_utils.py
---------------@@ -244,6 +244,7 @@ class TextTransform(object):
---------------     def __init__(self):
---------------         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
---------------         self.chars = string.ascii_lowercase+string.digits+' '
---------------+        self.vocabulary_size=len(self.chars)
--------------- 
---------------     def clean_text(self, text):
---------------         text = unidecode(text)
---------------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
---------------index fbc0abb..400061a 100644
------------------ a/models/recognition_model/log.txt
---------------+++ b/models/recognition_model/log.txt
---------------@@ -1,188 +1,2 @@
----------------57f8139449dd9286c2203ec2eca118a550638a7c
---------------+031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------------- 
----------------diff --git a/architecture.py b/architecture.py
----------------index 4fc3793..b22af61 100644
------------------- a/architecture.py
----------------+++ b/architecture.py
----------------@@ -4,7 +4,7 @@ import torch
---------------- from torch import nn
---------------- import torch.nn.functional as F
---------------- 
-----------------from transformer import TransformerEncoderLayer
----------------+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
---------------- 
---------------- from absl import flags
---------------- FLAGS = flags.FLAGS
----------------@@ -52,7 +52,9 @@ class Model(nn.Module):
----------------         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
---------------- 
----------------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
----------------+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----------------+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
----------------+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
----------------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
---------------- 
----------------         self.has_aux_out = num_aux_outs is not None
----------------@@ -76,7 +78,8 @@ class Model(nn.Module):
----------------         x = x_raw
---------------- 
----------------         x = x.transpose(0,1) # put time first
-----------------        x = self.transformer(x)
----------------+        x = self.transformerEncoder(x)
----------------+        x = self.transformerDecoder(x) #TODO I need the target EMG
----------------         x = x.transpose(0,1)
---------------- 
----------------         if self.has_aux_out:
----------------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
----------------index 571de9d..8563980 100644
------------------- a/models/recognition_model/log.txt
----------------+++ b/models/recognition_model/log.txt
----------------@@ -1,5 +1,2 @@
----------------+57f8139449dd9286c2203ec2eca118a550638a7c
---------------- 
-----------------
-----------------['recognition_model.py', '--output_directory', './models/recognition_model/']
-----------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------------train / dev split: 8055 200
----------------diff --git a/output/log.txt b/output/log.txt
----------------index ae42364..1d2cd8e 100644
------------------- a/output/log.txt
----------------+++ b/output/log.txt
----------------@@ -1,3 +1,13 @@
----------------+57f8139449dd9286c2203ec2eca118a550638a7c
---------------- 
----------------+diff --git a/output/log.txt b/output/log.txt
----------------+index ae42364..8563980 100644
----------------+--- a/output/log.txt
----------------++++ b/output/log.txt
----------------+@@ -1,3 +1,2 @@
----------------++57f8139449dd9286c2203ec2eca118a550638a7c
----------------+ 
----------------+-
----------------+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
---------------- 
---------------- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
----------------diff --git a/transformer.py b/transformer.py
----------------index 6743588..ac131be 100644
------------------- a/transformer.py
----------------+++ b/transformer.py
----------------@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
----------------         Shape:
----------------             see the docs in Transformer class.
----------------         """
-----------------        src2 = self.self_attn(src)
----------------+        src2 = self.self_attn(src, src, src)
----------------         src = src + self.dropout1(src2)
----------------         src = self.norm1(src)
----------------         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
----------------@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
----------------         src = self.norm2(src)
----------------         return src
---------------- 
----------------+class TransformerDecoderLayer(nn.Module):
----------------+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
----------------+    This standard decoder layer is based on the paper "Attention Is All You Need".
----------------+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
----------------+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
----------------+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
----------------+    in a different way during application.
----------------+
----------------+    Args:
----------------+        d_model: the number of expected features in the input (required).
----------------+        nhead: the number of heads in the multiheadattention models (required).
----------------+        dim_feedforward: the dimension of the feedforward network model (default=2048).
----------------+        dropout: the dropout value (default=0.1).
----------------+        activation: the activation function of the intermediate layer, can be a string
----------------+            ("relu" or "gelu") or a unary callable. Default: relu
----------------+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
----------------+        batch_first: If ``True``, then the input and output tensors are provided
----------------+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
----------------+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
----------------+            attention and feedforward operations, respectively. Otherwise it's done after.
----------------+            Default: ``False`` (after).
----------------+
----------------+    Examples::
----------------+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
----------------+        >>> memory = torch.rand(10, 32, 512)
----------------+        >>> tgt = torch.rand(20, 32, 512)
----------------+        >>> out = decoder_layer(tgt, memory)
----------------+    """
----------------+    # Adapted from pytorch source
----------------+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
----------------+        super(TransformerDecoderLayer, self).__init__()
----------------+        #Attention Mechanism
----------------+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
----------------+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
----------------+        # Implementation of Feedforward model
----------------+        self.linear1 = nn.Linear(d_model, dim_feedforward)
----------------+        self.dropout = nn.Dropout(dropout)
----------------+        self.linear2 = nn.Linear(dim_feedforward, d_model)
----------------+        #Normalization Layer and Dropout Layer
----------------+        self.norm1 = nn.LayerNorm(d_model)
----------------+        self.norm2 = nn.LayerNorm(d_model)
----------------+        self.norm3 = nn.LayerNorm(d_model)
----------------+        self.dropout1 = nn.Dropout(dropout)
----------------+        self.dropout2 = nn.Dropout(dropout)
----------------+        self.dropout3 = nn.Dropout(dropout)
----------------+        #Activation Function
----------------+        self.activation = nn.ReLU()
----------------+    
----------------+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
----------------+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
----------------+        r"""Pass the input through the encoder layer.
----------------+
----------------+        Args:
----------------+            tgt: the sequence to the decoder layer (required).
----------------+            memory: the sequence from the last layer of the encoder (required).
----------------+            tgt_mask: the mask for the tgt sequence (optional).
----------------+            memory_mask: the mask for the memory sequence (optional).
----------------+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
----------------+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
----------------+
----------------+        Shape:
----------------+            see the docs in Transformer class.
----------------+        """
----------------+        tgt2 = self.self_attn(tgt, tgt, tgt)
----------------+        tgt = tgt + self.dropout1(tgt2)
----------------+        tgt = self.norm1(tgt)
----------------+
----------------+        tgt2=self.multihead_attn(tgt, memory, memory)
----------------+        tgt = tgt + self.dropout1(tgt2)
----------------+        tgt = self.norm1(tgt)
----------------+
----------------+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
----------------+        tgt = tgt + self.dropout2(tgt2)
----------------+        tgt = self.norm2(tgt)
----------------+        return tgt
----------------+    
----------------+
---------------- class MultiHeadAttention(nn.Module):
----------------   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
----------------     super().__init__()
----------------@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
----------------     else:
----------------         self.relative_positional = None
---------------- 
-----------------  def forward(self, x):
----------------+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
----------------     """Runs the multi-head self-attention layer.
---------------- 
----------------     Args:
----------------@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
----------------       A single tensor containing the output from this layer
----------------     """
---------------- 
-----------------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-----------------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-----------------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
----------------+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
----------------+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
----------------+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
----------------     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
---------------- 
----------------     if self.relative_positional is not None:
----------------
----------------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
----------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----------------train / dev split: 8055 200
---------------diff --git a/recognition_model.py b/recognition_model.py
---------------index dea6d47..a46dff0 100644
------------------ a/recognition_model.py
---------------+++ b/recognition_model.py
---------------@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
--------------- 
---------------             X = combine_fixed_length(example['emg'], 200).to(device)
---------------             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
---------------+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---------------+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
---------------             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------------- 
----------------            pred = model(X, X_raw, sess)
---------------+            pred = model(X, X_raw, tgt, sess)
---------------             pred = F.log_softmax(pred, 2)
--------------- 
---------------             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
---------------
---------------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
---------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
---------------train / dev split: 8055 200
--------------diff --git a/recognition_model.py b/recognition_model.py
--------------index a46dff0..8fd300c 100644
----------------- a/recognition_model.py
--------------+++ b/recognition_model.py
--------------@@ -6,6 +6,7 @@ import subprocess
-------------- from ctcdecode import CTCBeamDecoder
-------------- import jiwer
-------------- import random
--------------+from torch.utils.tensorboard import SummaryWriter
-------------- 
-------------- import torch
-------------- from torch import nn
--------------@@ -13,7 +14,7 @@ import torch.nn.functional as F
-------------- 
-------------- from read_emg import EMGDataset, SizeAwareSampler
-------------- from architecture import Model
---------------from data_utils import combine_fixed_length, decollate_tensor
--------------+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-------------- from transformer import TransformerEncoderLayer
-------------- 
-------------- from absl import flags
--------------@@ -62,17 +63,21 @@ def test(model, testset, device):
--------------     return jiwer.wer(references, predictions)
-------------- 
-------------- 
---------------def train_model(trainset, devset, device, n_epochs=200):
---------------    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
---------------
--------------+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------------+    #Define Dataloader
--------------+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------------+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-------------- 
--------------+    #Define model and loss function
--------------     n_chars = len(devset.text_transform.chars)
--------------     model = Model(devset.num_features, n_chars+1).to(device)
--------------+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-------------- 
--------------     if FLAGS.start_training_from is not None:
--------------         state_dict = torch.load(FLAGS.start_training_from)
--------------         model.load_state_dict(state_dict, strict=False)
-------------- 
--------------+    #Define optimizer and scheduler for the learning rate
--------------     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
--------------     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-------------- 
--------------@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
--------------             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-------------- 
--------------     batch_idx = 0
--------------+    train_loss= 0
--------------+    eval_loss = 0
--------------     optim.zero_grad()
--------------     for epoch_idx in range(n_epochs):
--------------+        model.train()
--------------         losses = []
---------------        for example in dataloader:
--------------+        for example in dataloader_training:
--------------             schedule_lr(batch_idx)
-------------- 
--------------+            #Preprosessing of the input and target for the model
--------------             X = combine_fixed_length(example['emg'], 200).to(device)
--------------             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
---------------            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---------------            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------------             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------------+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-------------- 
--------------+            #Shifting target for input decoder and loss
--------------+            tgt= y[:,:-1]
--------------+            target= y[:,1:]
--------------+
--------------+            #Prediction
--------------             pred = model(X, X_raw, tgt, sess)
---------------            pred = F.log_softmax(pred, 2)
-------------- 
---------------            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
---------------            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---------------            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------------+            #Primary Loss
--------------+            pred=pred.permute(0,2,1)
--------------+            loss = loss_fn(pred, target)
--------------+
--------------+            #Auxiliary Loss
--------------+            #pred = F.log_softmax(pred, 2)
--------------+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------------+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------------+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------------             losses.append(loss.item())
--------------+            train_loss += loss.item()
-------------- 
--------------             loss.backward()
--------------             if (batch_idx+1) % 2 == 0:
--------------                 optim.step()
--------------                 optim.zero_grad()
-------------- 
--------------+            #Report plots in tensorboard
--------------+            if batch_idx % report_every == report_every - 2:     
--------------+                #Evaluation
--------------+                model.eval()
--------------+                with torch.no_grad():
--------------+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------------+                        X_raw = example['raw_emg'].to(device)
--------------+                        sess = example['session_ids'].to(device)
--------------+                        y = example['text_int'].to(device)
--------------+
--------------+                        #Shifting target for input decoder and loss
--------------+                        tgt= y[:,:-1]
--------------+                        target= y[:,1:]
--------------+
--------------+                        #Prediction without the 197-th batch because of missing label
--------------+                        if idx != 197:
--------------+                            pred = model(X, X_raw, tgt, sess)
--------------+                            #Primary Loss
--------------+                            pred=pred.permute(0,2,1)
--------------+                            loss = loss_fn(pred, target)
--------------+                            eval_loss += loss.item()
--------------+
--------------+                #Writing on tensorboard
--------------+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------------+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------------+                train_loss= 0
--------------+                eval_loss= 0
--------------+
--------------+            #Increment counter        
--------------             batch_idx += 1
---------------        train_loss = np.mean(losses)
--------------+
--------------+        #Testing and change learning rate
--------------         val = test(model, devset, device)
--------------+        writer.add_scalar('WER/Evaluation',val, batch_idx)
--------------         lr_sched.step()
--------------+    
--------------+        #Logging
--------------+        train_loss = np.mean(losses)
--------------         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
--------------         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-------------- 
--------------@@ -148,8 +201,9 @@ def main():
--------------     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-------------- 
--------------     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
--------------+    writer = SummaryWriter(log_dir="./content/runs")
-------------- 
---------------    model = train_model(trainset, devset, device)
--------------+    model = train_model(trainset, devset ,device, writer)
-------------- 
-------------- if __name__ == '__main__':
--------------     FLAGS(sys.argv)
--------------diff --git a/transformer.py b/transformer.py
--------------index ac131be..51e1f2e 100644
----------------- a/transformer.py
--------------+++ b/transformer.py
--------------@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
--------------     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------------     self.d_qkv = d_qkv
-------------- 
--------------+    #self.kdim = kdim if kdim is not None else embed_dim
--------------+    #self.vdim = vdim if vdim is not None else embed_dim
--------------+
--------------     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------------     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------------     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------------
--------------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------------train / dev split: 8055 200
-------------diff --git a/recognition_model.py b/recognition_model.py
-------------index fde5a40..6d5143b 100644
---------------- a/recognition_model.py
-------------+++ b/recognition_model.py
-------------@@ -63,14 +63,14 @@ def test(model, testset, device):
-------------     return jiwer.wer(references, predictions)
------------- 
------------- 
--------------def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-------------+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-------------     #Define Dataloader
-------------     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-------------     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
------------- 
-------------     #Define model and loss function
-------------     n_chars = len(devset.text_transform.chars)
--------------    model = Model(devset.num_features, n_chars+1).to(device)
-------------+    model = Model(devset.num_features, n_chars+1, True).to(device)
-------------     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
------------- 
-------------     if FLAGS.start_training_from is not None:
-------------@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-------------             target= y[:,1:]
------------- 
-------------             #Prediction
--------------            pred = model(X, X_raw, tgt, sess)
-------------+            out_enc, out_dec = model(X, X_raw, tgt, sess)
------------- 
-------------             #Primary Loss
--------------            pred=pred.permute(0,2,1)
--------------            loss = loss_fn(pred, target)
-------------+            out_dec=out_dec.permute(0,2,1)
-------------+            loss_dec = loss_fn(out_dec, target)
------------- 
-------------             #Auxiliary Loss
--------------            #pred = F.log_softmax(pred, 2)
--------------            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------------            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------------            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-------------+            out_enc = F.log_softmax(out_enc, 2)
-------------+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-------------+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-------------+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-------------+
-------------+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-------------             losses.append(loss.item())
-------------             train_loss += loss.item()
------------- 
-------------@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-------------             if (batch_idx+1) % 2 == 0:
-------------                 optim.step()
-------------                 optim.zero_grad()
--------------
--------------            if batch_idx % report_every == report_every - 2:     
-------------+            
-------------+            if False:
-------------+            #if batch_idx % report_every == report_every - 2:     
-------------                 #Evaluation
-------------                 model.eval()
-------------                 with torch.no_grad():
-------------                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------------                        X_raw = example['raw_emg'].to(device)
--------------                        sess = example['session_ids'].to(device)
--------------                        y = example['text_int'].to(device)
-------------+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-------------+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-------------+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
------------- 
-------------                         #Shifting target for input decoder and loss
-------------                         tgt= y[:,:-1]
-------------                         target= y[:,1:]
------------- 
-------------+                        print(idx)
-------------+
-------------                         #Prediction without the 197-th batch because of missing label
--------------                        if idx != 197:
-------------+                        if idx != 181:
-------------                             pred = model(X, X_raw, tgt, sess)
-------------                             #Primary Loss
-------------                             pred=pred.permute(0,2,1)
-------------@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
------------- 
-------------             #Increment counter        
-------------             batch_idx += 1
-------------+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
------------- 
-------------         #Testing and change learning rate
-------------         val = test(model, devset, device)
-------------
-------------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-------------train / dev split: 8055 200
------------diff --git a/recognition_model.py b/recognition_model.py
------------index 30c5ff2..2672d45 100644
--------------- a/recognition_model.py
------------+++ b/recognition_model.py
------------@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
------------ 
------------     #Define model and loss function
------------     n_chars = len(devset.text_transform.chars)
-------------    model = Model(devset.num_features, n_chars+1, True).to(device)
------------+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
------------     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
------------ 
------------     if FLAGS.start_training_from is not None:
------------diff --git a/transformer.py b/transformer.py
------------index 51e1f2e..c125841 100644
--------------- a/transformer.py
------------+++ b/transformer.py
------------@@ -1,3 +1,4 @@
------------+import math
------------ from typing import Optional
------------ 
------------ import torch
------------@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
------------         Shape:
------------             see the docs in Transformer class.
------------         """
-------------        src2 = self.self_attn(src, src, src)
------------+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
------------         src = src + self.dropout1(src2)
------------         src = self.norm1(src)
------------         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
------------@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
------------         Shape:
------------             see the docs in Transformer class.
------------         """
-------------        tgt2 = self.self_attn(tgt, tgt, tgt)
------------+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
------------+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
------------         tgt = tgt + self.dropout1(tgt2)
------------         tgt = self.norm1(tgt)
------------ 
-------------        tgt2=self.multihead_attn(tgt, memory, memory)
------------+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
------------         tgt = tgt + self.dropout1(tgt2)
------------         tgt = self.norm1(tgt)
------------ 
------------@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
------------     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
------------     self.d_qkv = d_qkv
------------ 
-------------    #self.kdim = kdim if kdim is not None else embed_dim
-------------    #self.vdim = vdim if vdim is not None else embed_dim
-------------
------------     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
------------     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
------------     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
------------@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
------------     else:
------------         self.relative_positional = None
------------ 
-------------  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
------------+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
------------     """Runs the multi-head self-attention layer.
------------ 
------------     Args:
------------@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
------------     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
------------     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
------------ 
------------+    if attn_mask is not None:
------------+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
------------+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
------------+
------------     if self.relative_positional is not None:
------------         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
------------         l,b,h,d = q_pos.size()
------------@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
------------             x = x.transpose(0, 1)
------------             x = x.contiguous().view(bsz_heads, length+1, length)
------------             return x[:, 1:, :]
------------+        
------------+
------------+########
------------+# Taken from:
------------+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
------------+# or also here:
------------+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
------------+class PositionalEncoding(nn.Module):
------------+
------------+    def __init__(self, d_model, dropout=0.0, max_len=5000):
------------+        super(PositionalEncoding, self).__init__()
------------+        self.dropout = nn.Dropout(p=dropout)
------------+        self.max_len = max_len
------------+
------------+        pe = torch.zeros(max_len, d_model)
------------+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
------------+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
------------+                             * (-math.log(10000.0) / d_model))
------------+        pe[:, 0::2] = torch.sin(position * div_term)
------------+        pe[:, 1::2] = torch.cos(position * div_term)
------------+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
------------+        self.register_buffer('pe', pe)  # Will not be trained.
------------+
------------+    def forward(self, x):
------------+        """Inputs of forward function
------------+        Args:
------------+            x: the sequence fed to the positional encoder model (required).
------------+        Shape:
------------+            x: [sequence length, batch size, embed dim]
------------+            output: [sequence length, batch size, embed dim]
------------+        """
------------+        assert x.size(0) < self.max_len, (
------------+            f"Too long sequence length: increase `max_len` of pos encoding")
------------+        # shape of x (len, B, dim)
------------+        x = x + self.pe[:x.size(0), :]
------------+        return self.dropout(x)
------------
------------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
------------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
------------train / dev split: 8055 200
-----------diff --git a/output/log.txt b/output/log.txt
-----------index 1d2cd8e..42b3343 100644
-------------- a/output/log.txt
-----------+++ b/output/log.txt
-----------@@ -1,13 +1,3339 @@
------------57f8139449dd9286c2203ec2eca118a550638a7c
-----------+be71135adc89793578f304adb405cea80a5b2b9a
----------- 
-----------+diff --git a/architecture.py b/architecture.py
-----------+index 94d0de0..222c88e 100644
-----------+--- a/architecture.py
-----------++++ b/architecture.py
-----------+@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------+         return F.relu(x + res)
-----------+ 
-----------+ class Model(nn.Module):
-----------+-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-----------+         super().__init__()
-----------+ 
-----------+         self.conv_blocks = nn.Sequential(
-----------+@@ -61,8 +61,11 @@ class Model(nn.Module):
-----------+         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+ 
-----------+         self.has_aux_loss = has_aux_loss
-----------++        if self.has_aux_loss:
-----------++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-----------+         self.device=device
-----------+ 
-----------++
-----------+     def create_src_padding_mask(self, src):
-----------+         # input src of shape ()
-----------+         src_padding_mask = src.transpose(1, 0) == 0
-----------+@@ -106,7 +109,7 @@ class Model(nn.Module):
-----------+         x_decoder = x_decoder.transpose(0,1)
-----------+ 
-----------+         if self.has_aux_loss:
-----------+-            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------++            return self.w_aux(x_encoder), self.w_out(x_decoder)
-----------+         else:
-----------+             return self.w_out(x)
-----------+ 
-----------+diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+index 2243f2d..a8cb5ee 100644
-----------+--- a/models/recognition_model/log.txt
-----------++++ b/models/recognition_model/log.txt
-----------+@@ -1,844 +1,2577 @@
-----------+-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------++be71135adc89793578f304adb405cea80a5b2b9a
-----------+ 
-----------+ diff --git a/architecture.py b/architecture.py
-----------+-index 2413a8a..94d0de0 100644
-----------++index 94d0de0..7170c48 100644
-----------+ --- a/architecture.py
-----------+ +++ b/architecture.py
-----------+-@@ -4,7 +4,7 @@ import torch
-----------+- from torch import nn
-----------+- import torch.nn.functional as F
-----------+- 
-----------+--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------+-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-----------+- 
-----------+- from absl import flags
-----------+- FLAGS = flags.FLAGS
-----------+ @@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------+          return F.relu(x + res)
-----------+  
-----------+  class Model(nn.Module):
-----------+--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------+-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------++-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------+++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-----------+          super().__init__()
-----------+  
-----------+          self.conv_blocks = nn.Sequential(
-----------+-@@ -52,6 +52,7 @@ class Model(nn.Module):
-----------+-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+- 
-----------+-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-----------+- 
-----------+-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+-@@ -60,9 +61,25 @@ class Model(nn.Module):
-----------++@@ -61,8 +61,11 @@ class Model(nn.Module):
-----------+          self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+  
-----------+          self.has_aux_loss = has_aux_loss
-----------+--
-----------+--    def forward(self, x_feat, x_raw, y,session_ids):
-----------+-+        self.device=device
-----------+-+
-----------+-+    def create_src_padding_mask(self, src):
-----------+-+        # input src of shape ()
-----------+-+        src_padding_mask = src.transpose(1, 0) == 0
-----------+-+        return src_padding_mask
-----------+-+
-----------+-+    def create_tgt_padding_mask(self, tgt):
-----------+-+        # input tgt of shape ()
-----------+-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-----------+-+        return tgt_padding_mask
-----------+-+    
-----------+-+    def forward(self, x_feat, x_raw, y, session_ids):
-----------+-         # x shape is (batch, time, electrode)
-----------+-+        # y shape is (batch, sequence_length)
-----------+-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-----------+-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-----------+-+        memory_key_padding_mask = src_key_padding_mask
-----------+-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-----------+++        if self.has_aux_out:
-----------+++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-----------++         self.device=device
-----------+  
-----------+-         if self.training:
-----------+-             r = random.randrange(8)
-----------+-@@ -74,14 +91,16 @@ class Model(nn.Module):
-----------+-         x_raw = self.conv_blocks(x_raw)
-----------+-         x_raw = x_raw.transpose(1,2)
-----------+-         x_raw = self.w_raw_in(x_raw)
-----------+--
-----------+-         x = x_raw
-----------+ +
-----------+-+        #Embedding and positional encoding of tgt
-----------+-         tgt=self.embedding_tgt(y)
-----------+-+        tgt=self.pos_encoder(tgt)
-----------++     def create_src_padding_mask(self, src):
-----------++         # input src of shape ()
-----------++         src_padding_mask = src.transpose(1, 0) == 0
-----------++@@ -106,7 +109,7 @@ class Model(nn.Module):
-----------++         x_decoder = x_decoder.transpose(0,1)
-----------+  
-----------+-         x = x.transpose(0,1) # put time first
-----------+-         tgt = tgt.transpose(0,1) # put channel after
-----------+--        x_encoder = self.transformerEncoder(x)
-----------+--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------+-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-----------+-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-----------++         if self.has_aux_loss:
-----------++-            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------+++            return self.w_aux(x_encoder), self.w_out(x_decoder)
-----------++         else:
-----------++             return self.w_out(x)
-----------+  
-----------+-         x_encoder = x_encoder.transpose(0,1)
-----------+-         x_decoder = x_decoder.transpose(0,1)
-----------+ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+-index 53839a0..1343d7d 100644
-----------++index 2243f2d..342fccd 100644
-----------+ --- a/models/recognition_model/log.txt
-----------+ +++ b/models/recognition_model/log.txt
-----------+-@@ -1,639 +1,2 @@
-----------+--2fa943cd85263a152b6be80d502eda27932ebb27
-----------+-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------++@@ -1,845 +1,2 @@
-----------++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------+++be71135adc89793578f304adb405cea80a5b2b9a
-----------+  
-----------+ -diff --git a/architecture.py b/architecture.py
-----------+--index a8c70f3..2413a8a 100644
-----------++-index 2413a8a..94d0de0 100644
-----------+ ---- a/architecture.py
-----------+ -+++ b/architecture.py
-----------++-@@ -4,7 +4,7 @@ import torch
-----------++- from torch import nn
-----------++- import torch.nn.functional as F
-----------++- 
-----------++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-----------++- 
-----------++- from absl import flags
-----------++- FLAGS = flags.FLAGS
-----------+ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------+ -         return F.relu(x + res)
-----------+ - 
-----------+ - class Model(nn.Module):
-----------+---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-----------+--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------+ -         super().__init__()
-----------+ - 
-----------+ -         self.conv_blocks = nn.Sequential(
-----------+--@@ -59,9 +59,7 @@ class Model(nn.Module):
-----------+--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-----------++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+ - 
-----------+---        self.has_aux_out = num_aux_outs is not None
-----------+---        if self.has_aux_out:
-----------+---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------+--+        self.has_aux_loss = has_aux_loss
-----------++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-----------+ - 
-----------+--     def forward(self, x_feat, x_raw, y,session_ids):
-----------+--         # x shape is (batch, time, electrode)
-----------+--@@ -82,12 +80,14 @@ class Model(nn.Module):
-----------++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-----------++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+ - 
-----------+--         x = x.transpose(0,1) # put time first
-----------+--         tgt = tgt.transpose(0,1) # put channel after
-----------+---        x = self.transformerEncoder(x)
-----------+---        x = self.transformerDecoder(tgt, x)
-----------+---        x = x.transpose(0,1)
-----------+--+        x_encoder = self.transformerEncoder(x)
-----------+--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++-         self.has_aux_loss = has_aux_loss
-----------++--
-----------++--    def forward(self, x_feat, x_raw, y,session_ids):
-----------++-+        self.device=device
-----------++-+
-----------++-+    def create_src_padding_mask(self, src):
-----------++-+        # input src of shape ()
-----------++-+        src_padding_mask = src.transpose(1, 0) == 0
-----------++-+        return src_padding_mask
-----------++-+
-----------++-+    def create_tgt_padding_mask(self, tgt):
-----------++-+        # input tgt of shape ()
-----------++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-----------++-+        return tgt_padding_mask
-----------++-+    
-----------++-+    def forward(self, x_feat, x_raw, y, session_ids):
-----------++-         # x shape is (batch, time, electrode)
-----------++-+        # y shape is (batch, sequence_length)
-----------++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-----------++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-----------++-+        memory_key_padding_mask = src_key_padding_mask
-----------++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-----------+ - 
-----------+---        if self.has_aux_out:
-----------+---            return self.w_out(x), self.w_aux(x)
-----------+--+        x_encoder = x_encoder.transpose(0,1)
-----------+--+        x_decoder = x_decoder.transpose(0,1)
-----------++-         if self.training:
-----------++-             r = random.randrange(8)
-----------++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-----------++-         x_raw = self.conv_blocks(x_raw)
-----------++-         x_raw = x_raw.transpose(1,2)
-----------++-         x_raw = self.w_raw_in(x_raw)
-----------++--
-----------++-         x = x_raw
-----------+ -+
-----------+--+        if self.has_aux_loss:
-----------+--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------+--         else:
-----------+--             return self.w_out(x)
-----------++-+        #Embedding and positional encoding of tgt
-----------++-         tgt=self.embedding_tgt(y)
-----------++-+        tgt=self.pos_encoder(tgt)
-----------+ - 
-----------+--diff --git a/data_utils.py b/data_utils.py
-----------+--index e2632e8..8b05213 100644
-----------+----- a/data_utils.py
-----------+--+++ b/data_utils.py
-----------+--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-----------++-         x = x.transpose(0,1) # put time first
-----------++-         tgt = tgt.transpose(0,1) # put channel after
-----------++--        x_encoder = self.transformerEncoder(x)
-----------++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-----------++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-----------+ - 
-----------+-- def combine_fixed_length_tgt(tensor_list, n_batch):
-----------+--     total_length = sum(t.size(0) for t in tensor_list)
-----------+--+    tensor_list = list(tensor_list) # copy
-----------+--     if total_length % n_batch != 0:
-----------+--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------+---        tensor_list = list(tensor_list) # copy
-----------+--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------+--         total_length += pad_length
-----------+--     tensor = torch.cat(tensor_list, 0)
-----------++-         x_encoder = x_encoder.transpose(0,1)
-----------++-         x_decoder = x_decoder.transpose(0,1)
-----------+ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+--index b8f7791..617dd85 100644
-----------++-index 53839a0..1343d7d 100644
-----------+ ---- a/models/recognition_model/log.txt
-----------+ -+++ b/models/recognition_model/log.txt
-----------+--@@ -1,480 +1,2 @@
-----------+---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------+--+2fa943cd85263a152b6be80d502eda27932ebb27
-----------++-@@ -1,639 +1,2 @@
-----------++--2fa943cd85263a152b6be80d502eda27932ebb27
-----------++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------+ - 
-----------+ --diff --git a/architecture.py b/architecture.py
-----------+---index d6e99b4..a8c70f3 100644
-----------++--index a8c70f3..2413a8a 100644
-----------+ ----- a/architecture.py
-----------+ --+++ b/architecture.py
-----------+---@@ -54,7 +54,7 @@ class Model(nn.Module):
-----------+---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------++--         return F.relu(x + res)
-----------+ -- 
-----------+---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++-- class Model(nn.Module):
-----------++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-----------++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++--         super().__init__()
-----------++-- 
-----------++--         self.conv_blocks = nn.Sequential(
-----------++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-----------+ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++-- 
-----------++---        self.has_aux_out = num_aux_outs is not None
-----------++---        if self.has_aux_out:
-----------++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------++--+        self.has_aux_loss = has_aux_loss
-----------++-- 
-----------++--     def forward(self, x_feat, x_raw, y,session_ids):
-----------++--         # x shape is (batch, time, electrode)
-----------++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-----------++-- 
-----------++--         x = x.transpose(0,1) # put time first
-----------++--         tgt = tgt.transpose(0,1) # put channel after
-----------++---        x = self.transformerEncoder(x)
-----------++---        x = self.transformerDecoder(tgt, x)
-----------++---        x = x.transpose(0,1)
-----------++--+        x_encoder = self.transformerEncoder(x)
-----------++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++-- 
-----------++---        if self.has_aux_out:
-----------++---            return self.w_out(x), self.w_aux(x)
-----------++--+        x_encoder = x_encoder.transpose(0,1)
-----------++--+        x_decoder = x_decoder.transpose(0,1)
-----------++--+
-----------++--+        if self.has_aux_loss:
-----------++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------++--         else:
-----------++--             return self.w_out(x)
-----------++-- 
-----------+ --diff --git a/data_utils.py b/data_utils.py
-----------+---index e4ac852..e2632e8 100644
-----------++--index e2632e8..8b05213 100644
-----------+ ----- a/data_utils.py
-----------+ --+++ b/data_utils.py
-----------+---@@ -1,3 +1,4 @@
-----------+---+import math
-----------+--- import string
-----------+--- 
-----------+--- import numpy as np
-----------+---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-----------+---     n = total_length // length
-----------+---     return tensor.view(n, length, *tensor.size()[1:])
-----------++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-----------+ -- 
-----------+---+def combine_fixed_length_tgt(tensor_list, n_batch):
-----------+---+    total_length = sum(t.size(0) for t in tensor_list)
-----------+---+    if total_length % n_batch != 0:
-----------+---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------+---+        tensor_list = list(tensor_list) # copy
-----------+---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------+---+        total_length += pad_length
-----------+---+    tensor = torch.cat(tensor_list, 0)
-----------+---+    length = total_length // n_batch
-----------+---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-----------+---+
-----------+--- def decollate_tensor(tensor, lengths):
-----------+---     b, s, d = tensor.size()
-----------+---     tensor = tensor.view(b*s, d)
-----------++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-----------++--     total_length = sum(t.size(0) for t in tensor_list)
-----------++--+    tensor_list = list(tensor_list) # copy
-----------++--     if total_length % n_batch != 0:
-----------++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------++---        tensor_list = list(tensor_list) # copy
-----------++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------++--         total_length += pad_length
-----------++--     tensor = torch.cat(tensor_list, 0)
-----------+ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+---index e890f0f..1ee3421 100644
-----------++--index b8f7791..617dd85 100644
-----------+ ----- a/models/recognition_model/log.txt
-----------+ --+++ b/models/recognition_model/log.txt
-----------+---@@ -1,265 +1,2 @@
-----------+----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------+---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------++--@@ -1,480 +1,2 @@
-----------++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------++--+2fa943cd85263a152b6be80d502eda27932ebb27
-----------+ -- 
-----------+ ---diff --git a/architecture.py b/architecture.py
-----------+----index b22af61..d6e99b4 100644
-----------++---index d6e99b4..a8c70f3 100644
-----------+ ------ a/architecture.py
-----------+ ---+++ b/architecture.py
-----------+----@@ -51,6 +51,8 @@ class Model(nn.Module):
-----------+----         )
-----------+----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-----------++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+ --- 
-----------+----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+----+
-----------+ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+----@@ -61,7 +63,7 @@ class Model(nn.Module):
-----------+----         if self.has_aux_out:
-----------+----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------+---- 
-----------+-----    def forward(self, x_feat, x_raw, session_ids):
-----------+----+    def forward(self, x_feat, x_raw, y,session_ids):
-----------+----         # x shape is (batch, time, electrode)
-----------+---- 
-----------+----         if self.training:
-----------+----@@ -76,10 +78,12 @@ class Model(nn.Module):
-----------+----         x_raw = self.w_raw_in(x_raw)
-----------+---- 
-----------+----         x = x_raw
-----------+----+        tgt=self.embedding_tgt(y)
-----------+---- 
-----------+----         x = x.transpose(0,1) # put time first
-----------+----+        tgt = tgt.transpose(0,1) # put channel after
-----------+----         x = self.transformerEncoder(x)
-----------+-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------+----+        x = self.transformerDecoder(tgt, x)
-----------+----         x = x.transpose(0,1)
-----------+---- 
-----------+----         if self.has_aux_out:
-----------++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+ ---diff --git a/data_utils.py b/data_utils.py
-----------+----index 11d4805..e4ac852 100644
-----------++---index e4ac852..e2632e8 100644
-----------+ ------ a/data_utils.py
-----------+ ---+++ b/data_utils.py
-----------+----@@ -244,6 +244,7 @@ class TextTransform(object):
-----------+----     def __init__(self):
-----------+----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-----------+----         self.chars = string.ascii_lowercase+string.digits+' '
-----------+----+        self.vocabulary_size=len(self.chars)
-----------++---@@ -1,3 +1,4 @@
-----------++---+import math
-----------++--- import string
-----------+ --- 
-----------+----     def clean_text(self, text):
-----------+----         text = unidecode(text)
-----------++--- import numpy as np
-----------++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-----------++---     n = total_length // length
-----------++---     return tensor.view(n, length, *tensor.size()[1:])
-----------++--- 
-----------++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-----------++---+    total_length = sum(t.size(0) for t in tensor_list)
-----------++---+    if total_length % n_batch != 0:
-----------++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------++---+        tensor_list = list(tensor_list) # copy
-----------++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------++---+        total_length += pad_length
-----------++---+    tensor = torch.cat(tensor_list, 0)
-----------++---+    length = total_length // n_batch
-----------++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-----------++---+
-----------++--- def decollate_tensor(tensor, lengths):
-----------++---     b, s, d = tensor.size()
-----------++---     tensor = tensor.view(b*s, d)
-----------+ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+----index fbc0abb..400061a 100644
-----------++---index e890f0f..1ee3421 100644
-----------+ ------ a/models/recognition_model/log.txt
-----------+ ---+++ b/models/recognition_model/log.txt
-----------+----@@ -1,188 +1,2 @@
-----------+-----57f8139449dd9286c2203ec2eca118a550638a7c
-----------+----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------++---@@ -1,265 +1,2 @@
-----------++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------+ --- 
-----------+ ----diff --git a/architecture.py b/architecture.py
-----------+-----index 4fc3793..b22af61 100644
-----------++----index b22af61..d6e99b4 100644
-----------+ ------- a/architecture.py
-----------+ ----+++ b/architecture.py
-----------+-----@@ -4,7 +4,7 @@ import torch
-----------+----- from torch import nn
-----------+----- import torch.nn.functional as F
-----------+----- 
-----------+------from transformer import TransformerEncoderLayer
-----------+-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------+----- 
-----------+----- from absl import flags
-----------+----- FLAGS = flags.FLAGS
-----------+-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-----------++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-----------++----         )
-----------+ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+ ---- 
-----------++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++----+
-----------+ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-----------++----         if self.has_aux_out:
-----------++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------++---- 
-----------++-----    def forward(self, x_feat, x_raw, session_ids):
-----------++----+    def forward(self, x_feat, x_raw, y,session_ids):
-----------++----         # x shape is (batch, time, electrode)
-----------++---- 
-----------++----         if self.training:
-----------++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-----------++----         x_raw = self.w_raw_in(x_raw)
-----------+ ---- 
-----------+-----         self.has_aux_out = num_aux_outs is not None
-----------+-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-----------+ ----         x = x_raw
-----------++----+        tgt=self.embedding_tgt(y)
-----------+ ---- 
-----------+ ----         x = x.transpose(0,1) # put time first
-----------+------        x = self.transformer(x)
-----------+-----+        x = self.transformerEncoder(x)
-----------+-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++----+        tgt = tgt.transpose(0,1) # put channel after
-----------++----         x = self.transformerEncoder(x)
-----------++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++----+        x = self.transformerDecoder(tgt, x)
-----------+ ----         x = x.transpose(0,1)
-----------+ ---- 
-----------+ ----         if self.has_aux_out:
-----------++----diff --git a/data_utils.py b/data_utils.py
-----------++----index 11d4805..e4ac852 100644
-----------++------- a/data_utils.py
-----------++----+++ b/data_utils.py
-----------++----@@ -244,6 +244,7 @@ class TextTransform(object):
-----------++----     def __init__(self):
-----------++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-----------++----         self.chars = string.ascii_lowercase+string.digits+' '
-----------++----+        self.vocabulary_size=len(self.chars)
-----------++---- 
-----------++----     def clean_text(self, text):
-----------++----         text = unidecode(text)
-----------+ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+-----index 571de9d..8563980 100644
-----------++----index fbc0abb..400061a 100644
-----------+ ------- a/models/recognition_model/log.txt
-----------+ ----+++ b/models/recognition_model/log.txt
-----------+-----@@ -1,5 +1,2 @@
-----------+-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++----@@ -1,188 +1,2 @@
-----------++-----57f8139449dd9286c2203ec2eca118a550638a7c
-----------++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------+ ---- 
-----------++-----diff --git a/architecture.py b/architecture.py
-----------++-----index 4fc3793..b22af61 100644
-----------++-------- a/architecture.py
-----------++-----+++ b/architecture.py
-----------++-----@@ -4,7 +4,7 @@ import torch
-----------++----- from torch import nn
-----------++----- import torch.nn.functional as F
-----------++----- 
-----------++------from transformer import TransformerEncoderLayer
-----------++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------++----- 
-----------++----- from absl import flags
-----------++----- FLAGS = flags.FLAGS
-----------++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-----------++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------++----- 
-----------++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++----- 
-----------++-----         self.has_aux_out = num_aux_outs is not None
-----------++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-----------++-----         x = x_raw
-----------++----- 
-----------++-----         x = x.transpose(0,1) # put time first
-----------++------        x = self.transformer(x)
-----------++-----+        x = self.transformerEncoder(x)
-----------++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++-----         x = x.transpose(0,1)
-----------++----- 
-----------++-----         if self.has_aux_out:
-----------++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------++-----index 571de9d..8563980 100644
-----------++-------- a/models/recognition_model/log.txt
-----------++-----+++ b/models/recognition_model/log.txt
-----------++-----@@ -1,5 +1,2 @@
-----------++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++----- 
-----------++------
-----------++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-----------++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------++------train / dev split: 8055 200
-----------++-----diff --git a/output/log.txt b/output/log.txt
-----------++-----index ae42364..1d2cd8e 100644
-----------++-------- a/output/log.txt
-----------++-----+++ b/output/log.txt
-----------++-----@@ -1,3 +1,13 @@
-----------++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++----- 
-----------++-----+diff --git a/output/log.txt b/output/log.txt
-----------++-----+index ae42364..8563980 100644
-----------++-----+--- a/output/log.txt
-----------++-----++++ b/output/log.txt
-----------++-----+@@ -1,3 +1,2 @@
-----------++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-----------++-----+ 
-----------++-----+-
-----------++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++----- 
-----------++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++-----diff --git a/transformer.py b/transformer.py
-----------++-----index 6743588..ac131be 100644
-----------++-------- a/transformer.py
-----------++-----+++ b/transformer.py
-----------++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-----------++-----         Shape:
-----------++-----             see the docs in Transformer class.
-----------++-----         """
-----------++------        src2 = self.self_attn(src)
-----------++-----+        src2 = self.self_attn(src, src, src)
-----------++-----         src = src + self.dropout1(src2)
-----------++-----         src = self.norm1(src)
-----------++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-----------++-----         src = self.norm2(src)
-----------++-----         return src
-----------++----- 
-----------++-----+class TransformerDecoderLayer(nn.Module):
-----------++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-----------++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-----------++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-----------++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-----------++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-----------++-----+    in a different way during application.
-----------++-----+
-----------++-----+    Args:
-----------++-----+        d_model: the number of expected features in the input (required).
-----------++-----+        nhead: the number of heads in the multiheadattention models (required).
-----------++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-----------++-----+        dropout: the dropout value (default=0.1).
-----------++-----+        activation: the activation function of the intermediate layer, can be a string
-----------++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-----------++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-----------++-----+        batch_first: If ``True``, then the input and output tensors are provided
-----------++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-----------++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-----------++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-----------++-----+            Default: ``False`` (after).
-----------++-----+
-----------++-----+    Examples::
-----------++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-----------++-----+        >>> memory = torch.rand(10, 32, 512)
-----------++-----+        >>> tgt = torch.rand(20, 32, 512)
-----------++-----+        >>> out = decoder_layer(tgt, memory)
-----------++-----+    """
-----------++-----+    # Adapted from pytorch source
-----------++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------++-----+        super(TransformerDecoderLayer, self).__init__()
-----------++-----+        #Attention Mechanism
-----------++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------++-----+        # Implementation of Feedforward model
-----------++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-----------++-----+        self.dropout = nn.Dropout(dropout)
-----------++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-----------++-----+        #Normalization Layer and Dropout Layer
-----------++-----+        self.norm1 = nn.LayerNorm(d_model)
-----------++-----+        self.norm2 = nn.LayerNorm(d_model)
-----------++-----+        self.norm3 = nn.LayerNorm(d_model)
-----------++-----+        self.dropout1 = nn.Dropout(dropout)
-----------++-----+        self.dropout2 = nn.Dropout(dropout)
-----------++-----+        self.dropout3 = nn.Dropout(dropout)
-----------++-----+        #Activation Function
-----------++-----+        self.activation = nn.ReLU()
-----------++-----+    
-----------++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-----------++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-----------++-----+        r"""Pass the input through the encoder layer.
-----------++-----+
-----------++-----+        Args:
-----------++-----+            tgt: the sequence to the decoder layer (required).
-----------++-----+            memory: the sequence from the last layer of the encoder (required).
-----------++-----+            tgt_mask: the mask for the tgt sequence (optional).
-----------++-----+            memory_mask: the mask for the memory sequence (optional).
-----------++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-----------++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-----------++-----+
-----------++-----+        Shape:
-----------++-----+            see the docs in Transformer class.
-----------++-----+        """
-----------++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------++-----+        tgt = tgt + self.dropout1(tgt2)
-----------++-----+        tgt = self.norm1(tgt)
-----------++-----+
-----------++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-----------++-----+        tgt = tgt + self.dropout1(tgt2)
-----------++-----+        tgt = self.norm1(tgt)
-----------++-----+
-----------++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-----------++-----+        tgt = tgt + self.dropout2(tgt2)
-----------++-----+        tgt = self.norm2(tgt)
-----------++-----+        return tgt
-----------++-----+    
-----------++-----+
-----------++----- class MultiHeadAttention(nn.Module):
-----------++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------++-----     super().__init__()
-----------++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-----------++-----     else:
-----------++-----         self.relative_positional = None
-----------++----- 
-----------++------  def forward(self, x):
-----------++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------++-----     """Runs the multi-head self-attention layer.
-----------++----- 
-----------++-----     Args:
-----------++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-----------++-----       A single tensor containing the output from this layer
-----------++-----     """
-----------++----- 
-----------++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-----------++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-----------++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-----------++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-----------++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-----------++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------++----- 
-----------++-----     if self.relative_positional is not None:
-----------+ -----
-----------+------['recognition_model.py', '--output_directory', './models/recognition_model/']
-----------++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+ -----train / dev split: 8055 200
-----------+-----diff --git a/output/log.txt b/output/log.txt
-----------+-----index ae42364..1d2cd8e 100644
-----------+-------- a/output/log.txt
-----------+-----+++ b/output/log.txt
-----------+-----@@ -1,3 +1,13 @@
-----------+-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++----diff --git a/recognition_model.py b/recognition_model.py
-----------++----index dea6d47..a46dff0 100644
-----------++------- a/recognition_model.py
-----------++----+++ b/recognition_model.py
-----------++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------+ ---- 
-----------+-----+diff --git a/output/log.txt b/output/log.txt
-----------+-----+index ae42364..8563980 100644
-----------+-----+--- a/output/log.txt
-----------+-----++++ b/output/log.txt
-----------+-----+@@ -1,3 +1,2 @@
-----------+-----++57f8139449dd9286c2203ec2eca118a550638a7c
-----------+-----+ 
-----------+-----+-
-----------+-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++----             X = combine_fixed_length(example['emg'], 200).to(device)
-----------++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+ ---- 
-----------+----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+-----diff --git a/transformer.py b/transformer.py
-----------+-----index 6743588..ac131be 100644
-----------+-------- a/transformer.py
-----------+-----+++ b/transformer.py
-----------+-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-----------+-----         Shape:
-----------+-----             see the docs in Transformer class.
-----------+-----         """
-----------+------        src2 = self.self_attn(src)
-----------+-----+        src2 = self.self_attn(src, src, src)
-----------+-----         src = src + self.dropout1(src2)
-----------+-----         src = self.norm1(src)
-----------+-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------+-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-----------+-----         src = self.norm2(src)
-----------+-----         return src
-----------++-----            pred = model(X, X_raw, sess)
-----------++----+            pred = model(X, X_raw, tgt, sess)
-----------++----             pred = F.log_softmax(pred, 2)
-----------+ ---- 
-----------+-----+class TransformerDecoderLayer(nn.Module):
-----------+-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-----------+-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-----------+-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-----------+-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-----------+-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-----------+-----+    in a different way during application.
-----------+-----+
-----------+-----+    Args:
-----------+-----+        d_model: the number of expected features in the input (required).
-----------+-----+        nhead: the number of heads in the multiheadattention models (required).
-----------+-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-----------+-----+        dropout: the dropout value (default=0.1).
-----------+-----+        activation: the activation function of the intermediate layer, can be a string
-----------+-----+            ("relu" or "gelu") or a unary callable. Default: relu
-----------+-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-----------+-----+        batch_first: If ``True``, then the input and output tensors are provided
-----------+-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-----------+-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-----------+-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-----------+-----+            Default: ``False`` (after).
-----------+-----+
-----------+-----+    Examples::
-----------+-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-----------+-----+        >>> memory = torch.rand(10, 32, 512)
-----------+-----+        >>> tgt = torch.rand(20, 32, 512)
-----------+-----+        >>> out = decoder_layer(tgt, memory)
-----------+-----+    """
-----------+-----+    # Adapted from pytorch source
-----------+-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------+-----+        super(TransformerDecoderLayer, self).__init__()
-----------+-----+        #Attention Mechanism
-----------+-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------+-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------+-----+        # Implementation of Feedforward model
-----------+-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-----------+-----+        self.dropout = nn.Dropout(dropout)
-----------+-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-----------+-----+        #Normalization Layer and Dropout Layer
-----------+-----+        self.norm1 = nn.LayerNorm(d_model)
-----------+-----+        self.norm2 = nn.LayerNorm(d_model)
-----------+-----+        self.norm3 = nn.LayerNorm(d_model)
-----------+-----+        self.dropout1 = nn.Dropout(dropout)
-----------+-----+        self.dropout2 = nn.Dropout(dropout)
-----------+-----+        self.dropout3 = nn.Dropout(dropout)
-----------+-----+        #Activation Function
-----------+-----+        self.activation = nn.ReLU()
-----------+-----+    
-----------+-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-----------+-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-----------+-----+        r"""Pass the input through the encoder layer.
-----------+-----+
-----------+-----+        Args:
-----------+-----+            tgt: the sequence to the decoder layer (required).
-----------+-----+            memory: the sequence from the last layer of the encoder (required).
-----------+-----+            tgt_mask: the mask for the tgt sequence (optional).
-----------+-----+            memory_mask: the mask for the memory sequence (optional).
-----------+-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-----------+-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-----------+-----+
-----------+-----+        Shape:
-----------+-----+            see the docs in Transformer class.
-----------+-----+        """
-----------+-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+-----+        tgt = tgt + self.dropout1(tgt2)
-----------+-----+        tgt = self.norm1(tgt)
-----------+-----+
-----------+-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-----------+-----+        tgt = tgt + self.dropout1(tgt2)
-----------+-----+        tgt = self.norm1(tgt)
-----------+-----+
-----------+-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-----------+-----+        tgt = tgt + self.dropout2(tgt2)
-----------+-----+        tgt = self.norm2(tgt)
-----------+-----+        return tgt
-----------+-----+    
-----------+-----+
-----------+----- class MultiHeadAttention(nn.Module):
-----------+-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------+-----     super().__init__()
-----------+-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-----------+-----     else:
-----------+-----         self.relative_positional = None
-----------+----- 
-----------+------  def forward(self, x):
-----------+-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------+-----     """Runs the multi-head self-attention layer.
-----------+----- 
-----------+-----     Args:
-----------+-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-----------+-----       A single tensor containing the output from this layer
-----------+-----     """
-----------+----- 
-----------+------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-----------+------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-----------+------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-----------+-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-----------+-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-----------+-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------+-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+----- 
-----------+-----     if self.relative_positional is not None:
-----------++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+ ----
-----------+ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+ ----train / dev split: 8055 200
-----------+ ---diff --git a/recognition_model.py b/recognition_model.py
-----------+----index dea6d47..a46dff0 100644
-----------++---index a46dff0..8fd300c 100644
-----------+ ------ a/recognition_model.py
-----------+ ---+++ b/recognition_model.py
-----------+----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------++---@@ -6,6 +6,7 @@ import subprocess
-----------++--- from ctcdecode import CTCBeamDecoder
-----------++--- import jiwer
-----------++--- import random
-----------++---+from torch.utils.tensorboard import SummaryWriter
-----------++--- 
-----------++--- import torch
-----------++--- from torch import nn
-----------++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-----------++--- 
-----------++--- from read_emg import EMGDataset, SizeAwareSampler
-----------++--- from architecture import Model
-----------++----from data_utils import combine_fixed_length, decollate_tensor
-----------++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-----------++--- from transformer import TransformerEncoderLayer
-----------++--- 
-----------++--- from absl import flags
-----------++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-----------++---     return jiwer.wer(references, predictions)
-----------++--- 
-----------++--- 
-----------++----def train_model(trainset, devset, device, n_epochs=200):
-----------++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++----
-----------++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------++---+    #Define Dataloader
-----------++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++--- 
-----------++---+    #Define model and loss function
-----------++---     n_chars = len(devset.text_transform.chars)
-----------++---     model = Model(devset.num_features, n_chars+1).to(device)
-----------++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------++--- 
-----------++---     if FLAGS.start_training_from is not None:
-----------++---         state_dict = torch.load(FLAGS.start_training_from)
-----------++---         model.load_state_dict(state_dict, strict=False)
-----------+ --- 
-----------++---+    #Define optimizer and scheduler for the learning rate
-----------++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-----------++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-----------++--- 
-----------++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-----------++--- 
-----------++---     batch_idx = 0
-----------++---+    train_loss= 0
-----------++---+    eval_loss = 0
-----------++---     optim.zero_grad()
-----------++---     for epoch_idx in range(n_epochs):
-----------++---+        model.train()
-----------++---         losses = []
-----------++----        for example in dataloader:
-----------++---+        for example in dataloader_training:
-----------++---             schedule_lr(batch_idx)
-----------++--- 
-----------++---+            #Preprosessing of the input and target for the model
-----------+ ---             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------+ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------++--- 
-----------++---+            #Shifting target for input decoder and loss
-----------++---+            tgt= y[:,:-1]
-----------++---+            target= y[:,1:]
-----------++---+
-----------++---+            #Prediction
-----------++---             pred = model(X, X_raw, tgt, sess)
-----------++----            pred = F.log_softmax(pred, 2)
-----------++--- 
-----------++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------++---+            #Primary Loss
-----------++---+            pred=pred.permute(0,2,1)
-----------++---+            loss = loss_fn(pred, target)
-----------++---+
-----------++---+            #Auxiliary Loss
-----------++---+            #pred = F.log_softmax(pred, 2)
-----------++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++---             losses.append(loss.item())
-----------++---+            train_loss += loss.item()
-----------++--- 
-----------++---             loss.backward()
-----------++---             if (batch_idx+1) % 2 == 0:
-----------++---                 optim.step()
-----------++---                 optim.zero_grad()
-----------++--- 
-----------++---+            #Report plots in tensorboard
-----------++---+            if batch_idx % report_every == report_every - 2:     
-----------++---+                #Evaluation
-----------++---+                model.eval()
-----------++---+                with torch.no_grad():
-----------++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------++---+                        X_raw = example['raw_emg'].to(device)
-----------++---+                        sess = example['session_ids'].to(device)
-----------++---+                        y = example['text_int'].to(device)
-----------++---+
-----------++---+                        #Shifting target for input decoder and loss
-----------++---+                        tgt= y[:,:-1]
-----------++---+                        target= y[:,1:]
-----------++---+
-----------++---+                        #Prediction without the 197-th batch because of missing label
-----------++---+                        if idx != 197:
-----------++---+                            pred = model(X, X_raw, tgt, sess)
-----------++---+                            #Primary Loss
-----------++---+                            pred=pred.permute(0,2,1)
-----------++---+                            loss = loss_fn(pred, target)
-----------++---+                            eval_loss += loss.item()
-----------++---+
-----------++---+                #Writing on tensorboard
-----------++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-----------++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-----------++---+                train_loss= 0
-----------++---+                eval_loss= 0
-----------++---+
-----------++---+            #Increment counter        
-----------++---             batch_idx += 1
-----------++----        train_loss = np.mean(losses)
-----------++---+
-----------++---+        #Testing and change learning rate
-----------++---         val = test(model, devset, device)
-----------++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-----------++---         lr_sched.step()
-----------++---+    
-----------++---+        #Logging
-----------++---+        train_loss = np.mean(losses)
-----------++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-----------++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-----------++--- 
-----------++---@@ -148,8 +201,9 @@ def main():
-----------++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-----------++--- 
-----------++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-----------++---+    writer = SummaryWriter(log_dir="./content/runs")
-----------++--- 
-----------++----    model = train_model(trainset, devset, device)
-----------++---+    model = train_model(trainset, devset ,device, writer)
-----------+ --- 
-----------+-----            pred = model(X, X_raw, sess)
-----------+----+            pred = model(X, X_raw, tgt, sess)
-----------+----             pred = F.log_softmax(pred, 2)
-----------++--- if __name__ == '__main__':
-----------++---     FLAGS(sys.argv)
-----------++---diff --git a/transformer.py b/transformer.py
-----------++---index ac131be..51e1f2e 100644
-----------++------ a/transformer.py
-----------++---+++ b/transformer.py
-----------++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-----------++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------++---     self.d_qkv = d_qkv
-----------+ --- 
-----------+----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++---+    #self.kdim = kdim if kdim is not None else embed_dim
-----------++---+    #self.vdim = vdim if vdim is not None else embed_dim
-----------++---+
-----------++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+ ---
-----------+ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+ ---train / dev split: 8055 200
-----------+ --diff --git a/recognition_model.py b/recognition_model.py
-----------+---index a46dff0..8fd300c 100644
-----------++--index fde5a40..6d5143b 100644
-----------+ ----- a/recognition_model.py
-----------+ --+++ b/recognition_model.py
-----------+---@@ -6,6 +6,7 @@ import subprocess
-----------+--- from ctcdecode import CTCBeamDecoder
-----------+--- import jiwer
-----------+--- import random
-----------+---+from torch.utils.tensorboard import SummaryWriter
-----------+--- 
-----------+--- import torch
-----------+--- from torch import nn
-----------+---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-----------+--- 
-----------+--- from read_emg import EMGDataset, SizeAwareSampler
-----------+--- from architecture import Model
-----------+----from data_utils import combine_fixed_length, decollate_tensor
-----------+---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-----------+--- from transformer import TransformerEncoderLayer
-----------+--- 
-----------+--- from absl import flags
-----------+---@@ -62,17 +63,21 @@ def test(model, testset, device):
-----------++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-----------+ --     return jiwer.wer(references, predictions)
-----------+ -- 
-----------+ -- 
-----------+----def train_model(trainset, devset, device, n_epochs=200):
-----------+----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+----
-----------+---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------+---+    #Define Dataloader
-----------+---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-----------++--     #Define Dataloader
-----------++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------+ -- 
-----------+---+    #Define model and loss function
-----------++--     #Define model and loss function
-----------+ --     n_chars = len(devset.text_transform.chars)
-----------+---     model = Model(devset.num_features, n_chars+1).to(device)
-----------+---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------++---    model = Model(devset.num_features, n_chars+1).to(device)
-----------++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------+ -- 
-----------+ --     if FLAGS.start_training_from is not None:
-----------+---         state_dict = torch.load(FLAGS.start_training_from)
-----------+---         model.load_state_dict(state_dict, strict=False)
-----------+--- 
-----------+---+    #Define optimizer and scheduler for the learning rate
-----------+---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-----------+---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-----------+--- 
-----------+---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------+---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-----------+--- 
-----------+---     batch_idx = 0
-----------+---+    train_loss= 0
-----------+---+    eval_loss = 0
-----------+---     optim.zero_grad()
-----------+---     for epoch_idx in range(n_epochs):
-----------+---+        model.train()
-----------+---         losses = []
-----------+----        for example in dataloader:
-----------+---+        for example in dataloader_training:
-----------+---             schedule_lr(batch_idx)
-----------++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------++--             target= y[:,1:]
-----------+ -- 
-----------+---+            #Preprosessing of the input and target for the model
-----------+---             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------+---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------++--             #Prediction
-----------++---            pred = model(X, X_raw, tgt, sess)
-----------++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-----------+ -- 
-----------+---+            #Shifting target for input decoder and loss
-----------+---+            tgt= y[:,:-1]
-----------+---+            target= y[:,1:]
-----------+---+
-----------+---+            #Prediction
-----------+---             pred = model(X, X_raw, tgt, sess)
-----------+----            pred = F.log_softmax(pred, 2)
-----------++--             #Primary Loss
-----------++---            pred=pred.permute(0,2,1)
-----------++---            loss = loss_fn(pred, target)
-----------++--+            out_dec=out_dec.permute(0,2,1)
-----------++--+            loss_dec = loss_fn(out_dec, target)
-----------+ -- 
-----------+----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+---+            #Primary Loss
-----------+---+            pred=pred.permute(0,2,1)
-----------+---+            loss = loss_fn(pred, target)
-----------++--             #Auxiliary Loss
-----------++---            #pred = F.log_softmax(pred, 2)
-----------++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++--+            out_enc = F.log_softmax(out_enc, 2)
-----------++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+ --+
-----------+---+            #Auxiliary Loss
-----------+---+            #pred = F.log_softmax(pred, 2)
-----------+---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------+---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-----------+ --             losses.append(loss.item())
-----------+---+            train_loss += loss.item()
-----------++--             train_loss += loss.item()
-----------+ -- 
-----------+---             loss.backward()
-----------++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+ --             if (batch_idx+1) % 2 == 0:
-----------+ --                 optim.step()
-----------+ --                 optim.zero_grad()
-----------++---
-----------++---            if batch_idx % report_every == report_every - 2:     
-----------++--+            
-----------++--+            if False:
-----------++--+            #if batch_idx % report_every == report_every - 2:     
-----------++--                 #Evaluation
-----------++--                 model.eval()
-----------++--                 with torch.no_grad():
-----------++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------++---                        X_raw = example['raw_emg'].to(device)
-----------++---                        sess = example['session_ids'].to(device)
-----------++---                        y = example['text_int'].to(device)
-----------++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+ -- 
-----------+---+            #Report plots in tensorboard
-----------+---+            if batch_idx % report_every == report_every - 2:     
-----------+---+                #Evaluation
-----------+---+                model.eval()
-----------+---+                with torch.no_grad():
-----------+---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------+---+                        X_raw = example['raw_emg'].to(device)
-----------+---+                        sess = example['session_ids'].to(device)
-----------+---+                        y = example['text_int'].to(device)
-----------+---+
-----------+---+                        #Shifting target for input decoder and loss
-----------+---+                        tgt= y[:,:-1]
-----------+---+                        target= y[:,1:]
-----------+---+
-----------+---+                        #Prediction without the 197-th batch because of missing label
-----------+---+                        if idx != 197:
-----------+---+                            pred = model(X, X_raw, tgt, sess)
-----------+---+                            #Primary Loss
-----------+---+                            pred=pred.permute(0,2,1)
-----------+---+                            loss = loss_fn(pred, target)
-----------+---+                            eval_loss += loss.item()
-----------+---+
-----------+---+                #Writing on tensorboard
-----------+---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-----------+---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-----------+---+                train_loss= 0
-----------+---+                eval_loss= 0
-----------+---+
-----------+---+            #Increment counter        
-----------+---             batch_idx += 1
-----------+----        train_loss = np.mean(losses)
-----------+---+
-----------+---+        #Testing and change learning rate
-----------+---         val = test(model, devset, device)
-----------+---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-----------+---         lr_sched.step()
-----------+---+    
-----------+---+        #Logging
-----------+---+        train_loss = np.mean(losses)
-----------+---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-----------+---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-----------+--- 
-----------+---@@ -148,8 +201,9 @@ def main():
-----------+---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-----------+--- 
-----------+---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-----------+---+    writer = SummaryWriter(log_dir="./content/runs")
-----------++--                         #Shifting target for input decoder and loss
-----------++--                         tgt= y[:,:-1]
-----------++--                         target= y[:,1:]
-----------+ -- 
-----------+----    model = train_model(trainset, devset, device)
-----------+---+    model = train_model(trainset, devset ,device, writer)
-----------++--+                        print(idx)
-----------++--+
-----------++--                         #Prediction without the 197-th batch because of missing label
-----------++---                        if idx != 197:
-----------++--+                        if idx != 181:
-----------++--                             pred = model(X, X_raw, tgt, sess)
-----------++--                             #Primary Loss
-----------++--                             pred=pred.permute(0,2,1)
-----------++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+ -- 
-----------+--- if __name__ == '__main__':
-----------+---     FLAGS(sys.argv)
-----------+---diff --git a/transformer.py b/transformer.py
-----------+---index ac131be..51e1f2e 100644
-----------+------ a/transformer.py
-----------+---+++ b/transformer.py
-----------+---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-----------+---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------+---     self.d_qkv = d_qkv
-----------++--             #Increment counter        
-----------++--             batch_idx += 1
-----------++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------+ -- 
-----------+---+    #self.kdim = kdim if kdim is not None else embed_dim
-----------+---+    #self.vdim = vdim if vdim is not None else embed_dim
-----------+---+
-----------+---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++--         #Testing and change learning rate
-----------++--         val = test(model, devset, device)
-----------+ --
-----------+ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+ --train / dev split: 8055 200
-----------+ -diff --git a/recognition_model.py b/recognition_model.py
-----------+--index fde5a40..6d5143b 100644
-----------++-index 30c5ff2..2672d45 100644
-----------+ ---- a/recognition_model.py
-----------+ -+++ b/recognition_model.py
-----------+--@@ -63,14 +63,14 @@ def test(model, testset, device):
-----------+--     return jiwer.wer(references, predictions)
-----------+-- 
-----------+-- 
-----------+---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------+--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-----------+--     #Define Dataloader
-----------+--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------+ - 
-----------+ -     #Define model and loss function
-----------+ -     n_chars = len(devset.text_transform.chars)
-----------+---    model = Model(devset.num_features, n_chars+1).to(device)
-----------+--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-----------+ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------+ - 
-----------+ -     if FLAGS.start_training_from is not None:
-----------+--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+--             target= y[:,1:]
-----------++-diff --git a/transformer.py b/transformer.py
-----------++-index 51e1f2e..c125841 100644
-----------++---- a/transformer.py
-----------++-+++ b/transformer.py
-----------++-@@ -1,3 +1,4 @@
-----------++-+import math
-----------++- from typing import Optional
-----------+ - 
-----------+--             #Prediction
-----------+---            pred = model(X, X_raw, tgt, sess)
-----------+--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-----------++- import torch
-----------++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-----------++-         Shape:
-----------++-             see the docs in Transformer class.
-----------++-         """
-----------++--        src2 = self.self_attn(src, src, src)
-----------++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-----------++-         src = src + self.dropout1(src2)
-----------++-         src = self.norm1(src)
-----------++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-----------++-         Shape:
-----------++-             see the docs in Transformer class.
-----------++-         """
-----------++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------++-         tgt = tgt + self.dropout1(tgt2)
-----------++-         tgt = self.norm1(tgt)
-----------+ - 
-----------+--             #Primary Loss
-----------+---            pred=pred.permute(0,2,1)
-----------+---            loss = loss_fn(pred, target)
-----------+--+            out_dec=out_dec.permute(0,2,1)
-----------+--+            loss_dec = loss_fn(out_dec, target)
-----------++--        tgt2=self.multihead_attn(tgt, memory, memory)
-----------++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-----------++-         tgt = tgt + self.dropout1(tgt2)
-----------++-         tgt = self.norm1(tgt)
-----------+ - 
-----------+--             #Auxiliary Loss
-----------+---            #pred = F.log_softmax(pred, 2)
-----------+---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------+---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------+--+            out_enc = F.log_softmax(out_enc, 2)
-----------+--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+--+
-----------+--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-----------+--             losses.append(loss.item())
-----------+--             train_loss += loss.item()
-----------++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-----------++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------++-     self.d_qkv = d_qkv
-----------+ - 
-----------+--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+--             if (batch_idx+1) % 2 == 0:
-----------+--                 optim.step()
-----------+--                 optim.zero_grad()
-----------++--    #self.kdim = kdim if kdim is not None else embed_dim
-----------++--    #self.vdim = vdim if vdim is not None else embed_dim
-----------+ --
-----------+---            if batch_idx % report_every == report_every - 2:     
-----------+--+            
-----------+--+            if False:
-----------+--+            #if batch_idx % report_every == report_every - 2:     
-----------+--                 #Evaluation
-----------+--                 model.eval()
-----------+--                 with torch.no_grad():
-----------+--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------+---                        X_raw = example['raw_emg'].to(device)
-----------+---                        sess = example['session_ids'].to(device)
-----------+---                        y = example['text_int'].to(device)
-----------+--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+-- 
-----------+--                         #Shifting target for input decoder and loss
-----------+--                         tgt= y[:,:-1]
-----------+--                         target= y[:,1:]
-----------++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-----------++-     else:
-----------++-         self.relative_positional = None
-----------+ - 
-----------+--+                        print(idx)
-----------+--+
-----------+--                         #Prediction without the 197-th batch because of missing label
-----------+---                        if idx != 197:
-----------+--+                        if idx != 181:
-----------+--                             pred = model(X, X_raw, tgt, sess)
-----------+--                             #Primary Loss
-----------+--                             pred=pred.permute(0,2,1)
-----------+--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-----------++-     """Runs the multi-head self-attention layer.
-----------+ - 
-----------+--             #Increment counter        
-----------+--             batch_idx += 1
-----------+--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------++-     Args:
-----------++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-----------++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+ - 
-----------+--         #Testing and change learning rate
-----------+--         val = test(model, devset, device)
-----------++-+    if attn_mask is not None:
-----------++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++-+
-----------++-     if self.relative_positional is not None:
-----------++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------++-         l,b,h,d = q_pos.size()
-----------++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-----------++-             x = x.transpose(0, 1)
-----------++-             x = x.contiguous().view(bsz_heads, length+1, length)
-----------++-             return x[:, 1:, :]
-----------++-+        
-----------++-+
-----------++-+########
-----------++-+# Taken from:
-----------++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-----------++-+# or also here:
-----------++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-----------++-+class PositionalEncoding(nn.Module):
-----------++-+
-----------++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-----------++-+        super(PositionalEncoding, self).__init__()
-----------++-+        self.dropout = nn.Dropout(p=dropout)
-----------++-+        self.max_len = max_len
-----------++-+
-----------++-+        pe = torch.zeros(max_len, d_model)
-----------++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-----------++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-----------++-+                             * (-math.log(10000.0) / d_model))
-----------++-+        pe[:, 0::2] = torch.sin(position * div_term)
-----------++-+        pe[:, 1::2] = torch.cos(position * div_term)
-----------++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-----------++-+        self.register_buffer('pe', pe)  # Will not be trained.
-----------++-+
-----------++-+    def forward(self, x):
-----------++-+        """Inputs of forward function
-----------++-+        Args:
-----------++-+            x: the sequence fed to the positional encoder model (required).
-----------++-+        Shape:
-----------++-+            x: [sequence length, batch size, embed dim]
-----------++-+            output: [sequence length, batch size, embed dim]
-----------++-+        """
-----------++-+        assert x.size(0) < self.max_len, (
-----------++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-----------++-+        # shape of x (len, B, dim)
-----------++-+        x = x + self.pe[:x.size(0), :]
-----------++-+        return self.dropout(x)
-----------+ -
-----------+ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+ -train / dev split: 8055 200
-----------++diff --git a/output/log.txt b/output/log.txt
-----------++index 1d2cd8e..979357b 100644
-----------++--- a/output/log.txt
-----------+++++ b/output/log.txt
-----------++@@ -1,13 +1,1651 @@
-----------++-57f8139449dd9286c2203ec2eca118a550638a7c
-----------+++be71135adc89793578f304adb405cea80a5b2b9a
-----------++ 
-----------+++diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++index 2243f2d..a2c8558 100644
-----------+++--- a/models/recognition_model/log.txt
-----------++++++ b/models/recognition_model/log.txt
-----------+++@@ -1,844 +1,898 @@
-----------+++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------++++be71135adc89793578f304adb405cea80a5b2b9a
-----------+++ 
-----------+++-diff --git a/architecture.py b/architecture.py
-----------+++-index 2413a8a..94d0de0 100644
-----------+++---- a/architecture.py
-----------+++-+++ b/architecture.py
-----------+++-@@ -4,7 +4,7 @@ import torch
-----------+++- from torch import nn
-----------+++- import torch.nn.functional as F
-----------+++- 
-----------+++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------+++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-----------+++- 
-----------+++- from absl import flags
-----------+++- FLAGS = flags.FLAGS
-----------+++-@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------+++-         return F.relu(x + res)
-----------+++- 
-----------+++- class Model(nn.Module):
-----------+++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------+++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------+++-         super().__init__()
-----------+++- 
-----------+++-         self.conv_blocks = nn.Sequential(
-----------+++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-----------+++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+++- 
-----------+++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-----------+++- 
-----------+++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-----------+++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+++- 
-----------+++-         self.has_aux_loss = has_aux_loss
-----------+++--
-----------+++--    def forward(self, x_feat, x_raw, y,session_ids):
-----------+++-+        self.device=device
-----------+++-+
-----------+++-+    def create_src_padding_mask(self, src):
-----------+++-+        # input src of shape ()
-----------+++-+        src_padding_mask = src.transpose(1, 0) == 0
-----------+++-+        return src_padding_mask
-----------+++-+
-----------+++-+    def create_tgt_padding_mask(self, tgt):
-----------+++-+        # input tgt of shape ()
-----------+++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-----------+++-+        return tgt_padding_mask
-----------+++-+    
-----------+++-+    def forward(self, x_feat, x_raw, y, session_ids):
-----------+++-         # x shape is (batch, time, electrode)
-----------+++-+        # y shape is (batch, sequence_length)
-----------+++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-----------+++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-----------+++-+        memory_key_padding_mask = src_key_padding_mask
-----------+++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-----------+++- 
-----------+++-         if self.training:
-----------+++-             r = random.randrange(8)
-----------+++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-----------+++-         x_raw = self.conv_blocks(x_raw)
-----------+++-         x_raw = x_raw.transpose(1,2)
-----------+++-         x_raw = self.w_raw_in(x_raw)
-----------+++--
-----------+++-         x = x_raw
-----------+++-+
-----------+++-+        #Embedding and positional encoding of tgt
-----------+++-         tgt=self.embedding_tgt(y)
-----------+++-+        tgt=self.pos_encoder(tgt)
-----------+++- 
-----------+++-         x = x.transpose(0,1) # put time first
-----------+++-         tgt = tgt.transpose(0,1) # put channel after
-----------+++--        x_encoder = self.transformerEncoder(x)
-----------+++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------+++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-----------+++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-----------+++- 
-----------+++-         x_encoder = x_encoder.transpose(0,1)
-----------+++-         x_decoder = x_decoder.transpose(0,1)
-----------+++ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++-index 53839a0..1343d7d 100644
-----------++++index 2243f2d..342fccd 100644
-----------+++ --- a/models/recognition_model/log.txt
-----------+++ +++ b/models/recognition_model/log.txt
-----------+++-@@ -1,639 +1,2 @@
-----------+++--2fa943cd85263a152b6be80d502eda27932ebb27
-----------+++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------++++@@ -1,845 +1,2 @@
-----------++++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------+++++be71135adc89793578f304adb405cea80a5b2b9a
-----------+++  
-----------+++ -diff --git a/architecture.py b/architecture.py
-----------+++--index a8c70f3..2413a8a 100644
-----------++++-index 2413a8a..94d0de0 100644
-----------+++ ---- a/architecture.py
-----------+++ -+++ b/architecture.py
-----------++++-@@ -4,7 +4,7 @@ import torch
-----------++++- from torch import nn
-----------++++- import torch.nn.functional as F
-----------++++- 
-----------++++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------++++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-----------++++- 
-----------++++- from absl import flags
-----------++++- FLAGS = flags.FLAGS
-----------+++ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------+++ -         return F.relu(x + res)
-----------+++ - 
-----------+++ - class Model(nn.Module):
-----------+++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-----------+++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-----------+++ -         super().__init__()
-----------+++ - 
-----------+++ -         self.conv_blocks = nn.Sequential(
-----------+++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-----------+++--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+++--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-----------++++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+++ - 
-----------+++---        self.has_aux_out = num_aux_outs is not None
-----------+++---        if self.has_aux_out:
-----------+++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------+++--+        self.has_aux_loss = has_aux_loss
-----------++++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-----------+++ - 
-----------+++--     def forward(self, x_feat, x_raw, y,session_ids):
-----------+++--         # x shape is (batch, time, electrode)
-----------+++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-----------++++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-----------++++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+++ - 
-----------+++--         x = x.transpose(0,1) # put time first
-----------+++--         tgt = tgt.transpose(0,1) # put channel after
-----------+++---        x = self.transformerEncoder(x)
-----------+++---        x = self.transformerDecoder(tgt, x)
-----------+++---        x = x.transpose(0,1)
-----------+++--+        x_encoder = self.transformerEncoder(x)
-----------+++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++++-         self.has_aux_loss = has_aux_loss
-----------++++--
-----------++++--    def forward(self, x_feat, x_raw, y,session_ids):
-----------++++-+        self.device=device
-----------++++-+
-----------++++-+    def create_src_padding_mask(self, src):
-----------++++-+        # input src of shape ()
-----------++++-+        src_padding_mask = src.transpose(1, 0) == 0
-----------++++-+        return src_padding_mask
-----------++++-+
-----------++++-+    def create_tgt_padding_mask(self, tgt):
-----------++++-+        # input tgt of shape ()
-----------++++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-----------++++-+        return tgt_padding_mask
-----------++++-+    
-----------++++-+    def forward(self, x_feat, x_raw, y, session_ids):
-----------++++-         # x shape is (batch, time, electrode)
-----------++++-+        # y shape is (batch, sequence_length)
-----------++++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-----------++++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-----------++++-+        memory_key_padding_mask = src_key_padding_mask
-----------++++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-----------+++ - 
-----------+++---        if self.has_aux_out:
-----------+++---            return self.w_out(x), self.w_aux(x)
-----------+++--+        x_encoder = x_encoder.transpose(0,1)
-----------+++--+        x_decoder = x_decoder.transpose(0,1)
-----------++++-         if self.training:
-----------++++-             r = random.randrange(8)
-----------++++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-----------++++-         x_raw = self.conv_blocks(x_raw)
-----------++++-         x_raw = x_raw.transpose(1,2)
-----------++++-         x_raw = self.w_raw_in(x_raw)
-----------++++--
-----------++++-         x = x_raw
-----------+++ -+
-----------+++--+        if self.has_aux_loss:
-----------+++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------+++--         else:
-----------+++--             return self.w_out(x)
-----------++++-+        #Embedding and positional encoding of tgt
-----------++++-         tgt=self.embedding_tgt(y)
-----------++++-+        tgt=self.pos_encoder(tgt)
-----------+++ - 
-----------+++--diff --git a/data_utils.py b/data_utils.py
-----------+++--index e2632e8..8b05213 100644
-----------+++----- a/data_utils.py
-----------+++--+++ b/data_utils.py
-----------+++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-----------++++-         x = x.transpose(0,1) # put time first
-----------++++-         tgt = tgt.transpose(0,1) # put channel after
-----------++++--        x_encoder = self.transformerEncoder(x)
-----------++++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-----------++++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-----------+++ - 
-----------+++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-----------+++--     total_length = sum(t.size(0) for t in tensor_list)
-----------+++--+    tensor_list = list(tensor_list) # copy
-----------+++--     if total_length % n_batch != 0:
-----------+++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------+++---        tensor_list = list(tensor_list) # copy
-----------+++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------+++--         total_length += pad_length
-----------+++--     tensor = torch.cat(tensor_list, 0)
-----------++++-         x_encoder = x_encoder.transpose(0,1)
-----------++++-         x_decoder = x_decoder.transpose(0,1)
-----------+++ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++--index b8f7791..617dd85 100644
-----------++++-index 53839a0..1343d7d 100644
-----------+++ ---- a/models/recognition_model/log.txt
-----------+++ -+++ b/models/recognition_model/log.txt
-----------+++--@@ -1,480 +1,2 @@
-----------+++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------+++--+2fa943cd85263a152b6be80d502eda27932ebb27
-----------++++-@@ -1,639 +1,2 @@
-----------++++--2fa943cd85263a152b6be80d502eda27932ebb27
-----------++++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-----------+++ - 
-----------+++ --diff --git a/architecture.py b/architecture.py
-----------+++---index d6e99b4..a8c70f3 100644
-----------++++--index a8c70f3..2413a8a 100644
-----------+++ ----- a/architecture.py
-----------+++ --+++ b/architecture.py
-----------+++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-----------+++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-----------++++--         return F.relu(x + res)
-----------++++-- 
-----------++++-- class Model(nn.Module):
-----------++++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-----------++++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-----------++++--         super().__init__()
-----------+++ -- 
-----------+++---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++++--         self.conv_blocks = nn.Sequential(
-----------++++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-----------+++ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+++ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++++-- 
-----------++++---        self.has_aux_out = num_aux_outs is not None
-----------++++---        if self.has_aux_out:
-----------++++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------++++--+        self.has_aux_loss = has_aux_loss
-----------++++-- 
-----------++++--     def forward(self, x_feat, x_raw, y,session_ids):
-----------++++--         # x shape is (batch, time, electrode)
-----------++++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-----------++++-- 
-----------++++--         x = x.transpose(0,1) # put time first
-----------++++--         tgt = tgt.transpose(0,1) # put channel after
-----------++++---        x = self.transformerEncoder(x)
-----------++++---        x = self.transformerDecoder(tgt, x)
-----------++++---        x = x.transpose(0,1)
-----------++++--+        x_encoder = self.transformerEncoder(x)
-----------++++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-----------++++-- 
-----------++++---        if self.has_aux_out:
-----------++++---            return self.w_out(x), self.w_aux(x)
-----------++++--+        x_encoder = x_encoder.transpose(0,1)
-----------++++--+        x_decoder = x_decoder.transpose(0,1)
-----------++++--+
-----------++++--+        if self.has_aux_loss:
-----------++++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-----------++++--         else:
-----------++++--             return self.w_out(x)
-----------++++-- 
-----------+++ --diff --git a/data_utils.py b/data_utils.py
-----------+++---index e4ac852..e2632e8 100644
-----------++++--index e2632e8..8b05213 100644
-----------+++ ----- a/data_utils.py
-----------+++ --+++ b/data_utils.py
-----------+++---@@ -1,3 +1,4 @@
-----------+++---+import math
-----------+++--- import string
-----------++++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-----------+++ -- 
-----------+++--- import numpy as np
-----------+++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-----------+++---     n = total_length // length
-----------+++---     return tensor.view(n, length, *tensor.size()[1:])
-----------+++--- 
-----------+++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-----------+++---+    total_length = sum(t.size(0) for t in tensor_list)
-----------+++---+    if total_length % n_batch != 0:
-----------+++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------+++---+        tensor_list = list(tensor_list) # copy
-----------+++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------+++---+        total_length += pad_length
-----------+++---+    tensor = torch.cat(tensor_list, 0)
-----------+++---+    length = total_length // n_batch
-----------+++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-----------+++---+
-----------+++--- def decollate_tensor(tensor, lengths):
-----------+++---     b, s, d = tensor.size()
-----------+++---     tensor = tensor.view(b*s, d)
-----------++++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-----------++++--     total_length = sum(t.size(0) for t in tensor_list)
-----------++++--+    tensor_list = list(tensor_list) # copy
-----------++++--     if total_length % n_batch != 0:
-----------++++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------++++---        tensor_list = list(tensor_list) # copy
-----------++++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------++++--         total_length += pad_length
-----------++++--     tensor = torch.cat(tensor_list, 0)
-----------+++ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++---index e890f0f..1ee3421 100644
-----------++++--index b8f7791..617dd85 100644
-----------+++ ----- a/models/recognition_model/log.txt
-----------+++ --+++ b/models/recognition_model/log.txt
-----------+++---@@ -1,265 +1,2 @@
-----------+++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------+++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------++++--@@ -1,480 +1,2 @@
-----------++++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------++++--+2fa943cd85263a152b6be80d502eda27932ebb27
-----------+++ -- 
-----------+++ ---diff --git a/architecture.py b/architecture.py
-----------+++----index b22af61..d6e99b4 100644
-----------++++---index d6e99b4..a8c70f3 100644
-----------+++ ------ a/architecture.py
-----------+++ ---+++ b/architecture.py
-----------+++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-----------+++----         )
-----------+++----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------++++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-----------++++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+++ --- 
-----------+++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------+++----+
-----------+++ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-----------+++----         if self.has_aux_out:
-----------+++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------+++---- 
-----------+++-----    def forward(self, x_feat, x_raw, session_ids):
-----------+++----+    def forward(self, x_feat, x_raw, y,session_ids):
-----------+++----         # x shape is (batch, time, electrode)
-----------+++---- 
-----------+++----         if self.training:
-----------+++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-----------+++----         x_raw = self.w_raw_in(x_raw)
-----------+++---- 
-----------+++----         x = x_raw
-----------+++----+        tgt=self.embedding_tgt(y)
-----------+++---- 
-----------+++----         x = x.transpose(0,1) # put time first
-----------+++----+        tgt = tgt.transpose(0,1) # put channel after
-----------+++----         x = self.transformerEncoder(x)
-----------+++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------+++----+        x = self.transformerDecoder(tgt, x)
-----------+++----         x = x.transpose(0,1)
-----------+++---- 
-----------+++----         if self.has_aux_out:
-----------++++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------++++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------+++ ---diff --git a/data_utils.py b/data_utils.py
-----------+++----index 11d4805..e4ac852 100644
-----------++++---index e4ac852..e2632e8 100644
-----------+++ ------ a/data_utils.py
-----------+++ ---+++ b/data_utils.py
-----------+++----@@ -244,6 +244,7 @@ class TextTransform(object):
-----------+++----     def __init__(self):
-----------+++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-----------+++----         self.chars = string.ascii_lowercase+string.digits+' '
-----------+++----+        self.vocabulary_size=len(self.chars)
-----------++++---@@ -1,3 +1,4 @@
-----------++++---+import math
-----------++++--- import string
-----------++++--- 
-----------++++--- import numpy as np
-----------++++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-----------++++---     n = total_length // length
-----------++++---     return tensor.view(n, length, *tensor.size()[1:])
-----------+++ --- 
-----------+++----     def clean_text(self, text):
-----------+++----         text = unidecode(text)
-----------++++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-----------++++---+    total_length = sum(t.size(0) for t in tensor_list)
-----------++++---+    if total_length % n_batch != 0:
-----------++++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-----------++++---+        tensor_list = list(tensor_list) # copy
-----------++++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-----------++++---+        total_length += pad_length
-----------++++---+    tensor = torch.cat(tensor_list, 0)
-----------++++---+    length = total_length // n_batch
-----------++++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-----------++++---+
-----------++++--- def decollate_tensor(tensor, lengths):
-----------++++---     b, s, d = tensor.size()
-----------++++---     tensor = tensor.view(b*s, d)
-----------+++ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++----index fbc0abb..400061a 100644
-----------++++---index e890f0f..1ee3421 100644
-----------+++ ------ a/models/recognition_model/log.txt
-----------+++ ---+++ b/models/recognition_model/log.txt
-----------+++----@@ -1,188 +1,2 @@
-----------+++-----57f8139449dd9286c2203ec2eca118a550638a7c
-----------+++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------++++---@@ -1,265 +1,2 @@
-----------++++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------++++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-----------+++ --- 
-----------+++ ----diff --git a/architecture.py b/architecture.py
-----------+++-----index 4fc3793..b22af61 100644
-----------++++----index b22af61..d6e99b4 100644
-----------+++ ------- a/architecture.py
-----------+++ ----+++ b/architecture.py
-----------+++-----@@ -4,7 +4,7 @@ import torch
-----------+++----- from torch import nn
-----------+++----- import torch.nn.functional as F
-----------+++----- 
-----------+++------from transformer import TransformerEncoderLayer
-----------+++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------+++----- 
-----------+++----- from absl import flags
-----------+++----- FLAGS = flags.FLAGS
-----------+++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-----------++++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-----------++++----         )
-----------+++ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------+++ ---- 
-----------++++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----------++++----+
-----------+++ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------+++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------+++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------+++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-----------++++----         if self.has_aux_out:
-----------++++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-----------++++---- 
-----------++++-----    def forward(self, x_feat, x_raw, session_ids):
-----------++++----+    def forward(self, x_feat, x_raw, y,session_ids):
-----------++++----         # x shape is (batch, time, electrode)
-----------++++---- 
-----------++++----         if self.training:
-----------++++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-----------++++----         x_raw = self.w_raw_in(x_raw)
-----------+++ ---- 
-----------+++-----         self.has_aux_out = num_aux_outs is not None
-----------+++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-----------+++ ----         x = x_raw
-----------++++----+        tgt=self.embedding_tgt(y)
-----------+++ ---- 
-----------+++ ----         x = x.transpose(0,1) # put time first
-----------+++------        x = self.transformer(x)
-----------+++-----+        x = self.transformerEncoder(x)
-----------+++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++++----+        tgt = tgt.transpose(0,1) # put channel after
-----------++++----         x = self.transformerEncoder(x)
-----------++++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++++----+        x = self.transformerDecoder(tgt, x)
-----------+++ ----         x = x.transpose(0,1)
-----------+++ ---- 
-----------+++ ----         if self.has_aux_out:
-----------++++----diff --git a/data_utils.py b/data_utils.py
-----------++++----index 11d4805..e4ac852 100644
-----------++++------- a/data_utils.py
-----------++++----+++ b/data_utils.py
-----------++++----@@ -244,6 +244,7 @@ class TextTransform(object):
-----------++++----     def __init__(self):
-----------++++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-----------++++----         self.chars = string.ascii_lowercase+string.digits+' '
-----------++++----+        self.vocabulary_size=len(self.chars)
-----------++++---- 
-----------++++----     def clean_text(self, text):
-----------++++----         text = unidecode(text)
-----------+++ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------+++-----index 571de9d..8563980 100644
-----------++++----index fbc0abb..400061a 100644
-----------+++ ------- a/models/recognition_model/log.txt
-----------+++ ----+++ b/models/recognition_model/log.txt
-----------+++-----@@ -1,5 +1,2 @@
-----------+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++----@@ -1,188 +1,2 @@
-----------++++-----57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-----------+++ ---- 
-----------++++-----diff --git a/architecture.py b/architecture.py
-----------++++-----index 4fc3793..b22af61 100644
-----------++++-------- a/architecture.py
-----------++++-----+++ b/architecture.py
-----------++++-----@@ -4,7 +4,7 @@ import torch
-----------++++----- from torch import nn
-----------++++----- import torch.nn.functional as F
-----------++++----- 
-----------++++------from transformer import TransformerEncoderLayer
-----------++++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-----------++++----- 
-----------++++----- from absl import flags
-----------++++----- FLAGS = flags.FLAGS
-----------++++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-----------++++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-----------++++----- 
-----------++++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----------++++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----------++++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-----------++++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-----------++++----- 
-----------++++-----         self.has_aux_out = num_aux_outs is not None
-----------++++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-----------++++-----         x = x_raw
-----------++++----- 
-----------++++-----         x = x.transpose(0,1) # put time first
-----------++++------        x = self.transformer(x)
-----------++++-----+        x = self.transformerEncoder(x)
-----------++++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-----------++++-----         x = x.transpose(0,1)
-----------++++----- 
-----------++++-----         if self.has_aux_out:
-----------++++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----------++++-----index 571de9d..8563980 100644
-----------++++-------- a/models/recognition_model/log.txt
-----------++++-----+++ b/models/recognition_model/log.txt
-----------++++-----@@ -1,5 +1,2 @@
-----------++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++----- 
-----------++++------
-----------++++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-----------++++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------++++------train / dev split: 8055 200
-----------++++-----diff --git a/output/log.txt b/output/log.txt
-----------++++-----index ae42364..1d2cd8e 100644
-----------++++-------- a/output/log.txt
-----------++++-----+++ b/output/log.txt
-----------++++-----@@ -1,3 +1,13 @@
-----------++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++----- 
-----------++++-----+diff --git a/output/log.txt b/output/log.txt
-----------++++-----+index ae42364..8563980 100644
-----------++++-----+--- a/output/log.txt
-----------++++-----++++ b/output/log.txt
-----------++++-----+@@ -1,3 +1,2 @@
-----------++++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++-----+ 
-----------++++-----+-
-----------++++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++++----- 
-----------++++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++++-----diff --git a/transformer.py b/transformer.py
-----------++++-----index 6743588..ac131be 100644
-----------++++-------- a/transformer.py
-----------++++-----+++ b/transformer.py
-----------++++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-----------++++-----         Shape:
-----------++++-----             see the docs in Transformer class.
-----------++++-----         """
-----------++++------        src2 = self.self_attn(src)
-----------++++-----+        src2 = self.self_attn(src, src, src)
-----------++++-----         src = src + self.dropout1(src2)
-----------++++-----         src = self.norm1(src)
-----------++++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------++++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-----------++++-----         src = self.norm2(src)
-----------++++-----         return src
-----------++++----- 
-----------++++-----+class TransformerDecoderLayer(nn.Module):
-----------++++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-----------++++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-----------++++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-----------++++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-----------++++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-----------++++-----+    in a different way during application.
-----------++++-----+
-----------++++-----+    Args:
-----------++++-----+        d_model: the number of expected features in the input (required).
-----------++++-----+        nhead: the number of heads in the multiheadattention models (required).
-----------++++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-----------++++-----+        dropout: the dropout value (default=0.1).
-----------++++-----+        activation: the activation function of the intermediate layer, can be a string
-----------++++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-----------++++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-----------++++-----+        batch_first: If ``True``, then the input and output tensors are provided
-----------++++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-----------++++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-----------++++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-----------++++-----+            Default: ``False`` (after).
-----------++++-----+
-----------++++-----+    Examples::
-----------++++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-----------++++-----+        >>> memory = torch.rand(10, 32, 512)
-----------++++-----+        >>> tgt = torch.rand(20, 32, 512)
-----------++++-----+        >>> out = decoder_layer(tgt, memory)
-----------++++-----+    """
-----------++++-----+    # Adapted from pytorch source
-----------++++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------++++-----+        super(TransformerDecoderLayer, self).__init__()
-----------++++-----+        #Attention Mechanism
-----------++++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------++++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------++++-----+        # Implementation of Feedforward model
-----------++++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-----------++++-----+        self.dropout = nn.Dropout(dropout)
-----------++++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-----------++++-----+        #Normalization Layer and Dropout Layer
-----------++++-----+        self.norm1 = nn.LayerNorm(d_model)
-----------++++-----+        self.norm2 = nn.LayerNorm(d_model)
-----------++++-----+        self.norm3 = nn.LayerNorm(d_model)
-----------++++-----+        self.dropout1 = nn.Dropout(dropout)
-----------++++-----+        self.dropout2 = nn.Dropout(dropout)
-----------++++-----+        self.dropout3 = nn.Dropout(dropout)
-----------++++-----+        #Activation Function
-----------++++-----+        self.activation = nn.ReLU()
-----------++++-----+    
-----------++++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-----------++++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-----------++++-----+        r"""Pass the input through the encoder layer.
-----------++++-----+
-----------++++-----+        Args:
-----------++++-----+            tgt: the sequence to the decoder layer (required).
-----------++++-----+            memory: the sequence from the last layer of the encoder (required).
-----------++++-----+            tgt_mask: the mask for the tgt sequence (optional).
-----------++++-----+            memory_mask: the mask for the memory sequence (optional).
-----------++++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-----------++++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-----------++++-----+
-----------++++-----+        Shape:
-----------++++-----+            see the docs in Transformer class.
-----------++++-----+        """
-----------++++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------++++-----+        tgt = tgt + self.dropout1(tgt2)
-----------++++-----+        tgt = self.norm1(tgt)
-----------++++-----+
-----------++++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-----------++++-----+        tgt = tgt + self.dropout1(tgt2)
-----------++++-----+        tgt = self.norm1(tgt)
-----------++++-----+
-----------++++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-----------++++-----+        tgt = tgt + self.dropout2(tgt2)
-----------++++-----+        tgt = self.norm2(tgt)
-----------++++-----+        return tgt
-----------++++-----+    
-----------++++-----+
-----------++++----- class MultiHeadAttention(nn.Module):
-----------++++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------++++-----     super().__init__()
-----------++++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-----------++++-----     else:
-----------++++-----         self.relative_positional = None
-----------++++----- 
-----------++++------  def forward(self, x):
-----------++++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------++++-----     """Runs the multi-head self-attention layer.
-----------++++----- 
-----------++++-----     Args:
-----------++++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-----------++++-----       A single tensor containing the output from this layer
-----------++++-----     """
-----------++++----- 
-----------++++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-----------++++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-----------++++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-----------++++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-----------++++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-----------++++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------++++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------++++----- 
-----------++++-----     if self.relative_positional is not None:
-----------+++ -----
-----------+++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-----------++++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++ -----train / dev split: 8055 200
-----------+++-----diff --git a/output/log.txt b/output/log.txt
-----------+++-----index ae42364..1d2cd8e 100644
-----------+++-------- a/output/log.txt
-----------+++-----+++ b/output/log.txt
-----------+++-----@@ -1,3 +1,13 @@
-----------+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++----diff --git a/recognition_model.py b/recognition_model.py
-----------++++----index dea6d47..a46dff0 100644
-----------++++------- a/recognition_model.py
-----------++++----+++ b/recognition_model.py
-----------++++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------+++ ---- 
-----------+++-----+diff --git a/output/log.txt b/output/log.txt
-----------+++-----+index ae42364..8563980 100644
-----------+++-----+--- a/output/log.txt
-----------+++-----++++ b/output/log.txt
-----------+++-----+@@ -1,3 +1,2 @@
-----------+++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-----------+++-----+ 
-----------+++-----+-
-----------+++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++++----             X = combine_fixed_length(example['emg'], 200).to(device)
-----------++++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------++++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+++ ---- 
-----------+++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+++-----diff --git a/transformer.py b/transformer.py
-----------+++-----index 6743588..ac131be 100644
-----------+++-------- a/transformer.py
-----------+++-----+++ b/transformer.py
-----------+++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-----------+++-----         Shape:
-----------+++-----             see the docs in Transformer class.
-----------+++-----         """
-----------+++------        src2 = self.self_attn(src)
-----------+++-----+        src2 = self.self_attn(src, src, src)
-----------+++-----         src = src + self.dropout1(src2)
-----------+++-----         src = self.norm1(src)
-----------+++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------+++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-----------+++-----         src = self.norm2(src)
-----------+++-----         return src
-----------++++-----            pred = model(X, X_raw, sess)
-----------++++----+            pred = model(X, X_raw, tgt, sess)
-----------++++----             pred = F.log_softmax(pred, 2)
-----------+++ ---- 
-----------+++-----+class TransformerDecoderLayer(nn.Module):
-----------+++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-----------+++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-----------+++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-----------+++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-----------+++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-----------+++-----+    in a different way during application.
-----------+++-----+
-----------+++-----+    Args:
-----------+++-----+        d_model: the number of expected features in the input (required).
-----------+++-----+        nhead: the number of heads in the multiheadattention models (required).
-----------+++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-----------+++-----+        dropout: the dropout value (default=0.1).
-----------+++-----+        activation: the activation function of the intermediate layer, can be a string
-----------+++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-----------+++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-----------+++-----+        batch_first: If ``True``, then the input and output tensors are provided
-----------+++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-----------+++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-----------+++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-----------+++-----+            Default: ``False`` (after).
-----------+++-----+
-----------+++-----+    Examples::
-----------+++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-----------+++-----+        >>> memory = torch.rand(10, 32, 512)
-----------+++-----+        >>> tgt = torch.rand(20, 32, 512)
-----------+++-----+        >>> out = decoder_layer(tgt, memory)
-----------+++-----+    """
-----------+++-----+    # Adapted from pytorch source
-----------+++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------+++-----+        super(TransformerDecoderLayer, self).__init__()
-----------+++-----+        #Attention Mechanism
-----------+++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------+++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-----------+++-----+        # Implementation of Feedforward model
-----------+++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-----------+++-----+        self.dropout = nn.Dropout(dropout)
-----------+++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-----------+++-----+        #Normalization Layer and Dropout Layer
-----------+++-----+        self.norm1 = nn.LayerNorm(d_model)
-----------+++-----+        self.norm2 = nn.LayerNorm(d_model)
-----------+++-----+        self.norm3 = nn.LayerNorm(d_model)
-----------+++-----+        self.dropout1 = nn.Dropout(dropout)
-----------+++-----+        self.dropout2 = nn.Dropout(dropout)
-----------+++-----+        self.dropout3 = nn.Dropout(dropout)
-----------+++-----+        #Activation Function
-----------+++-----+        self.activation = nn.ReLU()
-----------+++-----+    
-----------+++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-----------+++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-----------+++-----+        r"""Pass the input through the encoder layer.
-----------+++-----+
-----------+++-----+        Args:
-----------+++-----+            tgt: the sequence to the decoder layer (required).
-----------+++-----+            memory: the sequence from the last layer of the encoder (required).
-----------+++-----+            tgt_mask: the mask for the tgt sequence (optional).
-----------+++-----+            memory_mask: the mask for the memory sequence (optional).
-----------+++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-----------+++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-----------+++-----+
-----------+++-----+        Shape:
-----------+++-----+            see the docs in Transformer class.
-----------+++-----+        """
-----------+++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+++-----+        tgt = tgt + self.dropout1(tgt2)
-----------+++-----+        tgt = self.norm1(tgt)
-----------+++-----+
-----------+++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-----------+++-----+        tgt = tgt + self.dropout1(tgt2)
-----------+++-----+        tgt = self.norm1(tgt)
-----------+++-----+
-----------+++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-----------+++-----+        tgt = tgt + self.dropout2(tgt2)
-----------+++-----+        tgt = self.norm2(tgt)
-----------+++-----+        return tgt
-----------+++-----+    
-----------+++-----+
-----------+++----- class MultiHeadAttention(nn.Module):
-----------+++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----------+++-----     super().__init__()
-----------+++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-----------+++-----     else:
-----------+++-----         self.relative_positional = None
-----------+++----- 
-----------+++------  def forward(self, x):
-----------+++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------+++-----     """Runs the multi-head self-attention layer.
-----------+++----- 
-----------+++-----     Args:
-----------+++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-----------+++-----       A single tensor containing the output from this layer
-----------+++-----     """
-----------+++----- 
-----------+++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-----------+++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-----------+++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-----------+++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-----------+++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-----------+++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------+++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+++----- 
-----------+++-----     if self.relative_positional is not None:
-----------++++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+++ ----
-----------+++ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++ ----train / dev split: 8055 200
-----------+++ ---diff --git a/recognition_model.py b/recognition_model.py
-----------+++----index dea6d47..a46dff0 100644
-----------++++---index a46dff0..8fd300c 100644
-----------+++ ------ a/recognition_model.py
-----------+++ ---+++ b/recognition_model.py
-----------+++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------++++---@@ -6,6 +6,7 @@ import subprocess
-----------++++--- from ctcdecode import CTCBeamDecoder
-----------++++--- import jiwer
-----------++++--- import random
-----------++++---+from torch.utils.tensorboard import SummaryWriter
-----------++++--- 
-----------++++--- import torch
-----------++++--- from torch import nn
-----------++++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-----------++++--- 
-----------++++--- from read_emg import EMGDataset, SizeAwareSampler
-----------++++--- from architecture import Model
-----------++++----from data_utils import combine_fixed_length, decollate_tensor
-----------++++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-----------++++--- from transformer import TransformerEncoderLayer
-----------++++--- 
-----------++++--- from absl import flags
-----------++++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-----------++++---     return jiwer.wer(references, predictions)
-----------++++--- 
-----------++++--- 
-----------++++----def train_model(trainset, devset, device, n_epochs=200):
-----------++++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++++----
-----------++++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------++++---+    #Define Dataloader
-----------++++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++++--- 
-----------++++---+    #Define model and loss function
-----------++++---     n_chars = len(devset.text_transform.chars)
-----------++++---     model = Model(devset.num_features, n_chars+1).to(device)
-----------++++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------++++--- 
-----------++++---     if FLAGS.start_training_from is not None:
-----------++++---         state_dict = torch.load(FLAGS.start_training_from)
-----------++++---         model.load_state_dict(state_dict, strict=False)
-----------++++--- 
-----------++++---+    #Define optimizer and scheduler for the learning rate
-----------++++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-----------++++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-----------++++--- 
-----------++++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------++++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-----------+++ --- 
-----------++++---     batch_idx = 0
-----------++++---+    train_loss= 0
-----------++++---+    eval_loss = 0
-----------++++---     optim.zero_grad()
-----------++++---     for epoch_idx in range(n_epochs):
-----------++++---+        model.train()
-----------++++---         losses = []
-----------++++----        for example in dataloader:
-----------++++---+        for example in dataloader_training:
-----------++++---             schedule_lr(batch_idx)
-----------++++--- 
-----------++++---+            #Preprosessing of the input and target for the model
-----------+++ ---             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+++ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------++++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------+++ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------++++--- 
-----------++++---+            #Shifting target for input decoder and loss
-----------++++---+            tgt= y[:,:-1]
-----------++++---+            target= y[:,1:]
-----------++++---+
-----------++++---+            #Prediction
-----------++++---             pred = model(X, X_raw, tgt, sess)
-----------++++----            pred = F.log_softmax(pred, 2)
-----------++++--- 
-----------++++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------++++---+            #Primary Loss
-----------++++---+            pred=pred.permute(0,2,1)
-----------++++---+            loss = loss_fn(pred, target)
-----------++++---+
-----------++++---+            #Auxiliary Loss
-----------++++---+            #pred = F.log_softmax(pred, 2)
-----------++++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------++++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++++---             losses.append(loss.item())
-----------++++---+            train_loss += loss.item()
-----------++++--- 
-----------++++---             loss.backward()
-----------++++---             if (batch_idx+1) % 2 == 0:
-----------++++---                 optim.step()
-----------++++---                 optim.zero_grad()
-----------++++--- 
-----------++++---+            #Report plots in tensorboard
-----------++++---+            if batch_idx % report_every == report_every - 2:     
-----------++++---+                #Evaluation
-----------++++---+                model.eval()
-----------++++---+                with torch.no_grad():
-----------++++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------++++---+                        X_raw = example['raw_emg'].to(device)
-----------++++---+                        sess = example['session_ids'].to(device)
-----------++++---+                        y = example['text_int'].to(device)
-----------++++---+
-----------++++---+                        #Shifting target for input decoder and loss
-----------++++---+                        tgt= y[:,:-1]
-----------++++---+                        target= y[:,1:]
-----------++++---+
-----------++++---+                        #Prediction without the 197-th batch because of missing label
-----------++++---+                        if idx != 197:
-----------++++---+                            pred = model(X, X_raw, tgt, sess)
-----------++++---+                            #Primary Loss
-----------++++---+                            pred=pred.permute(0,2,1)
-----------++++---+                            loss = loss_fn(pred, target)
-----------++++---+                            eval_loss += loss.item()
-----------++++---+
-----------++++---+                #Writing on tensorboard
-----------++++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-----------++++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-----------++++---+                train_loss= 0
-----------++++---+                eval_loss= 0
-----------++++---+
-----------++++---+            #Increment counter        
-----------++++---             batch_idx += 1
-----------++++----        train_loss = np.mean(losses)
-----------++++---+
-----------++++---+        #Testing and change learning rate
-----------++++---         val = test(model, devset, device)
-----------++++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-----------++++---         lr_sched.step()
-----------++++---+    
-----------++++---+        #Logging
-----------++++---+        train_loss = np.mean(losses)
-----------++++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-----------++++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-----------+++ --- 
-----------+++-----            pred = model(X, X_raw, sess)
-----------+++----+            pred = model(X, X_raw, tgt, sess)
-----------+++----             pred = F.log_softmax(pred, 2)
-----------++++---@@ -148,8 +201,9 @@ def main():
-----------++++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-----------+++ --- 
-----------+++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-----------++++---+    writer = SummaryWriter(log_dir="./content/runs")
-----------++++--- 
-----------++++----    model = train_model(trainset, devset, device)
-----------++++---+    model = train_model(trainset, devset ,device, writer)
-----------++++--- 
-----------++++--- if __name__ == '__main__':
-----------++++---     FLAGS(sys.argv)
-----------++++---diff --git a/transformer.py b/transformer.py
-----------++++---index ac131be..51e1f2e 100644
-----------++++------ a/transformer.py
-----------++++---+++ b/transformer.py
-----------++++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-----------++++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------++++---     self.d_qkv = d_qkv
-----------++++--- 
-----------++++---+    #self.kdim = kdim if kdim is not None else embed_dim
-----------++++---+    #self.vdim = vdim if vdim is not None else embed_dim
-----------++++---+
-----------++++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++ ---
-----------+++ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++ ---train / dev split: 8055 200
-----------+++ --diff --git a/recognition_model.py b/recognition_model.py
-----------+++---index a46dff0..8fd300c 100644
-----------++++--index fde5a40..6d5143b 100644
-----------+++ ----- a/recognition_model.py
-----------+++ --+++ b/recognition_model.py
-----------+++---@@ -6,6 +6,7 @@ import subprocess
-----------+++--- from ctcdecode import CTCBeamDecoder
-----------+++--- import jiwer
-----------+++--- import random
-----------+++---+from torch.utils.tensorboard import SummaryWriter
-----------+++--- 
-----------+++--- import torch
-----------+++--- from torch import nn
-----------+++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-----------+++--- 
-----------+++--- from read_emg import EMGDataset, SizeAwareSampler
-----------+++--- from architecture import Model
-----------+++----from data_utils import combine_fixed_length, decollate_tensor
-----------+++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-----------+++--- from transformer import TransformerEncoderLayer
-----------+++--- 
-----------+++--- from absl import flags
-----------+++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-----------++++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-----------+++ --     return jiwer.wer(references, predictions)
-----------+++ -- 
-----------+++ -- 
-----------+++----def train_model(trainset, devset, device, n_epochs=200):
-----------+++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+++----
-----------+++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------+++---+    #Define Dataloader
-----------+++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------++++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-----------++++--     #Define Dataloader
-----------++++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------++++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------+++ -- 
-----------+++---+    #Define model and loss function
-----------++++--     #Define model and loss function
-----------+++ --     n_chars = len(devset.text_transform.chars)
-----------+++---     model = Model(devset.num_features, n_chars+1).to(device)
-----------+++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------++++---    model = Model(devset.num_features, n_chars+1).to(device)
-----------++++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------+++ -- 
-----------+++ --     if FLAGS.start_training_from is not None:
-----------+++---         state_dict = torch.load(FLAGS.start_training_from)
-----------+++---         model.load_state_dict(state_dict, strict=False)
-----------++++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------++++--             target= y[:,1:]
-----------+++ -- 
-----------+++---+    #Define optimizer and scheduler for the learning rate
-----------+++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-----------+++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-----------++++--             #Prediction
-----------++++---            pred = model(X, X_raw, tgt, sess)
-----------++++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-----------+++ -- 
-----------+++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-----------+++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-----------++++--             #Primary Loss
-----------++++---            pred=pred.permute(0,2,1)
-----------++++---            loss = loss_fn(pred, target)
-----------++++--+            out_dec=out_dec.permute(0,2,1)
-----------++++--+            loss_dec = loss_fn(out_dec, target)
-----------+++ -- 
-----------+++---     batch_idx = 0
-----------+++---+    train_loss= 0
-----------+++---+    eval_loss = 0
-----------+++---     optim.zero_grad()
-----------+++---     for epoch_idx in range(n_epochs):
-----------+++---+        model.train()
-----------+++---         losses = []
-----------+++----        for example in dataloader:
-----------+++---+        for example in dataloader_training:
-----------+++---             schedule_lr(batch_idx)
-----------+++--- 
-----------+++---+            #Preprosessing of the input and target for the model
-----------+++---             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+++---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----------+++---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+++--- 
-----------+++---+            #Shifting target for input decoder and loss
-----------+++---+            tgt= y[:,:-1]
-----------+++---+            target= y[:,1:]
-----------+++---+
-----------+++---+            #Prediction
-----------+++---             pred = model(X, X_raw, tgt, sess)
-----------+++----            pred = F.log_softmax(pred, 2)
-----------+++--- 
-----------+++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+++---+            #Primary Loss
-----------+++---+            pred=pred.permute(0,2,1)
-----------+++---+            loss = loss_fn(pred, target)
-----------++++--             #Auxiliary Loss
-----------++++---            #pred = F.log_softmax(pred, 2)
-----------++++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------++++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++++--+            out_enc = F.log_softmax(out_enc, 2)
-----------++++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------++++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------++++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+++ --+
-----------+++---+            #Auxiliary Loss
-----------+++---+            #pred = F.log_softmax(pred, 2)
-----------+++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------+++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------++++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-----------+++ --             losses.append(loss.item())
-----------+++---+            train_loss += loss.item()
-----------++++--             train_loss += loss.item()
-----------+++ -- 
-----------+++---             loss.backward()
-----------++++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+++ --             if (batch_idx+1) % 2 == 0:
-----------+++ --                 optim.step()
-----------+++ --                 optim.zero_grad()
-----------++++---
-----------++++---            if batch_idx % report_every == report_every - 2:     
-----------++++--+            
-----------++++--+            if False:
-----------++++--+            #if batch_idx % report_every == report_every - 2:     
-----------++++--                 #Evaluation
-----------++++--                 model.eval()
-----------++++--                 with torch.no_grad():
-----------++++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------++++---                        X_raw = example['raw_emg'].to(device)
-----------++++---                        sess = example['session_ids'].to(device)
-----------++++---                        y = example['text_int'].to(device)
-----------++++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+++ -- 
-----------+++---+            #Report plots in tensorboard
-----------+++---+            if batch_idx % report_every == report_every - 2:     
-----------+++---+                #Evaluation
-----------+++---+                model.eval()
-----------+++---+                with torch.no_grad():
-----------+++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------+++---+                        X_raw = example['raw_emg'].to(device)
-----------+++---+                        sess = example['session_ids'].to(device)
-----------+++---+                        y = example['text_int'].to(device)
-----------+++---+
-----------+++---+                        #Shifting target for input decoder and loss
-----------+++---+                        tgt= y[:,:-1]
-----------+++---+                        target= y[:,1:]
-----------+++---+
-----------+++---+                        #Prediction without the 197-th batch because of missing label
-----------+++---+                        if idx != 197:
-----------+++---+                            pred = model(X, X_raw, tgt, sess)
-----------+++---+                            #Primary Loss
-----------+++---+                            pred=pred.permute(0,2,1)
-----------+++---+                            loss = loss_fn(pred, target)
-----------+++---+                            eval_loss += loss.item()
-----------+++---+
-----------+++---+                #Writing on tensorboard
-----------+++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-----------+++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-----------+++---+                train_loss= 0
-----------+++---+                eval_loss= 0
-----------+++---+
-----------+++---+            #Increment counter        
-----------+++---             batch_idx += 1
-----------+++----        train_loss = np.mean(losses)
-----------+++---+
-----------+++---+        #Testing and change learning rate
-----------+++---         val = test(model, devset, device)
-----------+++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-----------+++---         lr_sched.step()
-----------+++---+    
-----------+++---+        #Logging
-----------+++---+        train_loss = np.mean(losses)
-----------+++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-----------+++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-----------+++--- 
-----------+++---@@ -148,8 +201,9 @@ def main():
-----------+++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-----------+++--- 
-----------+++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-----------+++---+    writer = SummaryWriter(log_dir="./content/runs")
-----------++++--                         #Shifting target for input decoder and loss
-----------++++--                         tgt= y[:,:-1]
-----------++++--                         target= y[:,1:]
-----------+++ -- 
-----------+++----    model = train_model(trainset, devset, device)
-----------+++---+    model = train_model(trainset, devset ,device, writer)
-----------++++--+                        print(idx)
-----------++++--+
-----------++++--                         #Prediction without the 197-th batch because of missing label
-----------++++---                        if idx != 197:
-----------++++--+                        if idx != 181:
-----------++++--                             pred = model(X, X_raw, tgt, sess)
-----------++++--                             #Primary Loss
-----------++++--                             pred=pred.permute(0,2,1)
-----------++++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+++ -- 
-----------+++--- if __name__ == '__main__':
-----------+++---     FLAGS(sys.argv)
-----------+++---diff --git a/transformer.py b/transformer.py
-----------+++---index ac131be..51e1f2e 100644
-----------+++------ a/transformer.py
-----------+++---+++ b/transformer.py
-----------+++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-----------+++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------+++---     self.d_qkv = d_qkv
-----------++++--             #Increment counter        
-----------++++--             batch_idx += 1
-----------++++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------+++ -- 
-----------+++---+    #self.kdim = kdim if kdim is not None else embed_dim
-----------+++---+    #self.vdim = vdim if vdim is not None else embed_dim
-----------+++---+
-----------+++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++--         #Testing and change learning rate
-----------++++--         val = test(model, devset, device)
-----------+++ --
-----------+++ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++ --train / dev split: 8055 200
-----------+++ -diff --git a/recognition_model.py b/recognition_model.py
-----------+++--index fde5a40..6d5143b 100644
-----------++++-index 30c5ff2..2672d45 100644
-----------+++ ---- a/recognition_model.py
-----------+++ -+++ b/recognition_model.py
-----------+++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-----------+++--     return jiwer.wer(references, predictions)
-----------+++-- 
-----------+++-- 
-----------+++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-----------+++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-----------+++--     #Define Dataloader
-----------+++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----------+++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-----------++++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------+++ - 
-----------+++ -     #Define model and loss function
-----------+++ -     n_chars = len(devset.text_transform.chars)
-----------+++---    model = Model(devset.num_features, n_chars+1).to(device)
-----------+++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------++++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-----------+++ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------+++ - 
-----------+++ -     if FLAGS.start_training_from is not None:
-----------+++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+++--             target= y[:,1:]
-----------++++-diff --git a/transformer.py b/transformer.py
-----------++++-index 51e1f2e..c125841 100644
-----------++++---- a/transformer.py
-----------++++-+++ b/transformer.py
-----------++++-@@ -1,3 +1,4 @@
-----------++++-+import math
-----------++++- from typing import Optional
-----------+++ - 
-----------+++--             #Prediction
-----------+++---            pred = model(X, X_raw, tgt, sess)
-----------+++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-----------++++- import torch
-----------++++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-----------++++-         Shape:
-----------++++-             see the docs in Transformer class.
-----------++++-         """
-----------++++--        src2 = self.self_attn(src, src, src)
-----------++++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-----------++++-         src = src + self.dropout1(src2)
-----------++++-         src = self.norm1(src)
-----------++++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------++++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-----------++++-         Shape:
-----------++++-             see the docs in Transformer class.
-----------++++-         """
-----------++++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------++++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------++++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------++++-         tgt = tgt + self.dropout1(tgt2)
-----------++++-         tgt = self.norm1(tgt)
-----------+++ - 
-----------+++--             #Primary Loss
-----------+++---            pred=pred.permute(0,2,1)
-----------+++---            loss = loss_fn(pred, target)
-----------+++--+            out_dec=out_dec.permute(0,2,1)
-----------+++--+            loss_dec = loss_fn(out_dec, target)
-----------++++--        tgt2=self.multihead_attn(tgt, memory, memory)
-----------++++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-----------++++-         tgt = tgt + self.dropout1(tgt2)
-----------++++-         tgt = self.norm1(tgt)
-----------+++ - 
-----------+++--             #Auxiliary Loss
-----------+++---            #pred = F.log_softmax(pred, 2)
-----------+++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-----------+++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-----------+++--+            out_enc = F.log_softmax(out_enc, 2)
-----------+++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------+++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------+++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-----------+++--+
-----------+++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-----------+++--             losses.append(loss.item())
-----------+++--             train_loss += loss.item()
-----------++++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-----------++++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------++++-     self.d_qkv = d_qkv
-----------+++ - 
-----------+++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+++--             if (batch_idx+1) % 2 == 0:
-----------+++--                 optim.step()
-----------+++--                 optim.zero_grad()
-----------++++--    #self.kdim = kdim if kdim is not None else embed_dim
-----------++++--    #self.vdim = vdim if vdim is not None else embed_dim
-----------+++ --
-----------+++---            if batch_idx % report_every == report_every - 2:     
-----------+++--+            
-----------+++--+            if False:
-----------+++--+            #if batch_idx % report_every == report_every - 2:     
-----------+++--                 #Evaluation
-----------+++--                 model.eval()
-----------+++--                 with torch.no_grad():
-----------+++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------+++---                        X_raw = example['raw_emg'].to(device)
-----------+++---                        sess = example['session_ids'].to(device)
-----------+++---                        y = example['text_int'].to(device)
-----------+++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------++++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------++++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-----------++++-     else:
-----------++++-         self.relative_positional = None
-----------+++ - 
-----------+++--                         #Shifting target for input decoder and loss
-----------+++--                         tgt= y[:,:-1]
-----------+++--                         target= y[:,1:]
-----------++++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------++++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-----------++++-     """Runs the multi-head self-attention layer.
-----------+++ - 
-----------+++--+                        print(idx)
-----------+++--+
-----------+++--                         #Prediction without the 197-th batch because of missing label
-----------+++---                        if idx != 197:
-----------+++--+                        if idx != 181:
-----------+++--                             pred = model(X, X_raw, tgt, sess)
-----------+++--                             #Primary Loss
-----------+++--                             pred=pred.permute(0,2,1)
-----------+++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-----------+++-- 
-----------+++--             #Increment counter        
-----------+++--             batch_idx += 1
-----------+++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------++++-     Args:
-----------++++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-----------++++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------++++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+++ - 
-----------+++--         #Testing and change learning rate
-----------+++--         val = test(model, devset, device)
-----------++++-+    if attn_mask is not None:
-----------++++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++++-+
-----------++++-     if self.relative_positional is not None:
-----------++++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------++++-         l,b,h,d = q_pos.size()
-----------++++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-----------++++-             x = x.transpose(0, 1)
-----------++++-             x = x.contiguous().view(bsz_heads, length+1, length)
-----------++++-             return x[:, 1:, :]
-----------++++-+        
-----------++++-+
-----------++++-+########
-----------++++-+# Taken from:
-----------++++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-----------++++-+# or also here:
-----------++++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-----------++++-+class PositionalEncoding(nn.Module):
-----------++++-+
-----------++++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-----------++++-+        super(PositionalEncoding, self).__init__()
-----------++++-+        self.dropout = nn.Dropout(p=dropout)
-----------++++-+        self.max_len = max_len
-----------++++-+
-----------++++-+        pe = torch.zeros(max_len, d_model)
-----------++++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-----------++++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-----------++++-+                             * (-math.log(10000.0) / d_model))
-----------++++-+        pe[:, 0::2] = torch.sin(position * div_term)
-----------++++-+        pe[:, 1::2] = torch.cos(position * div_term)
-----------++++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-----------++++-+        self.register_buffer('pe', pe)  # Will not be trained.
-----------++++-+
-----------++++-+    def forward(self, x):
-----------++++-+        """Inputs of forward function
-----------++++-+        Args:
-----------++++-+            x: the sequence fed to the positional encoder model (required).
-----------++++-+        Shape:
-----------++++-+            x: [sequence length, batch size, embed dim]
-----------++++-+            output: [sequence length, batch size, embed dim]
-----------++++-+        """
-----------++++-+        assert x.size(0) < self.max_len, (
-----------++++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-----------++++-+        # shape of x (len, B, dim)
-----------++++-+        x = x + self.pe[:x.size(0), :]
-----------++++-+        return self.dropout(x)
-----------+++ -
-----------+++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++ -train / dev split: 8055 200
-----------+++ diff --git a/recognition_model.py b/recognition_model.py
-----------+++-index 30c5ff2..2672d45 100644
-----------++++index 2672d45..d268f26 100644
-----------+++ --- a/recognition_model.py
-----------+++ +++ b/recognition_model.py
-----------+++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------+++- 
-----------+++-     #Define model and loss function
-----------+++-     n_chars = len(devset.text_transform.chars)
-----------+++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------+++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-----------+++-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------++++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------++++             X = combine_fixed_length(example['emg'], 200).to(device)
-----------++++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+++++            y = combine_fixed_length(example['text_int'], 200).to(device)
-----------+++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+++  
-----------+++-     if FLAGS.start_training_from is not None:
-----------++++             #Shifting target for input decoder and loss
-----------++++             tgt= y[:,:-1]
-----------+++ diff --git a/transformer.py b/transformer.py
-----------+++-index 51e1f2e..c125841 100644
-----------++++index c125841..73d805b 100644
-----------+++ --- a/transformer.py
-----------+++ +++ b/transformer.py
-----------+++-@@ -1,3 +1,4 @@
-----------+++-+import math
-----------+++- from typing import Optional
-----------+++- 
-----------+++- import torch
-----------+++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-----------+++-         Shape:
-----------+++-             see the docs in Transformer class.
-----------+++-         """
-----------+++--        src2 = self.self_attn(src, src, src)
-----------+++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-----------+++-         src = src + self.dropout1(src2)
-----------+++-         src = self.norm1(src)
-----------+++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------+++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-----------++++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
-----------+++          Shape:
-----------+++              see the docs in Transformer class.
-----------+++          """
-----------+++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------+++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------++++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------++++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------+++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------+++++        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+++          tgt = tgt + self.dropout1(tgt2)
-----------+++          tgt = self.norm1(tgt)
-----------+++  
-----------+++--        tgt2=self.multihead_attn(tgt, memory, memory)
-----------+++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-----------+++-         tgt = tgt + self.dropout1(tgt2)
-----------+++-         tgt = self.norm1(tgt)
-----------+++- 
-----------+++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-----------+++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------+++-     self.d_qkv = d_qkv
-----------+++- 
-----------+++--    #self.kdim = kdim if kdim is not None else embed_dim
-----------+++--    #self.vdim = vdim if vdim is not None else embed_dim
-----------+++--
-----------+++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-----------+++-     else:
-----------+++-         self.relative_positional = None
-----------+++- 
-----------+++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------+++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-----------+++-     """Runs the multi-head self-attention layer.
-----------+++- 
-----------+++-     Args:
-----------+++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-----------++++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
-----------+++      v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------+++      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+++  
-----------+++-+    if attn_mask is not None:
-----------+++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+++-+
-----------++++-    if attn_mask is not None:
-----------++++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+++++   # if attn_mask is not None:
-----------+++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++++ 
-----------+++      if self.relative_positional is not None:
-----------+++          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------+++-         l,b,h,d = q_pos.size()
-----------+++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-----------+++-             x = x.transpose(0, 1)
-----------+++-             x = x.contiguous().view(bsz_heads, length+1, length)
-----------+++-             return x[:, 1:, :]
-----------+++-+        
-----------+++-+
-----------+++-+########
-----------+++-+# Taken from:
-----------+++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-----------+++-+# or also here:
-----------+++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-----------+++-+class PositionalEncoding(nn.Module):
-----------+++-+
-----------+++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-----------+++-+        super(PositionalEncoding, self).__init__()
-----------+++-+        self.dropout = nn.Dropout(p=dropout)
-----------+++-+        self.max_len = max_len
-----------+++-+
-----------+++-+        pe = torch.zeros(max_len, d_model)
-----------+++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-----------+++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-----------+++-+                             * (-math.log(10000.0) / d_model))
-----------+++-+        pe[:, 0::2] = torch.sin(position * div_term)
-----------+++-+        pe[:, 1::2] = torch.cos(position * div_term)
-----------+++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-----------+++-+        self.register_buffer('pe', pe)  # Will not be trained.
-----------+++-+
-----------+++-+    def forward(self, x):
-----------+++-+        """Inputs of forward function
-----------+++-+        Args:
-----------+++-+            x: the sequence fed to the positional encoder model (required).
-----------+++-+        Shape:
-----------+++-+            x: [sequence length, batch size, embed dim]
-----------+++-+            output: [sequence length, batch size, embed dim]
-----------+++-+        """
-----------+++-+        assert x.size(0) < self.max_len, (
-----------+++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-----------+++-+        # shape of x (len, B, dim)
-----------+++-+        x = x + self.pe[:x.size(0), :]
-----------+++-+        return self.dropout(x)
-----------+++ 
-----------+++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+++ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------++ diff --git a/output/log.txt b/output/log.txt
-----------++-index ae42364..8563980 100644
-----------+++index 1d2cd8e..342fccd 100644
-----------++ --- a/output/log.txt
-----------++ +++ b/output/log.txt
-----------++-@@ -1,3 +1,2 @@
-----------++-+57f8139449dd9286c2203ec2eca118a550638a7c
-----------+++@@ -1,13 +1,2 @@
-----------+++-57f8139449dd9286c2203ec2eca118a550638a7c
-----------++++be71135adc89793578f304adb405cea80a5b2b9a
-----------++  
-----------+++-diff --git a/output/log.txt b/output/log.txt
-----------+++-index ae42364..8563980 100644
-----------+++---- a/output/log.txt
-----------+++-+++ b/output/log.txt
-----------+++-@@ -1,3 +1,2 @@
-----------+++-+57f8139449dd9286c2203ec2eca118a550638a7c
-----------+++- 
-----------+++--
-----------+++--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------++ -
-----------++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+++diff --git a/recognition_model.py b/recognition_model.py
-----------+++index 2672d45..d268f26 100644
-----------+++--- a/recognition_model.py
-----------++++++ b/recognition_model.py
-----------+++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------+++             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------++++            y = combine_fixed_length(example['text_int'], 200).to(device)
-----------++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+++ 
-----------+++             #Shifting target for input decoder and loss
-----------+++             tgt= y[:,:-1]
-----------+++diff --git a/transformer.py b/transformer.py
-----------+++index c125841..73d805b 100644
-----------+++--- a/transformer.py
-----------++++++ b/transformer.py
-----------+++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
-----------+++         Shape:
-----------+++             see the docs in Transformer class.
-----------+++         """
-----------+++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------+++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------++++        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+++         tgt = tgt + self.dropout1(tgt2)
-----------+++         tgt = self.norm1(tgt)
-----------+++ 
-----------+++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
-----------+++     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------+++     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+++ 
-----------+++-    if attn_mask is not None:
-----------+++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++++   # if attn_mask is not None:
-----------++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+++ 
-----------+++     if self.relative_positional is not None:
-----------+++         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------++ 
-----------++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+++output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+++train / dev split: 8055 200
-----------+ diff --git a/recognition_model.py b/recognition_model.py
-----------+-index 30c5ff2..2672d45 100644
-----------++index 2672d45..517c9a3 100644
-----------+ --- a/recognition_model.py
-----------+ +++ b/recognition_model.py
-----------+-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------++@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------++             X = combine_fixed_length(example['emg'], 200).to(device)
-----------++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------+++            y = combine_fixed_length(example['text_int'], 200).to(device)
-----------++             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+  
-----------+-     #Define model and loss function
-----------+-     n_chars = len(devset.text_transform.chars)
-----------+--    model = Model(devset.num_features, n_chars+1, True).to(device)
-----------+-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-----------+-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-----------+- 
-----------+-     if FLAGS.start_training_from is not None:
-----------++             #Shifting target for input decoder and loss
-----------+ diff --git a/transformer.py b/transformer.py
-----------+-index 51e1f2e..c125841 100644
-----------++index c125841..06e870b 100644
-----------+ --- a/transformer.py
-----------+ +++ b/transformer.py
-----------+-@@ -1,3 +1,4 @@
-----------+-+import math
-----------+- from typing import Optional
-----------+- 
-----------+- import torch
-----------+-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-----------+-         Shape:
-----------+-             see the docs in Transformer class.
-----------+-         """
-----------+--        src2 = self.self_attn(src, src, src)
-----------+-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-----------+-         src = src + self.dropout1(src2)
-----------+-         src = self.norm1(src)
-----------+-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------+-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-----------+-         Shape:
-----------+-             see the docs in Transformer class.
-----------+-         """
-----------+--        tgt2 = self.self_attn(tgt, tgt, tgt)
-----------+-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-----------+-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------+-         tgt = tgt + self.dropout1(tgt2)
-----------+-         tgt = self.norm1(tgt)
-----------+- 
-----------+--        tgt2=self.multihead_attn(tgt, memory, memory)
-----------+-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-----------+-         tgt = tgt + self.dropout1(tgt2)
-----------+-         tgt = self.norm1(tgt)
-----------+- 
-----------+-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-----------+-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-----------+-     self.d_qkv = d_qkv
-----------+- 
-----------+--    #self.kdim = kdim if kdim is not None else embed_dim
-----------+--    #self.vdim = vdim if vdim is not None else embed_dim
-----------+--
-----------+-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-----------+-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-----------+-     else:
-----------+-         self.relative_positional = None
-----------+- 
-----------+--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-----------+-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-----------+-     """Runs the multi-head self-attention layer.
-----------+- 
-----------+-     Args:
-----------+-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-----------+-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------++@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
-----------+      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+  
-----------+-+    if attn_mask is not None:
-----------+-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+-+
-----------++     if attn_mask is not None:
-----------++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++ 
-----------+      if self.relative_positional is not None:
-----------+          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------+-         l,b,h,d = q_pos.size()
-----------+-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-----------+-             x = x.transpose(0, 1)
-----------+-             x = x.contiguous().view(bsz_heads, length+1, length)
-----------+-             return x[:, 1:, :]
-----------+-+        
-----------+-+
-----------+-+########
-----------+-+# Taken from:
-----------+-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-----------+-+# or also here:
-----------+-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-----------+-+class PositionalEncoding(nn.Module):
-----------+-+
-----------+-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-----------+-+        super(PositionalEncoding, self).__init__()
-----------+-+        self.dropout = nn.Dropout(p=dropout)
-----------+-+        self.max_len = max_len
-----------+-+
-----------+-+        pe = torch.zeros(max_len, d_model)
-----------+-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-----------+-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-----------+-+                             * (-math.log(10000.0) / d_model))
-----------+-+        pe[:, 0::2] = torch.sin(position * div_term)
-----------+-+        pe[:, 1::2] = torch.cos(position * div_term)
-----------+-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-----------+-+        self.register_buffer('pe', pe)  # Will not be trained.
-----------+-+
-----------+-+    def forward(self, x):
-----------+-+        """Inputs of forward function
-----------+-+        Args:
-----------+-+            x: the sequence fed to the positional encoder model (required).
-----------+-+        Shape:
-----------+-+            x: [sequence length, batch size, embed dim]
-----------+-+            output: [sequence length, batch size, embed dim]
-----------+-+        """
-----------+-+        assert x.size(0) < self.max_len, (
-----------+-+            f"Too long sequence length: increase `max_len` of pos encoding")
-----------+-+        # shape of x (len, B, dim)
-----------+-+        x = x + self.pe[:x.size(0), :]
-----------+-+        return self.dropout(x)
-----------+ 
-----------+ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------+ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----------- diff --git a/output/log.txt b/output/log.txt
------------index ae42364..8563980 100644
-----------+index 1d2cd8e..342fccd 100644
----------- --- a/output/log.txt
----------- +++ b/output/log.txt
------------@@ -1,3 +1,2 @@
------------+57f8139449dd9286c2203ec2eca118a550638a7c
-----------+@@ -1,13 +1,2 @@
-----------+-57f8139449dd9286c2203ec2eca118a550638a7c
-----------++be71135adc89793578f304adb405cea80a5b2b9a
-----------  
-----------+-diff --git a/output/log.txt b/output/log.txt
-----------+-index ae42364..8563980 100644
-----------+---- a/output/log.txt
-----------+-+++ b/output/log.txt
-----------+-@@ -1,3 +1,2 @@
-----------+-+57f8139449dd9286c2203ec2eca118a550638a7c
-----------+- 
-----------+--
-----------+--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
----------- -
----------- -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+diff --git a/recognition_model.py b/recognition_model.py
-----------+index 2672d45..517c9a3 100644
-----------+--- a/recognition_model.py
-----------++++ b/recognition_model.py
-----------+@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------+             X = combine_fixed_length(example['emg'], 200).to(device)
-----------+             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------+             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------++            y = combine_fixed_length(example['text_int'], 200).to(device)
-----------+             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+ 
-----------+             #Shifting target for input decoder and loss
-----------+diff --git a/transformer.py b/transformer.py
-----------+index c125841..06e870b 100644
-----------+--- a/transformer.py
-----------++++ b/transformer.py
-----------+@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
-----------+     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-----------+ 
-----------+     if attn_mask is not None:
-----------+-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------+-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-----------++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+ 
-----------+     if self.relative_positional is not None:
-----------+         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
----------- 
----------- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-----------+output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------+train / dev split: 8055 200
-----------diff --git a/recognition_model.py b/recognition_model.py
-----------index 2672d45..506241a 100644
-------------- a/recognition_model.py
-----------+++ b/recognition_model.py
-----------@@ -39,23 +39,28 @@ def test(model, testset, device):
-----------     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
-----------     references = []
-----------     predictions = []
-----------+    batch_idx = 0
-----------     with torch.no_grad():
-----------         for example in dataloader:
------------            X = example['emg'].to(device)
-----------             X_raw = example['raw_emg'].to(device)
------------            sess = example['session_ids'].to(device)
-----------+            tgt = example['text_int'].to(device)
----------- 
------------            pred  = F.log_softmax(model(X, X_raw, sess), -1)
-----------+            #Prediction without the 197-th batch because of missing label
-----------+            if batch_idx != 181:
-----------+                out_enc, out_dec = model(X_raw, tgt)
-----------+                pred  = F.log_softmax(out_dec, -1)
----------- 
------------            beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
------------            pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
-----------+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
-----------+                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
----------- 
------------            pred_text = testset.text_transform.int_to_text(pred_int)
------------            target_text = testset.text_transform.clean_text(example['text'][0])
-----------+                pred_text = testset.text_transform.int_to_text(pred_int)
-----------+                target_text = testset.text_transform.clean_text(example['text'][0])
----------- 
------------            references.append(target_text)
------------            predictions.append(pred_text)
-----------+                references.append(target_text)
-----------+                predictions.append(pred_text)
----------- 
-----------+        batch_idx += 1
-----------+        
-----------     model.train()
-----------     #remove empty strings because I had an error in the calculation of WER function
-----------     predictions = [predictions[i] for i in range(len(predictions)) if len(references[i]) > 0]
-----------@@ -102,17 +107,17 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------             schedule_lr(batch_idx)
----------- 
-----------             #Preprosessing of the input and target for the model
------------            X = combine_fixed_length(example['emg'], 200).to(device)
-----------             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
------------            sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------+            #X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
-----------+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----------- 
-----------             #Shifting target for input decoder and loss
-----------             tgt= y[:,:-1]
-----------             target= y[:,1:]
----------- 
-----------             #Prediction
------------            out_enc, out_dec = model(X, X_raw, tgt, sess)
-----------+            out_enc, out_dec = model(X_raw, tgt)
----------- 
-----------             #Primary Loss
-----------             out_dec=out_dec.permute(0,2,1)
-----------@@ -168,6 +173,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-----------             #Increment counter        
-----------             batch_idx += 1
-----------             writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------+            val = test(model, devset, device)
----------- 
-----------         #Testing and change learning rate
-----------         val = test(model, devset, device)
-----------diff --git a/transformer.py b/transformer.py
-----------index c125841..0a305a4 100644
-------------- a/transformer.py
-----------+++ b/transformer.py
-----------@@ -52,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-----------         Shape:
-----------             see the docs in Transformer class.
-----------         """
------------        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-----------+        src2 = self.self_attn(src, src, src)
-----------         src = src + self.dropout1(src2)
-----------         src = self.norm1(src)
-----------         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-----------@@ -123,12 +123,11 @@ class TransformerDecoderLayer(nn.Module):
-----------         Shape:
-----------             see the docs in Transformer class.
-----------         """
------------        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
------------        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-----------+        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_key_padding_mask, tgt_mask)
-----------         tgt = tgt + self.dropout1(tgt2)
-----------         tgt = self.norm1(tgt)
----------- 
------------        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-----------+        tgt2=self.multihead_attn(tgt, memory, memory)
-----------         tgt = tgt + self.dropout1(tgt2)
-----------         tgt = self.norm1(tgt)
----------- 
-----------@@ -163,7 +162,7 @@ class MultiHeadAttention(nn.Module):
-----------     else:
-----------         self.relative_positional = None
----------- 
------------  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-----------+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None, attn_mask: Optional[torch.Tensor] = None):
-----------     """Runs the multi-head self-attention layer.
----------- 
-----------     Args:
-----------@@ -171,15 +170,39 @@ class MultiHeadAttention(nn.Module):
-----------     Returns:
-----------       A single tensor containing the output from this layer
-----------     """
-----------+    # Apply mask to the keys if provided
-----------+    if attn_mask is not None:
-----------+        attn_mask = attn_mask.unsqueeze(2)
-----------+        attn_mask = attn_mask.unsqueeze(3)
-----------+        key = key.masked_fill(attn_mask == float('-inf'), float('-inf'))[0,:,:,:]
-----------+    
-----------+    # Apply mask to the query if provided
-----------+    if key_padding_mask is not None:
-----------+        query = query.float().masked_fill(
-----------+            key_padding_mask, float('-inf')).type_as(query)  
-----------+    
-----------+    # Apply mask to the values if provided
-----------+    if key_padding_mask is not None:
-----------+        value = value.float().masked_fill(
-----------+            key_padding_mask, float(0)).type_as(value) 
----------- 
-----------+    #Computes projections
-----------     q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-----------     k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-----------     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-----------+     
-----------+    # Compute scaled dot-product attention
-----------     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
----------- 
-----------+    
-----------+    # Apply mask to the attention weights if provided
-----------     if attn_mask is not None:
------------        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
------------        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-----------+        attn_mask = attn_mask.squeeze(2)       
-----------+        attn_mask = attn_mask.squeeze(2)
-----------+        attn_mask = attn_mask.unsqueeze(0)
-----------+        attn_mask = attn_mask.unsqueeze(1)
-----------+        logits = logits.masked_fill(attn_mask == float('-inf'), float('-inf'))
-----------+    
----------- 
-----------     if self.relative_positional is not None:
-----------         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-----------
-----------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----------train / dev split: 8055 200
----------diff --git a/read_emg.py b/read_emg.py
----------index 664aa94..27787d5 100644
------------- a/read_emg.py
----------+++ b/read_emg.py
----------@@ -72,8 +72,8 @@ def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_
----------     x = apply_to_all(notch_harmonics, x, 60, 1000)
----------     x = apply_to_all(remove_drift, x, 1000)
----------     x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]
-----------    emg_orig = apply_to_all(subsample, x, 689.06, 1000)
-----------    x = apply_to_all(subsample, x, 516.79, 1000)
----------+    emg_orig = apply_to_all(subsample, x, 400.00, 1000)
----------+    x = apply_to_all(subsample, x, 300, 1000)
----------     emg = x
---------- 
----------     for c in FLAGS.remove_channels:
----------@@ -240,6 +240,7 @@ class EMGDataset(torch.utils.data.Dataset):
----------         session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)
----------         audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'
---------- 
----------+        #self.text_transform.add_new_words(text)
----------         text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)
---------- 
----------         result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}
----------diff --git a/recognition_model.py b/recognition_model.py
----------index 506241a..547fde4 100644
------------- a/recognition_model.py
----------+++ b/recognition_model.py
----------@@ -3,7 +3,7 @@ import sys
---------- import numpy as np
---------- import logging
---------- import subprocess
-----------from ctcdecode import CTCBeamDecoder
----------+from ctcdecode import OnlineCTCBeamDecoder, DecoderState
---------- import jiwer
---------- import random
---------- from torch.utils.tensorboard import SummaryWriter
----------@@ -14,8 +14,7 @@ import torch.nn.functional as F
---------- 
---------- from read_emg import EMGDataset, SizeAwareSampler
---------- from architecture import Model
-----------from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-----------from transformer import TransformerEncoderLayer
----------+from data_utils import combine_fixed_length, decollate_tensor
---------- 
---------- from absl import flags
---------- FLAGS = flags.FLAGS
----------@@ -32,32 +31,32 @@ flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file'
---------- def test(model, testset, device):
----------     model.eval()
---------- 
-----------    blank_id = len(testset.text_transform.chars)
-----------    decoder = CTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,
----------+    blank_id = 1
----------+    decoder = OnlineCTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
----------             model_path='lm.binary', alpha=1.5, beta=1.85)
-----------
----------+    state = DecoderState(decoder)
----------     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
----------     references = []
----------     predictions = []
----------     batch_idx = 0
----------     with torch.no_grad():
----------         for example in dataloader:
-----------            X_raw = example['raw_emg'].to(device)
-----------            tgt = example['text_int'].to(device)
----------+            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----------+            tgt = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---------- 
----------             #Prediction without the 197-th batch because of missing label
----------             if batch_idx != 181:
----------                 out_enc, out_dec = model(X_raw, tgt)
----------                 pred  = F.log_softmax(out_dec, -1)
---------- 
-----------                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
-----------                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
----------+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred, [state], [False])
----------+                #pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
---------- 
-----------                pred_text = testset.text_transform.int_to_text(pred_int)
-----------                target_text = testset.text_transform.clean_text(example['text'][0])
----------+                #pred_text = testset.text_transform.int_to_text(pred_int)
----------+                #target_text = testset.text_transform.clean_text(example['text'][0])
---------- 
-----------                references.append(target_text)
-----------                predictions.append(pred_text)
----------+                #references.append(target_text)
----------+                #predictions.append(pred_text)
---------- 
----------         batch_idx += 1
----------         
----------@@ -70,12 +69,12 @@ def test(model, testset, device):
---------- 
---------- def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
----------     #Define Dataloader
-----------    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
----------+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_size=2)
----------     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
---------- 
----------     #Define model and loss function
----------     n_chars = len(devset.text_transform.chars)
-----------    model = Model(devset.num_features, n_chars+1, device, True).to(device)
----------+    model = Model(devset.num_features, n_chars, device, True).to(device)
----------     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
---------- 
----------     if FLAGS.start_training_from is not None:
----------@@ -107,10 +106,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
----------             schedule_lr(batch_idx)
---------- 
----------             #Preprosessing of the input and target for the model
-----------            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------            #X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
-----------            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----------+            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----------+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---------- 
----------             #Shifting target for input decoder and loss
----------             tgt= y[:,:-1]
----------@@ -119,15 +116,14 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
----------             #Prediction
----------             out_enc, out_dec = model(X_raw, tgt)
---------- 
-----------            #Primary Loss
----------+            #Decoder Loss
----------             out_dec=out_dec.permute(0,2,1)
----------             loss_dec = loss_fn(out_dec, target)
---------- 
-----------            #Auxiliary Loss
----------+            #Encoder Loss
----------             out_enc = F.log_softmax(out_enc, 2)
-----------            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-----------            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----------            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
----------+            out_enc = out_enc.transpose(1,0)
----------+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = 1) # 1 is the blank token according to TextTransform
---------- 
----------             #Combination the two losses
----------             loss = (1 - alpha) * loss_dec + alpha * loss_enc
----------@@ -140,41 +136,43 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
----------                 optim.step()
----------                 optim.zero_grad()
----------             
-----------            if False:
-----------            #if batch_idx % report_every == report_every - 2:     
----------+            
----------+            #Increment counter and print the loss training       
----------+            batch_idx += 1
----------+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
----------+            train_loss= 0
----------+
----------+
----------+            #Debug
----------+            val = test(model, devset, device)
----------+
----------+            if batch_idx % report_every == 0:     
----------                 #Evaluation
----------                 model.eval()
----------                 with torch.no_grad():
-----------                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-----------                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----------                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-----------                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-----------
----------+                    for idx, example in enumerate(dataloader_evaluation):
----------+                        X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----------+                        y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----------+                    
----------                         #Shifting target for input decoder and loss
----------                         tgt= y[:,:-1]
----------                         target= y[:,1:]
---------- 
-----------                        print(idx)
-----------
----------                         #Prediction without the 197-th batch because of missing label
-----------                        if idx != 181:
-----------                            pred = model(X, X_raw, tgt, sess)
-----------                            #Primary Loss
-----------                            pred=pred.permute(0,2,1)
-----------                            loss = loss_fn(pred, target)
-----------                            eval_loss += loss.item()
----------+                        out_enc, out_dec = model(X_raw, tgt)
----------+                        #Decoder Loss
----------+                        out_dec=out_dec.permute(0,2,1)
----------+                        loss = loss_fn(out_dec, target)
----------+                        eval_loss += loss.item()
----------+                        
----------+                        #just for now
----------+                        if idx == 10:
----------+                            break
---------- 
----------                 #Writing on tensorboard
----------                 writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-----------                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-----------                train_loss= 0
----------                 eval_loss= 0
---------- 
-----------            #Increment counter        
-----------            batch_idx += 1
-----------            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-----------            val = test(model, devset, device)
-----------
----------         #Testing and change learning rate
----------         val = test(model, devset, device)
----------         writer.add_scalar('WER/Evaluation',val, batch_idx)
----------diff --git a/transformer.py b/transformer.py
----------index 0a305a4..47a7ec7 100644
------------- a/transformer.py
----------+++ b/transformer.py
----------@@ -170,22 +170,6 @@ class MultiHeadAttention(nn.Module):
----------     Returns:
----------       A single tensor containing the output from this layer
----------     """
-----------    # Apply mask to the keys if provided
-----------    if attn_mask is not None:
-----------        attn_mask = attn_mask.unsqueeze(2)
-----------        attn_mask = attn_mask.unsqueeze(3)
-----------        key = key.masked_fill(attn_mask == float('-inf'), float('-inf'))[0,:,:,:]
-----------    
-----------    # Apply mask to the query if provided
-----------    if key_padding_mask is not None:
-----------        query = query.float().masked_fill(
-----------            key_padding_mask, float('-inf')).type_as(query)  
-----------    
-----------    # Apply mask to the values if provided
-----------    if key_padding_mask is not None:
-----------        value = value.float().masked_fill(
-----------            key_padding_mask, float(0)).type_as(value) 
-----------
----------     #Computes projections
----------     q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
----------     k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
----------@@ -194,16 +178,18 @@ class MultiHeadAttention(nn.Module):
----------     # Compute scaled dot-product attention
----------     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
---------- 
-----------    
-----------    # Apply mask to the attention weights if provided
----------+    # Apply att_mask to the attention weights if provided
----------     if attn_mask is not None:
-----------        attn_mask = attn_mask.squeeze(2)       
-----------        attn_mask = attn_mask.squeeze(2)
-----------        attn_mask = attn_mask.unsqueeze(0)
-----------        attn_mask = attn_mask.unsqueeze(1)
----------+        attn_mask=attn_mask.unsqueeze(0)
----------+        attn_mask=attn_mask.unsqueeze(1)
----------         logits = logits.masked_fill(attn_mask == float('-inf'), float('-inf'))
----------     
-----------
----------+    
----------+    # Apply padding_mask to the attention weights if provided
----------+    if key_padding_mask is not None:
----------+        logits = logits.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
----------+    
----------+    
----------     if self.relative_positional is not None:
----------         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
----------         l,b,h,d = q_pos.size()
----------
----------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
----------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----------train / dev split: 8055 200
---------diff --git a/recognition_model.py b/recognition_model.py
---------index 547fde4..62184a7 100644
------------ a/recognition_model.py
---------+++ b/recognition_model.py
---------@@ -3,7 +3,7 @@ import sys
--------- import numpy as np
--------- import logging
--------- import subprocess
----------from ctcdecode import OnlineCTCBeamDecoder, DecoderState
---------+from ctcdecode import CTCBeamDecoder, DecoderState
--------- import jiwer
--------- import random
--------- from torch.utils.tensorboard import SummaryWriter
---------@@ -32,7 +32,7 @@ def test(model, testset, device):
---------     model.eval()
--------- 
---------     blank_id = 1
----------    decoder = OnlineCTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
---------+    decoder = CTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
---------             model_path='lm.binary', alpha=1.5, beta=1.85)
---------     state = DecoderState(decoder)
---------     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
---------@@ -49,14 +49,14 @@ def test(model, testset, device):
---------                 out_enc, out_dec = model(X_raw, tgt)
---------                 pred  = F.log_softmax(out_dec, -1)
--------- 
----------                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred, [state], [False])
----------                #pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
---------+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
---------+                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
--------- 
----------                #pred_text = testset.text_transform.int_to_text(pred_int)
----------                #target_text = testset.text_transform.clean_text(example['text'][0])
---------+                pred_text = testset.text_transform.int_to_text(pred_int)
---------+                target_text = testset.text_transform.clean_text(example['text'][0])
--------- 
----------                #references.append(target_text)
----------                #predictions.append(pred_text)
---------+                references.append(target_text)
---------+                predictions.append(pred_text)
--------- 
---------         batch_idx += 1
---------         
---------@@ -74,7 +74,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------- 
---------     #Define model and loss function
---------     n_chars = len(devset.text_transform.chars)
----------    model = Model(devset.num_features, n_chars, device, True).to(device)
---------+    model = Model(devset.num_features, n_chars + 1, device, True).to(device)
---------     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------- 
---------     if FLAGS.start_training_from is not None:
---------@@ -123,7 +123,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
---------             #Encoder Loss
---------             out_enc = F.log_softmax(out_enc, 2)
---------             out_enc = out_enc.transpose(1,0)
----------            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = 1) # 1 is the blank token according to TextTransform
---------+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)+1) 
--------- 
---------             #Combination the two losses
---------             loss = (1 - alpha) * loss_dec + alpha * loss_enc
---------@@ -143,9 +143,6 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
---------             train_loss= 0
--------- 
--------- 
----------            #Debug
----------            val = test(model, devset, device)
----------
---------             if batch_idx % report_every == 0:     
---------                 #Evaluation
---------                 model.eval()
---------
---------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
---------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
---------train / dev split: 8055 200
--------diff --git a/output/log.txt b/output/log.txt
--------index 1d2cd8e..42b3343 100644
----------- a/output/log.txt
--------+++ b/output/log.txt
--------@@ -1,13 +1,3339 @@
---------57f8139449dd9286c2203ec2eca118a550638a7c
--------+be71135adc89793578f304adb405cea80a5b2b9a
-------- 
--------+diff --git a/architecture.py b/architecture.py
--------+index 94d0de0..222c88e 100644
--------+--- a/architecture.py
--------++++ b/architecture.py
--------+@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------+         return F.relu(x + res)
--------+ 
--------+ class Model(nn.Module):
--------+-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
--------+         super().__init__()
--------+ 
--------+         self.conv_blocks = nn.Sequential(
--------+@@ -61,8 +61,11 @@ class Model(nn.Module):
--------+         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+ 
--------+         self.has_aux_loss = has_aux_loss
--------++        if self.has_aux_loss:
--------++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
--------+         self.device=device
--------+ 
--------++
--------+     def create_src_padding_mask(self, src):
--------+         # input src of shape ()
--------+         src_padding_mask = src.transpose(1, 0) == 0
--------+@@ -106,7 +109,7 @@ class Model(nn.Module):
--------+         x_decoder = x_decoder.transpose(0,1)
--------+ 
--------+         if self.has_aux_loss:
--------+-            return self.w_out(x_encoder), self.w_out(x_decoder)
--------++            return self.w_aux(x_encoder), self.w_out(x_decoder)
--------+         else:
--------+             return self.w_out(x)
--------+ 
--------+diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+index 2243f2d..a8cb5ee 100644
--------+--- a/models/recognition_model/log.txt
--------++++ b/models/recognition_model/log.txt
--------+@@ -1,844 +1,2577 @@
--------+-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------++be71135adc89793578f304adb405cea80a5b2b9a
--------+ 
--------+ diff --git a/architecture.py b/architecture.py
--------+-index 2413a8a..94d0de0 100644
--------++index 94d0de0..7170c48 100644
--------+ --- a/architecture.py
--------+ +++ b/architecture.py
--------+-@@ -4,7 +4,7 @@ import torch
--------+- from torch import nn
--------+- import torch.nn.functional as F
--------+- 
--------+--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------+-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
--------+- 
--------+- from absl import flags
--------+- FLAGS = flags.FLAGS
--------+ @@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------+          return F.relu(x + res)
--------+  
--------+  class Model(nn.Module):
--------+--    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------+-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------++-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------+++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
--------+          super().__init__()
--------+  
--------+          self.conv_blocks = nn.Sequential(
--------+-@@ -52,6 +52,7 @@ class Model(nn.Module):
--------+-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+- 
--------+-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
--------+- 
--------+-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+-@@ -60,9 +61,25 @@ class Model(nn.Module):
--------++@@ -61,8 +61,11 @@ class Model(nn.Module):
--------+          self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+  
--------+          self.has_aux_loss = has_aux_loss
--------+--
--------+--    def forward(self, x_feat, x_raw, y,session_ids):
--------+-+        self.device=device
--------+-+
--------+-+    def create_src_padding_mask(self, src):
--------+-+        # input src of shape ()
--------+-+        src_padding_mask = src.transpose(1, 0) == 0
--------+-+        return src_padding_mask
--------+-+
--------+-+    def create_tgt_padding_mask(self, tgt):
--------+-+        # input tgt of shape ()
--------+-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
--------+-+        return tgt_padding_mask
--------+-+    
--------+-+    def forward(self, x_feat, x_raw, y, session_ids):
--------+-         # x shape is (batch, time, electrode)
--------+-+        # y shape is (batch, sequence_length)
--------+-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--------+-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--------+-+        memory_key_padding_mask = src_key_padding_mask
--------+-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
--------+++        if self.has_aux_out:
--------+++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
--------++         self.device=device
--------+  
--------+-         if self.training:
--------+-             r = random.randrange(8)
--------+-@@ -74,14 +91,16 @@ class Model(nn.Module):
--------+-         x_raw = self.conv_blocks(x_raw)
--------+-         x_raw = x_raw.transpose(1,2)
--------+-         x_raw = self.w_raw_in(x_raw)
--------+--
--------+-         x = x_raw
--------+ +
--------+-+        #Embedding and positional encoding of tgt
--------+-         tgt=self.embedding_tgt(y)
--------+-+        tgt=self.pos_encoder(tgt)
--------++     def create_src_padding_mask(self, src):
--------++         # input src of shape ()
--------++         src_padding_mask = src.transpose(1, 0) == 0
--------++@@ -106,7 +109,7 @@ class Model(nn.Module):
--------++         x_decoder = x_decoder.transpose(0,1)
--------+  
--------+-         x = x.transpose(0,1) # put time first
--------+-         tgt = tgt.transpose(0,1) # put channel after
--------+--        x_encoder = self.transformerEncoder(x)
--------+--        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------+-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--------+-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
--------++         if self.has_aux_loss:
--------++-            return self.w_out(x_encoder), self.w_out(x_decoder)
--------+++            return self.w_aux(x_encoder), self.w_out(x_decoder)
--------++         else:
--------++             return self.w_out(x)
--------+  
--------+-         x_encoder = x_encoder.transpose(0,1)
--------+-         x_decoder = x_decoder.transpose(0,1)
--------+ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+-index 53839a0..1343d7d 100644
--------++index 2243f2d..342fccd 100644
--------+ --- a/models/recognition_model/log.txt
--------+ +++ b/models/recognition_model/log.txt
--------+-@@ -1,639 +1,2 @@
--------+--2fa943cd85263a152b6be80d502eda27932ebb27
--------+-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------++@@ -1,845 +1,2 @@
--------++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------+++be71135adc89793578f304adb405cea80a5b2b9a
--------+  
--------+ -diff --git a/architecture.py b/architecture.py
--------+--index a8c70f3..2413a8a 100644
--------++-index 2413a8a..94d0de0 100644
--------+ ---- a/architecture.py
--------+ -+++ b/architecture.py
--------++-@@ -4,7 +4,7 @@ import torch
--------++- from torch import nn
--------++- import torch.nn.functional as F
--------++- 
--------++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
--------++- 
--------++- from absl import flags
--------++- FLAGS = flags.FLAGS
--------+ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------+ -         return F.relu(x + res)
--------+ - 
--------+ - class Model(nn.Module):
--------+---    def __init__(self, num_features, num_outs, num_aux_outs=None):
--------+--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------+ -         super().__init__()
--------+ - 
--------+ -         self.conv_blocks = nn.Sequential(
--------+--@@ -59,9 +59,7 @@ class Model(nn.Module):
--------+--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++-@@ -52,6 +52,7 @@ class Model(nn.Module):
--------++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+ - 
--------+---        self.has_aux_out = num_aux_outs is not None
--------+---        if self.has_aux_out:
--------+---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------+--+        self.has_aux_loss = has_aux_loss
--------++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
--------+ - 
--------+--     def forward(self, x_feat, x_raw, y,session_ids):
--------+--         # x shape is (batch, time, electrode)
--------+--@@ -82,12 +80,14 @@ class Model(nn.Module):
--------++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++-@@ -60,9 +61,25 @@ class Model(nn.Module):
--------++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+ - 
--------+--         x = x.transpose(0,1) # put time first
--------+--         tgt = tgt.transpose(0,1) # put channel after
--------+---        x = self.transformerEncoder(x)
--------+---        x = self.transformerDecoder(tgt, x)
--------+---        x = x.transpose(0,1)
--------+--+        x_encoder = self.transformerEncoder(x)
--------+--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++-         self.has_aux_loss = has_aux_loss
--------++--
--------++--    def forward(self, x_feat, x_raw, y,session_ids):
--------++-+        self.device=device
--------++-+
--------++-+    def create_src_padding_mask(self, src):
--------++-+        # input src of shape ()
--------++-+        src_padding_mask = src.transpose(1, 0) == 0
--------++-+        return src_padding_mask
--------++-+
--------++-+    def create_tgt_padding_mask(self, tgt):
--------++-+        # input tgt of shape ()
--------++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
--------++-+        return tgt_padding_mask
--------++-+    
--------++-+    def forward(self, x_feat, x_raw, y, session_ids):
--------++-         # x shape is (batch, time, electrode)
--------++-+        # y shape is (batch, sequence_length)
--------++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--------++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--------++-+        memory_key_padding_mask = src_key_padding_mask
--------++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
--------+ - 
--------+---        if self.has_aux_out:
--------+---            return self.w_out(x), self.w_aux(x)
--------+--+        x_encoder = x_encoder.transpose(0,1)
--------+--+        x_decoder = x_decoder.transpose(0,1)
--------++-         if self.training:
--------++-             r = random.randrange(8)
--------++-@@ -74,14 +91,16 @@ class Model(nn.Module):
--------++-         x_raw = self.conv_blocks(x_raw)
--------++-         x_raw = x_raw.transpose(1,2)
--------++-         x_raw = self.w_raw_in(x_raw)
--------++--
--------++-         x = x_raw
--------+ -+
--------+--+        if self.has_aux_loss:
--------+--+            return self.w_out(x_encoder), self.w_out(x_decoder)
--------+--         else:
--------+--             return self.w_out(x)
--------++-+        #Embedding and positional encoding of tgt
--------++-         tgt=self.embedding_tgt(y)
--------++-+        tgt=self.pos_encoder(tgt)
--------+ - 
--------+--diff --git a/data_utils.py b/data_utils.py
--------+--index e2632e8..8b05213 100644
--------+----- a/data_utils.py
--------+--+++ b/data_utils.py
--------+--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
--------++-         x = x.transpose(0,1) # put time first
--------++-         tgt = tgt.transpose(0,1) # put channel after
--------++--        x_encoder = self.transformerEncoder(x)
--------++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--------++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
--------+ - 
--------+-- def combine_fixed_length_tgt(tensor_list, n_batch):
--------+--     total_length = sum(t.size(0) for t in tensor_list)
--------+--+    tensor_list = list(tensor_list) # copy
--------+--     if total_length % n_batch != 0:
--------+--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------+---        tensor_list = list(tensor_list) # copy
--------+--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------+--         total_length += pad_length
--------+--     tensor = torch.cat(tensor_list, 0)
--------++-         x_encoder = x_encoder.transpose(0,1)
--------++-         x_decoder = x_decoder.transpose(0,1)
--------+ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+--index b8f7791..617dd85 100644
--------++-index 53839a0..1343d7d 100644
--------+ ---- a/models/recognition_model/log.txt
--------+ -+++ b/models/recognition_model/log.txt
--------+--@@ -1,480 +1,2 @@
--------+---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------+--+2fa943cd85263a152b6be80d502eda27932ebb27
--------++-@@ -1,639 +1,2 @@
--------++--2fa943cd85263a152b6be80d502eda27932ebb27
--------++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------+ - 
--------+ --diff --git a/architecture.py b/architecture.py
--------+---index d6e99b4..a8c70f3 100644
--------++--index a8c70f3..2413a8a 100644
--------+ ----- a/architecture.py
--------+ --+++ b/architecture.py
--------+---@@ -54,7 +54,7 @@ class Model(nn.Module):
--------+---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------++--         return F.relu(x + res)
--------+ -- 
--------+---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++-- class Model(nn.Module):
--------++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
--------++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++--         super().__init__()
--------++-- 
--------++--         self.conv_blocks = nn.Sequential(
--------++--@@ -59,9 +59,7 @@ class Model(nn.Module):
--------+ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++-- 
--------++---        self.has_aux_out = num_aux_outs is not None
--------++---        if self.has_aux_out:
--------++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------++--+        self.has_aux_loss = has_aux_loss
--------++-- 
--------++--     def forward(self, x_feat, x_raw, y,session_ids):
--------++--         # x shape is (batch, time, electrode)
--------++--@@ -82,12 +80,14 @@ class Model(nn.Module):
--------++-- 
--------++--         x = x.transpose(0,1) # put time first
--------++--         tgt = tgt.transpose(0,1) # put channel after
--------++---        x = self.transformerEncoder(x)
--------++---        x = self.transformerDecoder(tgt, x)
--------++---        x = x.transpose(0,1)
--------++--+        x_encoder = self.transformerEncoder(x)
--------++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++-- 
--------++---        if self.has_aux_out:
--------++---            return self.w_out(x), self.w_aux(x)
--------++--+        x_encoder = x_encoder.transpose(0,1)
--------++--+        x_decoder = x_decoder.transpose(0,1)
--------++--+
--------++--+        if self.has_aux_loss:
--------++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
--------++--         else:
--------++--             return self.w_out(x)
--------++-- 
--------+ --diff --git a/data_utils.py b/data_utils.py
--------+---index e4ac852..e2632e8 100644
--------++--index e2632e8..8b05213 100644
--------+ ----- a/data_utils.py
--------+ --+++ b/data_utils.py
--------+---@@ -1,3 +1,4 @@
--------+---+import math
--------+--- import string
--------+--- 
--------+--- import numpy as np
--------+---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
--------+---     n = total_length // length
--------+---     return tensor.view(n, length, *tensor.size()[1:])
--------++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
--------+ -- 
--------+---+def combine_fixed_length_tgt(tensor_list, n_batch):
--------+---+    total_length = sum(t.size(0) for t in tensor_list)
--------+---+    if total_length % n_batch != 0:
--------+---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------+---+        tensor_list = list(tensor_list) # copy
--------+---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------+---+        total_length += pad_length
--------+---+    tensor = torch.cat(tensor_list, 0)
--------+---+    length = total_length // n_batch
--------+---+    return tensor.view(n_batch, length, *tensor.size()[1:])
--------+---+
--------+--- def decollate_tensor(tensor, lengths):
--------+---     b, s, d = tensor.size()
--------+---     tensor = tensor.view(b*s, d)
--------++-- def combine_fixed_length_tgt(tensor_list, n_batch):
--------++--     total_length = sum(t.size(0) for t in tensor_list)
--------++--+    tensor_list = list(tensor_list) # copy
--------++--     if total_length % n_batch != 0:
--------++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------++---        tensor_list = list(tensor_list) # copy
--------++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------++--         total_length += pad_length
--------++--     tensor = torch.cat(tensor_list, 0)
--------+ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+---index e890f0f..1ee3421 100644
--------++--index b8f7791..617dd85 100644
--------+ ----- a/models/recognition_model/log.txt
--------+ --+++ b/models/recognition_model/log.txt
--------+---@@ -1,265 +1,2 @@
--------+----031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------+---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------++--@@ -1,480 +1,2 @@
--------++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------++--+2fa943cd85263a152b6be80d502eda27932ebb27
--------+ -- 
--------+ ---diff --git a/architecture.py b/architecture.py
--------+----index b22af61..d6e99b4 100644
--------++---index d6e99b4..a8c70f3 100644
--------+ ------ a/architecture.py
--------+ ---+++ b/architecture.py
--------+----@@ -51,6 +51,8 @@ class Model(nn.Module):
--------+----         )
--------+----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------++---@@ -54,7 +54,7 @@ class Model(nn.Module):
--------++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+ --- 
--------+----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+----+
--------+ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+----@@ -61,7 +63,7 @@ class Model(nn.Module):
--------+----         if self.has_aux_out:
--------+----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------+---- 
--------+-----    def forward(self, x_feat, x_raw, session_ids):
--------+----+    def forward(self, x_feat, x_raw, y,session_ids):
--------+----         # x shape is (batch, time, electrode)
--------+---- 
--------+----         if self.training:
--------+----@@ -76,10 +78,12 @@ class Model(nn.Module):
--------+----         x_raw = self.w_raw_in(x_raw)
--------+---- 
--------+----         x = x_raw
--------+----+        tgt=self.embedding_tgt(y)
--------+---- 
--------+----         x = x.transpose(0,1) # put time first
--------+----+        tgt = tgt.transpose(0,1) # put channel after
--------+----         x = self.transformerEncoder(x)
--------+-----        x = self.transformerDecoder(x) #TODO I need the target EMG
--------+----+        x = self.transformerDecoder(tgt, x)
--------+----         x = x.transpose(0,1)
--------+---- 
--------+----         if self.has_aux_out:
--------++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+ ---diff --git a/data_utils.py b/data_utils.py
--------+----index 11d4805..e4ac852 100644
--------++---index e4ac852..e2632e8 100644
--------+ ------ a/data_utils.py
--------+ ---+++ b/data_utils.py
--------+----@@ -244,6 +244,7 @@ class TextTransform(object):
--------+----     def __init__(self):
--------+----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--------+----         self.chars = string.ascii_lowercase+string.digits+' '
--------+----+        self.vocabulary_size=len(self.chars)
--------++---@@ -1,3 +1,4 @@
--------++---+import math
--------++--- import string
--------+ --- 
--------+----     def clean_text(self, text):
--------+----         text = unidecode(text)
--------++--- import numpy as np
--------++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
--------++---     n = total_length // length
--------++---     return tensor.view(n, length, *tensor.size()[1:])
--------++--- 
--------++---+def combine_fixed_length_tgt(tensor_list, n_batch):
--------++---+    total_length = sum(t.size(0) for t in tensor_list)
--------++---+    if total_length % n_batch != 0:
--------++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------++---+        tensor_list = list(tensor_list) # copy
--------++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------++---+        total_length += pad_length
--------++---+    tensor = torch.cat(tensor_list, 0)
--------++---+    length = total_length // n_batch
--------++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
--------++---+
--------++--- def decollate_tensor(tensor, lengths):
--------++---     b, s, d = tensor.size()
--------++---     tensor = tensor.view(b*s, d)
--------+ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+----index fbc0abb..400061a 100644
--------++---index e890f0f..1ee3421 100644
--------+ ------ a/models/recognition_model/log.txt
--------+ ---+++ b/models/recognition_model/log.txt
--------+----@@ -1,188 +1,2 @@
--------+-----57f8139449dd9286c2203ec2eca118a550638a7c
--------+----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------++---@@ -1,265 +1,2 @@
--------++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------+ --- 
--------+ ----diff --git a/architecture.py b/architecture.py
--------+-----index 4fc3793..b22af61 100644
--------++----index b22af61..d6e99b4 100644
--------+ ------- a/architecture.py
--------+ ----+++ b/architecture.py
--------+-----@@ -4,7 +4,7 @@ import torch
--------+----- from torch import nn
--------+----- import torch.nn.functional as F
--------+----- 
--------+------from transformer import TransformerEncoderLayer
--------+-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------+----- 
--------+----- from absl import flags
--------+----- FLAGS = flags.FLAGS
--------+-----@@ -52,7 +52,9 @@ class Model(nn.Module):
--------++----@@ -51,6 +51,8 @@ class Model(nn.Module):
--------++----         )
--------+ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+ ---- 
--------++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++----+
--------+ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++----@@ -61,7 +63,7 @@ class Model(nn.Module):
--------++----         if self.has_aux_out:
--------++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------++---- 
--------++-----    def forward(self, x_feat, x_raw, session_ids):
--------++----+    def forward(self, x_feat, x_raw, y,session_ids):
--------++----         # x shape is (batch, time, electrode)
--------++---- 
--------++----         if self.training:
--------++----@@ -76,10 +78,12 @@ class Model(nn.Module):
--------++----         x_raw = self.w_raw_in(x_raw)
--------+ ---- 
--------+-----         self.has_aux_out = num_aux_outs is not None
--------+-----@@ -76,7 +78,8 @@ class Model(nn.Module):
--------+ ----         x = x_raw
--------++----+        tgt=self.embedding_tgt(y)
--------+ ---- 
--------+ ----         x = x.transpose(0,1) # put time first
--------+------        x = self.transformer(x)
--------+-----+        x = self.transformerEncoder(x)
--------+-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++----+        tgt = tgt.transpose(0,1) # put channel after
--------++----         x = self.transformerEncoder(x)
--------++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++----+        x = self.transformerDecoder(tgt, x)
--------+ ----         x = x.transpose(0,1)
--------+ ---- 
--------+ ----         if self.has_aux_out:
--------++----diff --git a/data_utils.py b/data_utils.py
--------++----index 11d4805..e4ac852 100644
--------++------- a/data_utils.py
--------++----+++ b/data_utils.py
--------++----@@ -244,6 +244,7 @@ class TextTransform(object):
--------++----     def __init__(self):
--------++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--------++----         self.chars = string.ascii_lowercase+string.digits+' '
--------++----+        self.vocabulary_size=len(self.chars)
--------++---- 
--------++----     def clean_text(self, text):
--------++----         text = unidecode(text)
--------+ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+-----index 571de9d..8563980 100644
--------++----index fbc0abb..400061a 100644
--------+ ------- a/models/recognition_model/log.txt
--------+ ----+++ b/models/recognition_model/log.txt
--------+-----@@ -1,5 +1,2 @@
--------+-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++----@@ -1,188 +1,2 @@
--------++-----57f8139449dd9286c2203ec2eca118a550638a7c
--------++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------+ ---- 
--------++-----diff --git a/architecture.py b/architecture.py
--------++-----index 4fc3793..b22af61 100644
--------++-------- a/architecture.py
--------++-----+++ b/architecture.py
--------++-----@@ -4,7 +4,7 @@ import torch
--------++----- from torch import nn
--------++----- import torch.nn.functional as F
--------++----- 
--------++------from transformer import TransformerEncoderLayer
--------++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------++----- 
--------++----- from absl import flags
--------++----- FLAGS = flags.FLAGS
--------++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
--------++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------++----- 
--------++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++----- 
--------++-----         self.has_aux_out = num_aux_outs is not None
--------++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
--------++-----         x = x_raw
--------++----- 
--------++-----         x = x.transpose(0,1) # put time first
--------++------        x = self.transformer(x)
--------++-----+        x = self.transformerEncoder(x)
--------++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++-----         x = x.transpose(0,1)
--------++----- 
--------++-----         if self.has_aux_out:
--------++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------++-----index 571de9d..8563980 100644
--------++-------- a/models/recognition_model/log.txt
--------++-----+++ b/models/recognition_model/log.txt
--------++-----@@ -1,5 +1,2 @@
--------++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++----- 
--------++------
--------++------['recognition_model.py', '--output_directory', './models/recognition_model/']
--------++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------++------train / dev split: 8055 200
--------++-----diff --git a/output/log.txt b/output/log.txt
--------++-----index ae42364..1d2cd8e 100644
--------++-------- a/output/log.txt
--------++-----+++ b/output/log.txt
--------++-----@@ -1,3 +1,13 @@
--------++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++----- 
--------++-----+diff --git a/output/log.txt b/output/log.txt
--------++-----+index ae42364..8563980 100644
--------++-----+--- a/output/log.txt
--------++-----++++ b/output/log.txt
--------++-----+@@ -1,3 +1,2 @@
--------++-----++57f8139449dd9286c2203ec2eca118a550638a7c
--------++-----+ 
--------++-----+-
--------++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++----- 
--------++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++-----diff --git a/transformer.py b/transformer.py
--------++-----index 6743588..ac131be 100644
--------++-------- a/transformer.py
--------++-----+++ b/transformer.py
--------++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
--------++-----         Shape:
--------++-----             see the docs in Transformer class.
--------++-----         """
--------++------        src2 = self.self_attn(src)
--------++-----+        src2 = self.self_attn(src, src, src)
--------++-----         src = src + self.dropout1(src2)
--------++-----         src = self.norm1(src)
--------++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
--------++-----         src = self.norm2(src)
--------++-----         return src
--------++----- 
--------++-----+class TransformerDecoderLayer(nn.Module):
--------++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
--------++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
--------++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
--------++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
--------++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
--------++-----+    in a different way during application.
--------++-----+
--------++-----+    Args:
--------++-----+        d_model: the number of expected features in the input (required).
--------++-----+        nhead: the number of heads in the multiheadattention models (required).
--------++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
--------++-----+        dropout: the dropout value (default=0.1).
--------++-----+        activation: the activation function of the intermediate layer, can be a string
--------++-----+            ("relu" or "gelu") or a unary callable. Default: relu
--------++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
--------++-----+        batch_first: If ``True``, then the input and output tensors are provided
--------++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
--------++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
--------++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
--------++-----+            Default: ``False`` (after).
--------++-----+
--------++-----+    Examples::
--------++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
--------++-----+        >>> memory = torch.rand(10, 32, 512)
--------++-----+        >>> tgt = torch.rand(20, 32, 512)
--------++-----+        >>> out = decoder_layer(tgt, memory)
--------++-----+    """
--------++-----+    # Adapted from pytorch source
--------++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------++-----+        super(TransformerDecoderLayer, self).__init__()
--------++-----+        #Attention Mechanism
--------++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------++-----+        # Implementation of Feedforward model
--------++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
--------++-----+        self.dropout = nn.Dropout(dropout)
--------++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
--------++-----+        #Normalization Layer and Dropout Layer
--------++-----+        self.norm1 = nn.LayerNorm(d_model)
--------++-----+        self.norm2 = nn.LayerNorm(d_model)
--------++-----+        self.norm3 = nn.LayerNorm(d_model)
--------++-----+        self.dropout1 = nn.Dropout(dropout)
--------++-----+        self.dropout2 = nn.Dropout(dropout)
--------++-----+        self.dropout3 = nn.Dropout(dropout)
--------++-----+        #Activation Function
--------++-----+        self.activation = nn.ReLU()
--------++-----+    
--------++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
--------++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
--------++-----+        r"""Pass the input through the encoder layer.
--------++-----+
--------++-----+        Args:
--------++-----+            tgt: the sequence to the decoder layer (required).
--------++-----+            memory: the sequence from the last layer of the encoder (required).
--------++-----+            tgt_mask: the mask for the tgt sequence (optional).
--------++-----+            memory_mask: the mask for the memory sequence (optional).
--------++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
--------++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
--------++-----+
--------++-----+        Shape:
--------++-----+            see the docs in Transformer class.
--------++-----+        """
--------++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
--------++-----+        tgt = tgt + self.dropout1(tgt2)
--------++-----+        tgt = self.norm1(tgt)
--------++-----+
--------++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
--------++-----+        tgt = tgt + self.dropout1(tgt2)
--------++-----+        tgt = self.norm1(tgt)
--------++-----+
--------++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
--------++-----+        tgt = tgt + self.dropout2(tgt2)
--------++-----+        tgt = self.norm2(tgt)
--------++-----+        return tgt
--------++-----+    
--------++-----+
--------++----- class MultiHeadAttention(nn.Module):
--------++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------++-----     super().__init__()
--------++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
--------++-----     else:
--------++-----         self.relative_positional = None
--------++----- 
--------++------  def forward(self, x):
--------++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------++-----     """Runs the multi-head self-attention layer.
--------++----- 
--------++-----     Args:
--------++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
--------++-----       A single tensor containing the output from this layer
--------++-----     """
--------++----- 
--------++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
--------++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
--------++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
--------++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
--------++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
--------++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------++----- 
--------++-----     if self.relative_positional is not None:
--------+ -----
--------+------['recognition_model.py', '--output_directory', './models/recognition_model/']
--------++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+ -----train / dev split: 8055 200
--------+-----diff --git a/output/log.txt b/output/log.txt
--------+-----index ae42364..1d2cd8e 100644
--------+-------- a/output/log.txt
--------+-----+++ b/output/log.txt
--------+-----@@ -1,3 +1,13 @@
--------+-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++----diff --git a/recognition_model.py b/recognition_model.py
--------++----index dea6d47..a46dff0 100644
--------++------- a/recognition_model.py
--------++----+++ b/recognition_model.py
--------++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
--------+ ---- 
--------+-----+diff --git a/output/log.txt b/output/log.txt
--------+-----+index ae42364..8563980 100644
--------+-----+--- a/output/log.txt
--------+-----++++ b/output/log.txt
--------+-----+@@ -1,3 +1,2 @@
--------+-----++57f8139449dd9286c2203ec2eca118a550638a7c
--------+-----+ 
--------+-----+-
--------+-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++----             X = combine_fixed_length(example['emg'], 200).to(device)
--------++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+ ---- 
--------+----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+-----diff --git a/transformer.py b/transformer.py
--------+-----index 6743588..ac131be 100644
--------+-------- a/transformer.py
--------+-----+++ b/transformer.py
--------+-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
--------+-----         Shape:
--------+-----             see the docs in Transformer class.
--------+-----         """
--------+------        src2 = self.self_attn(src)
--------+-----+        src2 = self.self_attn(src, src, src)
--------+-----         src = src + self.dropout1(src2)
--------+-----         src = self.norm1(src)
--------+-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------+-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
--------+-----         src = self.norm2(src)
--------+-----         return src
--------++-----            pred = model(X, X_raw, sess)
--------++----+            pred = model(X, X_raw, tgt, sess)
--------++----             pred = F.log_softmax(pred, 2)
--------+ ---- 
--------+-----+class TransformerDecoderLayer(nn.Module):
--------+-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
--------+-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
--------+-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
--------+-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
--------+-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
--------+-----+    in a different way during application.
--------+-----+
--------+-----+    Args:
--------+-----+        d_model: the number of expected features in the input (required).
--------+-----+        nhead: the number of heads in the multiheadattention models (required).
--------+-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
--------+-----+        dropout: the dropout value (default=0.1).
--------+-----+        activation: the activation function of the intermediate layer, can be a string
--------+-----+            ("relu" or "gelu") or a unary callable. Default: relu
--------+-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
--------+-----+        batch_first: If ``True``, then the input and output tensors are provided
--------+-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
--------+-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
--------+-----+            attention and feedforward operations, respectively. Otherwise it's done after.
--------+-----+            Default: ``False`` (after).
--------+-----+
--------+-----+    Examples::
--------+-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
--------+-----+        >>> memory = torch.rand(10, 32, 512)
--------+-----+        >>> tgt = torch.rand(20, 32, 512)
--------+-----+        >>> out = decoder_layer(tgt, memory)
--------+-----+    """
--------+-----+    # Adapted from pytorch source
--------+-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------+-----+        super(TransformerDecoderLayer, self).__init__()
--------+-----+        #Attention Mechanism
--------+-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------+-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------+-----+        # Implementation of Feedforward model
--------+-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
--------+-----+        self.dropout = nn.Dropout(dropout)
--------+-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
--------+-----+        #Normalization Layer and Dropout Layer
--------+-----+        self.norm1 = nn.LayerNorm(d_model)
--------+-----+        self.norm2 = nn.LayerNorm(d_model)
--------+-----+        self.norm3 = nn.LayerNorm(d_model)
--------+-----+        self.dropout1 = nn.Dropout(dropout)
--------+-----+        self.dropout2 = nn.Dropout(dropout)
--------+-----+        self.dropout3 = nn.Dropout(dropout)
--------+-----+        #Activation Function
--------+-----+        self.activation = nn.ReLU()
--------+-----+    
--------+-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
--------+-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
--------+-----+        r"""Pass the input through the encoder layer.
--------+-----+
--------+-----+        Args:
--------+-----+            tgt: the sequence to the decoder layer (required).
--------+-----+            memory: the sequence from the last layer of the encoder (required).
--------+-----+            tgt_mask: the mask for the tgt sequence (optional).
--------+-----+            memory_mask: the mask for the memory sequence (optional).
--------+-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
--------+-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
--------+-----+
--------+-----+        Shape:
--------+-----+            see the docs in Transformer class.
--------+-----+        """
--------+-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+-----+        tgt = tgt + self.dropout1(tgt2)
--------+-----+        tgt = self.norm1(tgt)
--------+-----+
--------+-----+        tgt2=self.multihead_attn(tgt, memory, memory)
--------+-----+        tgt = tgt + self.dropout1(tgt2)
--------+-----+        tgt = self.norm1(tgt)
--------+-----+
--------+-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
--------+-----+        tgt = tgt + self.dropout2(tgt2)
--------+-----+        tgt = self.norm2(tgt)
--------+-----+        return tgt
--------+-----+    
--------+-----+
--------+----- class MultiHeadAttention(nn.Module):
--------+-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------+-----     super().__init__()
--------+-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
--------+-----     else:
--------+-----         self.relative_positional = None
--------+----- 
--------+------  def forward(self, x):
--------+-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------+-----     """Runs the multi-head self-attention layer.
--------+----- 
--------+-----     Args:
--------+-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
--------+-----       A single tensor containing the output from this layer
--------+-----     """
--------+----- 
--------+------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
--------+------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
--------+------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
--------+-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
--------+-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
--------+-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------+-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+----- 
--------+-----     if self.relative_positional is not None:
--------++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+ ----
--------+ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+ ----train / dev split: 8055 200
--------+ ---diff --git a/recognition_model.py b/recognition_model.py
--------+----index dea6d47..a46dff0 100644
--------++---index a46dff0..8fd300c 100644
--------+ ------ a/recognition_model.py
--------+ ---+++ b/recognition_model.py
--------+----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
--------++---@@ -6,6 +6,7 @@ import subprocess
--------++--- from ctcdecode import CTCBeamDecoder
--------++--- import jiwer
--------++--- import random
--------++---+from torch.utils.tensorboard import SummaryWriter
--------++--- 
--------++--- import torch
--------++--- from torch import nn
--------++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
--------++--- 
--------++--- from read_emg import EMGDataset, SizeAwareSampler
--------++--- from architecture import Model
--------++----from data_utils import combine_fixed_length, decollate_tensor
--------++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
--------++--- from transformer import TransformerEncoderLayer
--------++--- 
--------++--- from absl import flags
--------++---@@ -62,17 +63,21 @@ def test(model, testset, device):
--------++---     return jiwer.wer(references, predictions)
--------++--- 
--------++--- 
--------++----def train_model(trainset, devset, device, n_epochs=200):
--------++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++----
--------++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------++---+    #Define Dataloader
--------++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++--- 
--------++---+    #Define model and loss function
--------++---     n_chars = len(devset.text_transform.chars)
--------++---     model = Model(devset.num_features, n_chars+1).to(device)
--------++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------++--- 
--------++---     if FLAGS.start_training_from is not None:
--------++---         state_dict = torch.load(FLAGS.start_training_from)
--------++---         model.load_state_dict(state_dict, strict=False)
--------+ --- 
--------++---+    #Define optimizer and scheduler for the learning rate
--------++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
--------++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
--------++--- 
--------++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
--------++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
--------++--- 
--------++---     batch_idx = 0
--------++---+    train_loss= 0
--------++---+    eval_loss = 0
--------++---     optim.zero_grad()
--------++---     for epoch_idx in range(n_epochs):
--------++---+        model.train()
--------++---         losses = []
--------++----        for example in dataloader:
--------++---+        for example in dataloader_training:
--------++---             schedule_lr(batch_idx)
--------++--- 
--------++---+            #Preprosessing of the input and target for the model
--------+ ---             X = combine_fixed_length(example['emg'], 200).to(device)
--------+ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------+ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------++--- 
--------++---+            #Shifting target for input decoder and loss
--------++---+            tgt= y[:,:-1]
--------++---+            target= y[:,1:]
--------++---+
--------++---+            #Prediction
--------++---             pred = model(X, X_raw, tgt, sess)
--------++----            pred = F.log_softmax(pred, 2)
--------++--- 
--------++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------++---+            #Primary Loss
--------++---+            pred=pred.permute(0,2,1)
--------++---+            loss = loss_fn(pred, target)
--------++---+
--------++---+            #Auxiliary Loss
--------++---+            #pred = F.log_softmax(pred, 2)
--------++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++---             losses.append(loss.item())
--------++---+            train_loss += loss.item()
--------++--- 
--------++---             loss.backward()
--------++---             if (batch_idx+1) % 2 == 0:
--------++---                 optim.step()
--------++---                 optim.zero_grad()
--------++--- 
--------++---+            #Report plots in tensorboard
--------++---+            if batch_idx % report_every == report_every - 2:     
--------++---+                #Evaluation
--------++---+                model.eval()
--------++---+                with torch.no_grad():
--------++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------++---+                        X_raw = example['raw_emg'].to(device)
--------++---+                        sess = example['session_ids'].to(device)
--------++---+                        y = example['text_int'].to(device)
--------++---+
--------++---+                        #Shifting target for input decoder and loss
--------++---+                        tgt= y[:,:-1]
--------++---+                        target= y[:,1:]
--------++---+
--------++---+                        #Prediction without the 197-th batch because of missing label
--------++---+                        if idx != 197:
--------++---+                            pred = model(X, X_raw, tgt, sess)
--------++---+                            #Primary Loss
--------++---+                            pred=pred.permute(0,2,1)
--------++---+                            loss = loss_fn(pred, target)
--------++---+                            eval_loss += loss.item()
--------++---+
--------++---+                #Writing on tensorboard
--------++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------++---+                train_loss= 0
--------++---+                eval_loss= 0
--------++---+
--------++---+            #Increment counter        
--------++---             batch_idx += 1
--------++----        train_loss = np.mean(losses)
--------++---+
--------++---+        #Testing and change learning rate
--------++---         val = test(model, devset, device)
--------++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
--------++---         lr_sched.step()
--------++---+    
--------++---+        #Logging
--------++---+        train_loss = np.mean(losses)
--------++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
--------++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
--------++--- 
--------++---@@ -148,8 +201,9 @@ def main():
--------++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
--------++--- 
--------++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
--------++---+    writer = SummaryWriter(log_dir="./content/runs")
--------++--- 
--------++----    model = train_model(trainset, devset, device)
--------++---+    model = train_model(trainset, devset ,device, writer)
--------+ --- 
--------+-----            pred = model(X, X_raw, sess)
--------+----+            pred = model(X, X_raw, tgt, sess)
--------+----             pred = F.log_softmax(pred, 2)
--------++--- if __name__ == '__main__':
--------++---     FLAGS(sys.argv)
--------++---diff --git a/transformer.py b/transformer.py
--------++---index ac131be..51e1f2e 100644
--------++------ a/transformer.py
--------++---+++ b/transformer.py
--------++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
--------++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------++---     self.d_qkv = d_qkv
--------+ --- 
--------+----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++---+    #self.kdim = kdim if kdim is not None else embed_dim
--------++---+    #self.vdim = vdim if vdim is not None else embed_dim
--------++---+
--------++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+ ---
--------+ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+ ---train / dev split: 8055 200
--------+ --diff --git a/recognition_model.py b/recognition_model.py
--------+---index a46dff0..8fd300c 100644
--------++--index fde5a40..6d5143b 100644
--------+ ----- a/recognition_model.py
--------+ --+++ b/recognition_model.py
--------+---@@ -6,6 +6,7 @@ import subprocess
--------+--- from ctcdecode import CTCBeamDecoder
--------+--- import jiwer
--------+--- import random
--------+---+from torch.utils.tensorboard import SummaryWriter
--------+--- 
--------+--- import torch
--------+--- from torch import nn
--------+---@@ -13,7 +14,7 @@ import torch.nn.functional as F
--------+--- 
--------+--- from read_emg import EMGDataset, SizeAwareSampler
--------+--- from architecture import Model
--------+----from data_utils import combine_fixed_length, decollate_tensor
--------+---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
--------+--- from transformer import TransformerEncoderLayer
--------+--- 
--------+--- from absl import flags
--------+---@@ -62,17 +63,21 @@ def test(model, testset, device):
--------++--@@ -63,14 +63,14 @@ def test(model, testset, device):
--------+ --     return jiwer.wer(references, predictions)
--------+ -- 
--------+ -- 
--------+----def train_model(trainset, devset, device, n_epochs=200):
--------+----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+----
--------+---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------+---+    #Define Dataloader
--------+---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
--------++--     #Define Dataloader
--------++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------+ -- 
--------+---+    #Define model and loss function
--------++--     #Define model and loss function
--------+ --     n_chars = len(devset.text_transform.chars)
--------+---     model = Model(devset.num_features, n_chars+1).to(device)
--------+---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------++---    model = Model(devset.num_features, n_chars+1).to(device)
--------++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------+ -- 
--------+ --     if FLAGS.start_training_from is not None:
--------+---         state_dict = torch.load(FLAGS.start_training_from)
--------+---         model.load_state_dict(state_dict, strict=False)
--------+--- 
--------+---+    #Define optimizer and scheduler for the learning rate
--------+---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
--------+---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
--------+--- 
--------+---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
--------+---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
--------+--- 
--------+---     batch_idx = 0
--------+---+    train_loss= 0
--------+---+    eval_loss = 0
--------+---     optim.zero_grad()
--------+---     for epoch_idx in range(n_epochs):
--------+---+        model.train()
--------+---         losses = []
--------+----        for example in dataloader:
--------+---+        for example in dataloader_training:
--------+---             schedule_lr(batch_idx)
--------++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------++--             target= y[:,1:]
--------+ -- 
--------+---+            #Preprosessing of the input and target for the model
--------+---             X = combine_fixed_length(example['emg'], 200).to(device)
--------+---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------+---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------++--             #Prediction
--------++---            pred = model(X, X_raw, tgt, sess)
--------++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
--------+ -- 
--------+---+            #Shifting target for input decoder and loss
--------+---+            tgt= y[:,:-1]
--------+---+            target= y[:,1:]
--------+---+
--------+---+            #Prediction
--------+---             pred = model(X, X_raw, tgt, sess)
--------+----            pred = F.log_softmax(pred, 2)
--------++--             #Primary Loss
--------++---            pred=pred.permute(0,2,1)
--------++---            loss = loss_fn(pred, target)
--------++--+            out_dec=out_dec.permute(0,2,1)
--------++--+            loss_dec = loss_fn(out_dec, target)
--------+ -- 
--------+----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+---+            #Primary Loss
--------+---+            pred=pred.permute(0,2,1)
--------+---+            loss = loss_fn(pred, target)
--------++--             #Auxiliary Loss
--------++---            #pred = F.log_softmax(pred, 2)
--------++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++--+            out_enc = F.log_softmax(out_enc, 2)
--------++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+ --+
--------+---+            #Auxiliary Loss
--------+---+            #pred = F.log_softmax(pred, 2)
--------+---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------+---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
--------+ --             losses.append(loss.item())
--------+---+            train_loss += loss.item()
--------++--             train_loss += loss.item()
--------+ -- 
--------+---             loss.backward()
--------++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+ --             if (batch_idx+1) % 2 == 0:
--------+ --                 optim.step()
--------+ --                 optim.zero_grad()
--------++---
--------++---            if batch_idx % report_every == report_every - 2:     
--------++--+            
--------++--+            if False:
--------++--+            #if batch_idx % report_every == report_every - 2:     
--------++--                 #Evaluation
--------++--                 model.eval()
--------++--                 with torch.no_grad():
--------++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------++---                        X_raw = example['raw_emg'].to(device)
--------++---                        sess = example['session_ids'].to(device)
--------++---                        y = example['text_int'].to(device)
--------++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+ -- 
--------+---+            #Report plots in tensorboard
--------+---+            if batch_idx % report_every == report_every - 2:     
--------+---+                #Evaluation
--------+---+                model.eval()
--------+---+                with torch.no_grad():
--------+---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------+---+                        X_raw = example['raw_emg'].to(device)
--------+---+                        sess = example['session_ids'].to(device)
--------+---+                        y = example['text_int'].to(device)
--------+---+
--------+---+                        #Shifting target for input decoder and loss
--------+---+                        tgt= y[:,:-1]
--------+---+                        target= y[:,1:]
--------+---+
--------+---+                        #Prediction without the 197-th batch because of missing label
--------+---+                        if idx != 197:
--------+---+                            pred = model(X, X_raw, tgt, sess)
--------+---+                            #Primary Loss
--------+---+                            pred=pred.permute(0,2,1)
--------+---+                            loss = loss_fn(pred, target)
--------+---+                            eval_loss += loss.item()
--------+---+
--------+---+                #Writing on tensorboard
--------+---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------+---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------+---+                train_loss= 0
--------+---+                eval_loss= 0
--------+---+
--------+---+            #Increment counter        
--------+---             batch_idx += 1
--------+----        train_loss = np.mean(losses)
--------+---+
--------+---+        #Testing and change learning rate
--------+---         val = test(model, devset, device)
--------+---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
--------+---         lr_sched.step()
--------+---+    
--------+---+        #Logging
--------+---+        train_loss = np.mean(losses)
--------+---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
--------+---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
--------+--- 
--------+---@@ -148,8 +201,9 @@ def main():
--------+---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
--------+--- 
--------+---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
--------+---+    writer = SummaryWriter(log_dir="./content/runs")
--------++--                         #Shifting target for input decoder and loss
--------++--                         tgt= y[:,:-1]
--------++--                         target= y[:,1:]
--------+ -- 
--------+----    model = train_model(trainset, devset, device)
--------+---+    model = train_model(trainset, devset ,device, writer)
--------++--+                        print(idx)
--------++--+
--------++--                         #Prediction without the 197-th batch because of missing label
--------++---                        if idx != 197:
--------++--+                        if idx != 181:
--------++--                             pred = model(X, X_raw, tgt, sess)
--------++--                             #Primary Loss
--------++--                             pred=pred.permute(0,2,1)
--------++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+ -- 
--------+--- if __name__ == '__main__':
--------+---     FLAGS(sys.argv)
--------+---diff --git a/transformer.py b/transformer.py
--------+---index ac131be..51e1f2e 100644
--------+------ a/transformer.py
--------+---+++ b/transformer.py
--------+---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
--------+---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------+---     self.d_qkv = d_qkv
--------++--             #Increment counter        
--------++--             batch_idx += 1
--------++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--------+ -- 
--------+---+    #self.kdim = kdim if kdim is not None else embed_dim
--------+---+    #self.vdim = vdim if vdim is not None else embed_dim
--------+---+
--------+---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++--         #Testing and change learning rate
--------++--         val = test(model, devset, device)
--------+ --
--------+ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+ --train / dev split: 8055 200
--------+ -diff --git a/recognition_model.py b/recognition_model.py
--------+--index fde5a40..6d5143b 100644
--------++-index 30c5ff2..2672d45 100644
--------+ ---- a/recognition_model.py
--------+ -+++ b/recognition_model.py
--------+--@@ -63,14 +63,14 @@ def test(model, testset, device):
--------+--     return jiwer.wer(references, predictions)
--------+-- 
--------+-- 
--------+---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------+--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
--------+--     #Define Dataloader
--------+--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------+ - 
--------+ -     #Define model and loss function
--------+ -     n_chars = len(devset.text_transform.chars)
--------+---    model = Model(devset.num_features, n_chars+1).to(device)
--------+--+    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++--    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
--------+ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------+ - 
--------+ -     if FLAGS.start_training_from is not None:
--------+--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+--             target= y[:,1:]
--------++-diff --git a/transformer.py b/transformer.py
--------++-index 51e1f2e..c125841 100644
--------++---- a/transformer.py
--------++-+++ b/transformer.py
--------++-@@ -1,3 +1,4 @@
--------++-+import math
--------++- from typing import Optional
--------+ - 
--------+--             #Prediction
--------+---            pred = model(X, X_raw, tgt, sess)
--------+--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
--------++- import torch
--------++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
--------++-         Shape:
--------++-             see the docs in Transformer class.
--------++-         """
--------++--        src2 = self.self_attn(src, src, src)
--------++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
--------++-         src = src + self.dropout1(src2)
--------++-         src = self.norm1(src)
--------++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
--------++-         Shape:
--------++-             see the docs in Transformer class.
--------++-         """
--------++--        tgt2 = self.self_attn(tgt, tgt, tgt)
--------++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------++-         tgt = tgt + self.dropout1(tgt2)
--------++-         tgt = self.norm1(tgt)
--------+ - 
--------+--             #Primary Loss
--------+---            pred=pred.permute(0,2,1)
--------+---            loss = loss_fn(pred, target)
--------+--+            out_dec=out_dec.permute(0,2,1)
--------+--+            loss_dec = loss_fn(out_dec, target)
--------++--        tgt2=self.multihead_attn(tgt, memory, memory)
--------++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
--------++-         tgt = tgt + self.dropout1(tgt2)
--------++-         tgt = self.norm1(tgt)
--------+ - 
--------+--             #Auxiliary Loss
--------+---            #pred = F.log_softmax(pred, 2)
--------+---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------+---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------+--+            out_enc = F.log_softmax(out_enc, 2)
--------+--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+--+
--------+--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
--------+--             losses.append(loss.item())
--------+--             train_loss += loss.item()
--------++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
--------++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------++-     self.d_qkv = d_qkv
--------+ - 
--------+--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+--             if (batch_idx+1) % 2 == 0:
--------+--                 optim.step()
--------+--                 optim.zero_grad()
--------++--    #self.kdim = kdim if kdim is not None else embed_dim
--------++--    #self.vdim = vdim if vdim is not None else embed_dim
--------+ --
--------+---            if batch_idx % report_every == report_every - 2:     
--------+--+            
--------+--+            if False:
--------+--+            #if batch_idx % report_every == report_every - 2:     
--------+--                 #Evaluation
--------+--                 model.eval()
--------+--                 with torch.no_grad():
--------+--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------+---                        X_raw = example['raw_emg'].to(device)
--------+---                        sess = example['session_ids'].to(device)
--------+---                        y = example['text_int'].to(device)
--------+--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+-- 
--------+--                         #Shifting target for input decoder and loss
--------+--                         tgt= y[:,:-1]
--------+--                         target= y[:,1:]
--------++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
--------++-     else:
--------++-         self.relative_positional = None
--------+ - 
--------+--+                        print(idx)
--------+--+
--------+--                         #Prediction without the 197-th batch because of missing label
--------+---                        if idx != 197:
--------+--+                        if idx != 181:
--------+--                             pred = model(X, X_raw, tgt, sess)
--------+--                             #Primary Loss
--------+--                             pred=pred.permute(0,2,1)
--------+--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
--------++-     """Runs the multi-head self-attention layer.
--------+ - 
--------+--             #Increment counter        
--------+--             batch_idx += 1
--------+--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--------++-     Args:
--------++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
--------++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+ - 
--------+--         #Testing and change learning rate
--------+--         val = test(model, devset, device)
--------++-+    if attn_mask is not None:
--------++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++-+
--------++-     if self.relative_positional is not None:
--------++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--------++-         l,b,h,d = q_pos.size()
--------++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
--------++-             x = x.transpose(0, 1)
--------++-             x = x.contiguous().view(bsz_heads, length+1, length)
--------++-             return x[:, 1:, :]
--------++-+        
--------++-+
--------++-+########
--------++-+# Taken from:
--------++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
--------++-+# or also here:
--------++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
--------++-+class PositionalEncoding(nn.Module):
--------++-+
--------++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
--------++-+        super(PositionalEncoding, self).__init__()
--------++-+        self.dropout = nn.Dropout(p=dropout)
--------++-+        self.max_len = max_len
--------++-+
--------++-+        pe = torch.zeros(max_len, d_model)
--------++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
--------++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
--------++-+                             * (-math.log(10000.0) / d_model))
--------++-+        pe[:, 0::2] = torch.sin(position * div_term)
--------++-+        pe[:, 1::2] = torch.cos(position * div_term)
--------++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
--------++-+        self.register_buffer('pe', pe)  # Will not be trained.
--------++-+
--------++-+    def forward(self, x):
--------++-+        """Inputs of forward function
--------++-+        Args:
--------++-+            x: the sequence fed to the positional encoder model (required).
--------++-+        Shape:
--------++-+            x: [sequence length, batch size, embed dim]
--------++-+            output: [sequence length, batch size, embed dim]
--------++-+        """
--------++-+        assert x.size(0) < self.max_len, (
--------++-+            f"Too long sequence length: increase `max_len` of pos encoding")
--------++-+        # shape of x (len, B, dim)
--------++-+        x = x + self.pe[:x.size(0), :]
--------++-+        return self.dropout(x)
--------+ -
--------+ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+ -train / dev split: 8055 200
--------++diff --git a/output/log.txt b/output/log.txt
--------++index 1d2cd8e..979357b 100644
--------++--- a/output/log.txt
--------+++++ b/output/log.txt
--------++@@ -1,13 +1,1651 @@
--------++-57f8139449dd9286c2203ec2eca118a550638a7c
--------+++be71135adc89793578f304adb405cea80a5b2b9a
--------++ 
--------+++diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++index 2243f2d..a2c8558 100644
--------+++--- a/models/recognition_model/log.txt
--------++++++ b/models/recognition_model/log.txt
--------+++@@ -1,844 +1,898 @@
--------+++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------++++be71135adc89793578f304adb405cea80a5b2b9a
--------+++ 
--------+++-diff --git a/architecture.py b/architecture.py
--------+++-index 2413a8a..94d0de0 100644
--------+++---- a/architecture.py
--------+++-+++ b/architecture.py
--------+++-@@ -4,7 +4,7 @@ import torch
--------+++- from torch import nn
--------+++- import torch.nn.functional as F
--------+++- 
--------+++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------+++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
--------+++- 
--------+++- from absl import flags
--------+++- FLAGS = flags.FLAGS
--------+++-@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------+++-         return F.relu(x + res)
--------+++- 
--------+++- class Model(nn.Module):
--------+++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------+++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------+++-         super().__init__()
--------+++- 
--------+++-         self.conv_blocks = nn.Sequential(
--------+++-@@ -52,6 +52,7 @@ class Model(nn.Module):
--------+++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+++- 
--------+++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
--------+++- 
--------+++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++-@@ -60,9 +61,25 @@ class Model(nn.Module):
--------+++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+++- 
--------+++-         self.has_aux_loss = has_aux_loss
--------+++--
--------+++--    def forward(self, x_feat, x_raw, y,session_ids):
--------+++-+        self.device=device
--------+++-+
--------+++-+    def create_src_padding_mask(self, src):
--------+++-+        # input src of shape ()
--------+++-+        src_padding_mask = src.transpose(1, 0) == 0
--------+++-+        return src_padding_mask
--------+++-+
--------+++-+    def create_tgt_padding_mask(self, tgt):
--------+++-+        # input tgt of shape ()
--------+++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
--------+++-+        return tgt_padding_mask
--------+++-+    
--------+++-+    def forward(self, x_feat, x_raw, y, session_ids):
--------+++-         # x shape is (batch, time, electrode)
--------+++-+        # y shape is (batch, sequence_length)
--------+++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--------+++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--------+++-+        memory_key_padding_mask = src_key_padding_mask
--------+++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
--------+++- 
--------+++-         if self.training:
--------+++-             r = random.randrange(8)
--------+++-@@ -74,14 +91,16 @@ class Model(nn.Module):
--------+++-         x_raw = self.conv_blocks(x_raw)
--------+++-         x_raw = x_raw.transpose(1,2)
--------+++-         x_raw = self.w_raw_in(x_raw)
--------+++--
--------+++-         x = x_raw
--------+++-+
--------+++-+        #Embedding and positional encoding of tgt
--------+++-         tgt=self.embedding_tgt(y)
--------+++-+        tgt=self.pos_encoder(tgt)
--------+++- 
--------+++-         x = x.transpose(0,1) # put time first
--------+++-         tgt = tgt.transpose(0,1) # put channel after
--------+++--        x_encoder = self.transformerEncoder(x)
--------+++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------+++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--------+++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
--------+++- 
--------+++-         x_encoder = x_encoder.transpose(0,1)
--------+++-         x_decoder = x_decoder.transpose(0,1)
--------+++ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++-index 53839a0..1343d7d 100644
--------++++index 2243f2d..342fccd 100644
--------+++ --- a/models/recognition_model/log.txt
--------+++ +++ b/models/recognition_model/log.txt
--------+++-@@ -1,639 +1,2 @@
--------+++--2fa943cd85263a152b6be80d502eda27932ebb27
--------+++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------++++@@ -1,845 +1,2 @@
--------++++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------+++++be71135adc89793578f304adb405cea80a5b2b9a
--------+++  
--------+++ -diff --git a/architecture.py b/architecture.py
--------+++--index a8c70f3..2413a8a 100644
--------++++-index 2413a8a..94d0de0 100644
--------+++ ---- a/architecture.py
--------+++ -+++ b/architecture.py
--------++++-@@ -4,7 +4,7 @@ import torch
--------++++- from torch import nn
--------++++- import torch.nn.functional as F
--------++++- 
--------++++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------++++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
--------++++- 
--------++++- from absl import flags
--------++++- FLAGS = flags.FLAGS
--------+++ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------+++ -         return F.relu(x + res)
--------+++ - 
--------+++ - class Model(nn.Module):
--------+++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
--------+++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--------+++ -         super().__init__()
--------+++ - 
--------+++ -         self.conv_blocks = nn.Sequential(
--------+++--@@ -59,9 +59,7 @@ class Model(nn.Module):
--------+++--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+++--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++++-@@ -52,6 +52,7 @@ class Model(nn.Module):
--------++++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+++ - 
--------+++---        self.has_aux_out = num_aux_outs is not None
--------+++---        if self.has_aux_out:
--------+++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------+++--+        self.has_aux_loss = has_aux_loss
--------++++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
--------+++ - 
--------+++--     def forward(self, x_feat, x_raw, y,session_ids):
--------+++--         # x shape is (batch, time, electrode)
--------+++--@@ -82,12 +80,14 @@ class Model(nn.Module):
--------++++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++-@@ -60,9 +61,25 @@ class Model(nn.Module):
--------++++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+++ - 
--------+++--         x = x.transpose(0,1) # put time first
--------+++--         tgt = tgt.transpose(0,1) # put channel after
--------+++---        x = self.transformerEncoder(x)
--------+++---        x = self.transformerDecoder(tgt, x)
--------+++---        x = x.transpose(0,1)
--------+++--+        x_encoder = self.transformerEncoder(x)
--------+++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++++-         self.has_aux_loss = has_aux_loss
--------++++--
--------++++--    def forward(self, x_feat, x_raw, y,session_ids):
--------++++-+        self.device=device
--------++++-+
--------++++-+    def create_src_padding_mask(self, src):
--------++++-+        # input src of shape ()
--------++++-+        src_padding_mask = src.transpose(1, 0) == 0
--------++++-+        return src_padding_mask
--------++++-+
--------++++-+    def create_tgt_padding_mask(self, tgt):
--------++++-+        # input tgt of shape ()
--------++++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
--------++++-+        return tgt_padding_mask
--------++++-+    
--------++++-+    def forward(self, x_feat, x_raw, y, session_ids):
--------++++-         # x shape is (batch, time, electrode)
--------++++-+        # y shape is (batch, sequence_length)
--------++++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--------++++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--------++++-+        memory_key_padding_mask = src_key_padding_mask
--------++++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
--------+++ - 
--------+++---        if self.has_aux_out:
--------+++---            return self.w_out(x), self.w_aux(x)
--------+++--+        x_encoder = x_encoder.transpose(0,1)
--------+++--+        x_decoder = x_decoder.transpose(0,1)
--------++++-         if self.training:
--------++++-             r = random.randrange(8)
--------++++-@@ -74,14 +91,16 @@ class Model(nn.Module):
--------++++-         x_raw = self.conv_blocks(x_raw)
--------++++-         x_raw = x_raw.transpose(1,2)
--------++++-         x_raw = self.w_raw_in(x_raw)
--------++++--
--------++++-         x = x_raw
--------+++ -+
--------+++--+        if self.has_aux_loss:
--------+++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
--------+++--         else:
--------+++--             return self.w_out(x)
--------++++-+        #Embedding and positional encoding of tgt
--------++++-         tgt=self.embedding_tgt(y)
--------++++-+        tgt=self.pos_encoder(tgt)
--------+++ - 
--------+++--diff --git a/data_utils.py b/data_utils.py
--------+++--index e2632e8..8b05213 100644
--------+++----- a/data_utils.py
--------+++--+++ b/data_utils.py
--------+++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
--------++++-         x = x.transpose(0,1) # put time first
--------++++-         tgt = tgt.transpose(0,1) # put channel after
--------++++--        x_encoder = self.transformerEncoder(x)
--------++++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--------++++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
--------+++ - 
--------+++-- def combine_fixed_length_tgt(tensor_list, n_batch):
--------+++--     total_length = sum(t.size(0) for t in tensor_list)
--------+++--+    tensor_list = list(tensor_list) # copy
--------+++--     if total_length % n_batch != 0:
--------+++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------+++---        tensor_list = list(tensor_list) # copy
--------+++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------+++--         total_length += pad_length
--------+++--     tensor = torch.cat(tensor_list, 0)
--------++++-         x_encoder = x_encoder.transpose(0,1)
--------++++-         x_decoder = x_decoder.transpose(0,1)
--------+++ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++--index b8f7791..617dd85 100644
--------++++-index 53839a0..1343d7d 100644
--------+++ ---- a/models/recognition_model/log.txt
--------+++ -+++ b/models/recognition_model/log.txt
--------+++--@@ -1,480 +1,2 @@
--------+++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------+++--+2fa943cd85263a152b6be80d502eda27932ebb27
--------++++-@@ -1,639 +1,2 @@
--------++++--2fa943cd85263a152b6be80d502eda27932ebb27
--------++++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
--------+++ - 
--------+++ --diff --git a/architecture.py b/architecture.py
--------+++---index d6e99b4..a8c70f3 100644
--------++++--index a8c70f3..2413a8a 100644
--------+++ ----- a/architecture.py
--------+++ --+++ b/architecture.py
--------+++---@@ -54,7 +54,7 @@ class Model(nn.Module):
--------+++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--------++++--         return F.relu(x + res)
--------++++-- 
--------++++-- class Model(nn.Module):
--------++++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
--------++++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
--------++++--         super().__init__()
--------+++ -- 
--------+++---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++++--         self.conv_blocks = nn.Sequential(
--------++++--@@ -59,9 +59,7 @@ class Model(nn.Module):
--------+++ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+++ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++++-- 
--------++++---        self.has_aux_out = num_aux_outs is not None
--------++++---        if self.has_aux_out:
--------++++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------++++--+        self.has_aux_loss = has_aux_loss
--------++++-- 
--------++++--     def forward(self, x_feat, x_raw, y,session_ids):
--------++++--         # x shape is (batch, time, electrode)
--------++++--@@ -82,12 +80,14 @@ class Model(nn.Module):
--------++++-- 
--------++++--         x = x.transpose(0,1) # put time first
--------++++--         tgt = tgt.transpose(0,1) # put channel after
--------++++---        x = self.transformerEncoder(x)
--------++++---        x = self.transformerDecoder(tgt, x)
--------++++---        x = x.transpose(0,1)
--------++++--+        x_encoder = self.transformerEncoder(x)
--------++++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
--------++++-- 
--------++++---        if self.has_aux_out:
--------++++---            return self.w_out(x), self.w_aux(x)
--------++++--+        x_encoder = x_encoder.transpose(0,1)
--------++++--+        x_decoder = x_decoder.transpose(0,1)
--------++++--+
--------++++--+        if self.has_aux_loss:
--------++++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
--------++++--         else:
--------++++--             return self.w_out(x)
--------++++-- 
--------+++ --diff --git a/data_utils.py b/data_utils.py
--------+++---index e4ac852..e2632e8 100644
--------++++--index e2632e8..8b05213 100644
--------+++ ----- a/data_utils.py
--------+++ --+++ b/data_utils.py
--------+++---@@ -1,3 +1,4 @@
--------+++---+import math
--------+++--- import string
--------++++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
--------+++ -- 
--------+++--- import numpy as np
--------+++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
--------+++---     n = total_length // length
--------+++---     return tensor.view(n, length, *tensor.size()[1:])
--------+++--- 
--------+++---+def combine_fixed_length_tgt(tensor_list, n_batch):
--------+++---+    total_length = sum(t.size(0) for t in tensor_list)
--------+++---+    if total_length % n_batch != 0:
--------+++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------+++---+        tensor_list = list(tensor_list) # copy
--------+++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------+++---+        total_length += pad_length
--------+++---+    tensor = torch.cat(tensor_list, 0)
--------+++---+    length = total_length // n_batch
--------+++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
--------+++---+
--------+++--- def decollate_tensor(tensor, lengths):
--------+++---     b, s, d = tensor.size()
--------+++---     tensor = tensor.view(b*s, d)
--------++++-- def combine_fixed_length_tgt(tensor_list, n_batch):
--------++++--     total_length = sum(t.size(0) for t in tensor_list)
--------++++--+    tensor_list = list(tensor_list) # copy
--------++++--     if total_length % n_batch != 0:
--------++++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------++++---        tensor_list = list(tensor_list) # copy
--------++++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------++++--         total_length += pad_length
--------++++--     tensor = torch.cat(tensor_list, 0)
--------+++ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++---index e890f0f..1ee3421 100644
--------++++--index b8f7791..617dd85 100644
--------+++ ----- a/models/recognition_model/log.txt
--------+++ --+++ b/models/recognition_model/log.txt
--------+++---@@ -1,265 +1,2 @@
--------+++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------+++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------++++--@@ -1,480 +1,2 @@
--------++++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------++++--+2fa943cd85263a152b6be80d502eda27932ebb27
--------+++ -- 
--------+++ ---diff --git a/architecture.py b/architecture.py
--------+++----index b22af61..d6e99b4 100644
--------++++---index d6e99b4..a8c70f3 100644
--------+++ ------ a/architecture.py
--------+++ ---+++ b/architecture.py
--------+++----@@ -51,6 +51,8 @@ class Model(nn.Module):
--------+++----         )
--------+++----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------++++---@@ -54,7 +54,7 @@ class Model(nn.Module):
--------++++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+++ --- 
--------+++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------+++----+
--------+++ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+++----@@ -61,7 +63,7 @@ class Model(nn.Module):
--------+++----         if self.has_aux_out:
--------+++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------+++---- 
--------+++-----    def forward(self, x_feat, x_raw, session_ids):
--------+++----+    def forward(self, x_feat, x_raw, y,session_ids):
--------+++----         # x shape is (batch, time, electrode)
--------+++---- 
--------+++----         if self.training:
--------+++----@@ -76,10 +78,12 @@ class Model(nn.Module):
--------+++----         x_raw = self.w_raw_in(x_raw)
--------+++---- 
--------+++----         x = x_raw
--------+++----+        tgt=self.embedding_tgt(y)
--------+++---- 
--------+++----         x = x.transpose(0,1) # put time first
--------+++----+        tgt = tgt.transpose(0,1) # put channel after
--------+++----         x = self.transformerEncoder(x)
--------+++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
--------+++----+        x = self.transformerDecoder(tgt, x)
--------+++----         x = x.transpose(0,1)
--------+++---- 
--------+++----         if self.has_aux_out:
--------++++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------++++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------+++ ---diff --git a/data_utils.py b/data_utils.py
--------+++----index 11d4805..e4ac852 100644
--------++++---index e4ac852..e2632e8 100644
--------+++ ------ a/data_utils.py
--------+++ ---+++ b/data_utils.py
--------+++----@@ -244,6 +244,7 @@ class TextTransform(object):
--------+++----     def __init__(self):
--------+++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--------+++----         self.chars = string.ascii_lowercase+string.digits+' '
--------+++----+        self.vocabulary_size=len(self.chars)
--------++++---@@ -1,3 +1,4 @@
--------++++---+import math
--------++++--- import string
--------++++--- 
--------++++--- import numpy as np
--------++++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
--------++++---     n = total_length // length
--------++++---     return tensor.view(n, length, *tensor.size()[1:])
--------+++ --- 
--------+++----     def clean_text(self, text):
--------+++----         text = unidecode(text)
--------++++---+def combine_fixed_length_tgt(tensor_list, n_batch):
--------++++---+    total_length = sum(t.size(0) for t in tensor_list)
--------++++---+    if total_length % n_batch != 0:
--------++++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
--------++++---+        tensor_list = list(tensor_list) # copy
--------++++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
--------++++---+        total_length += pad_length
--------++++---+    tensor = torch.cat(tensor_list, 0)
--------++++---+    length = total_length // n_batch
--------++++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
--------++++---+
--------++++--- def decollate_tensor(tensor, lengths):
--------++++---     b, s, d = tensor.size()
--------++++---     tensor = tensor.view(b*s, d)
--------+++ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++----index fbc0abb..400061a 100644
--------++++---index e890f0f..1ee3421 100644
--------+++ ------ a/models/recognition_model/log.txt
--------+++ ---+++ b/models/recognition_model/log.txt
--------+++----@@ -1,188 +1,2 @@
--------+++-----57f8139449dd9286c2203ec2eca118a550638a7c
--------+++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------++++---@@ -1,265 +1,2 @@
--------++++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------++++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
--------+++ --- 
--------+++ ----diff --git a/architecture.py b/architecture.py
--------+++-----index 4fc3793..b22af61 100644
--------++++----index b22af61..d6e99b4 100644
--------+++ ------- a/architecture.py
--------+++ ----+++ b/architecture.py
--------+++-----@@ -4,7 +4,7 @@ import torch
--------+++----- from torch import nn
--------+++----- import torch.nn.functional as F
--------+++----- 
--------+++------from transformer import TransformerEncoderLayer
--------+++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------+++----- 
--------+++----- from absl import flags
--------+++----- FLAGS = flags.FLAGS
--------+++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
--------++++----@@ -51,6 +51,8 @@ class Model(nn.Module):
--------++++----         )
--------+++ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------+++ ---- 
--------++++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--------++++----+
--------+++ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------+++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------+++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------+++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++++----@@ -61,7 +63,7 @@ class Model(nn.Module):
--------++++----         if self.has_aux_out:
--------++++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
--------++++---- 
--------++++-----    def forward(self, x_feat, x_raw, session_ids):
--------++++----+    def forward(self, x_feat, x_raw, y,session_ids):
--------++++----         # x shape is (batch, time, electrode)
--------++++---- 
--------++++----         if self.training:
--------++++----@@ -76,10 +78,12 @@ class Model(nn.Module):
--------++++----         x_raw = self.w_raw_in(x_raw)
--------+++ ---- 
--------+++-----         self.has_aux_out = num_aux_outs is not None
--------+++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
--------+++ ----         x = x_raw
--------++++----+        tgt=self.embedding_tgt(y)
--------+++ ---- 
--------+++ ----         x = x.transpose(0,1) # put time first
--------+++------        x = self.transformer(x)
--------+++-----+        x = self.transformerEncoder(x)
--------+++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++++----+        tgt = tgt.transpose(0,1) # put channel after
--------++++----         x = self.transformerEncoder(x)
--------++++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++++----+        x = self.transformerDecoder(tgt, x)
--------+++ ----         x = x.transpose(0,1)
--------+++ ---- 
--------+++ ----         if self.has_aux_out:
--------++++----diff --git a/data_utils.py b/data_utils.py
--------++++----index 11d4805..e4ac852 100644
--------++++------- a/data_utils.py
--------++++----+++ b/data_utils.py
--------++++----@@ -244,6 +244,7 @@ class TextTransform(object):
--------++++----     def __init__(self):
--------++++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--------++++----         self.chars = string.ascii_lowercase+string.digits+' '
--------++++----+        self.vocabulary_size=len(self.chars)
--------++++---- 
--------++++----     def clean_text(self, text):
--------++++----         text = unidecode(text)
--------+++ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------+++-----index 571de9d..8563980 100644
--------++++----index fbc0abb..400061a 100644
--------+++ ------- a/models/recognition_model/log.txt
--------+++ ----+++ b/models/recognition_model/log.txt
--------+++-----@@ -1,5 +1,2 @@
--------+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++++----@@ -1,188 +1,2 @@
--------++++-----57f8139449dd9286c2203ec2eca118a550638a7c
--------++++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
--------+++ ---- 
--------++++-----diff --git a/architecture.py b/architecture.py
--------++++-----index 4fc3793..b22af61 100644
--------++++-------- a/architecture.py
--------++++-----+++ b/architecture.py
--------++++-----@@ -4,7 +4,7 @@ import torch
--------++++----- from torch import nn
--------++++----- import torch.nn.functional as F
--------++++----- 
--------++++------from transformer import TransformerEncoderLayer
--------++++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--------++++----- 
--------++++----- from absl import flags
--------++++----- FLAGS = flags.FLAGS
--------++++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
--------++++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
--------++++----- 
--------++++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--------++++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
--------++++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
--------++++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--------++++----- 
--------++++-----         self.has_aux_out = num_aux_outs is not None
--------++++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
--------++++-----         x = x_raw
--------++++----- 
--------++++-----         x = x.transpose(0,1) # put time first
--------++++------        x = self.transformer(x)
--------++++-----+        x = self.transformerEncoder(x)
--------++++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
--------++++-----         x = x.transpose(0,1)
--------++++----- 
--------++++-----         if self.has_aux_out:
--------++++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--------++++-----index 571de9d..8563980 100644
--------++++-------- a/models/recognition_model/log.txt
--------++++-----+++ b/models/recognition_model/log.txt
--------++++-----@@ -1,5 +1,2 @@
--------++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++++----- 
--------++++------
--------++++------['recognition_model.py', '--output_directory', './models/recognition_model/']
--------++++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------++++------train / dev split: 8055 200
--------++++-----diff --git a/output/log.txt b/output/log.txt
--------++++-----index ae42364..1d2cd8e 100644
--------++++-------- a/output/log.txt
--------++++-----+++ b/output/log.txt
--------++++-----@@ -1,3 +1,13 @@
--------++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++++----- 
--------++++-----+diff --git a/output/log.txt b/output/log.txt
--------++++-----+index ae42364..8563980 100644
--------++++-----+--- a/output/log.txt
--------++++-----++++ b/output/log.txt
--------++++-----+@@ -1,3 +1,2 @@
--------++++-----++57f8139449dd9286c2203ec2eca118a550638a7c
--------++++-----+ 
--------++++-----+-
--------++++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++++----- 
--------++++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++++-----diff --git a/transformer.py b/transformer.py
--------++++-----index 6743588..ac131be 100644
--------++++-------- a/transformer.py
--------++++-----+++ b/transformer.py
--------++++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
--------++++-----         Shape:
--------++++-----             see the docs in Transformer class.
--------++++-----         """
--------++++------        src2 = self.self_attn(src)
--------++++-----+        src2 = self.self_attn(src, src, src)
--------++++-----         src = src + self.dropout1(src2)
--------++++-----         src = self.norm1(src)
--------++++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------++++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
--------++++-----         src = self.norm2(src)
--------++++-----         return src
--------++++----- 
--------++++-----+class TransformerDecoderLayer(nn.Module):
--------++++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
--------++++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
--------++++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
--------++++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
--------++++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
--------++++-----+    in a different way during application.
--------++++-----+
--------++++-----+    Args:
--------++++-----+        d_model: the number of expected features in the input (required).
--------++++-----+        nhead: the number of heads in the multiheadattention models (required).
--------++++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
--------++++-----+        dropout: the dropout value (default=0.1).
--------++++-----+        activation: the activation function of the intermediate layer, can be a string
--------++++-----+            ("relu" or "gelu") or a unary callable. Default: relu
--------++++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
--------++++-----+        batch_first: If ``True``, then the input and output tensors are provided
--------++++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
--------++++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
--------++++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
--------++++-----+            Default: ``False`` (after).
--------++++-----+
--------++++-----+    Examples::
--------++++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
--------++++-----+        >>> memory = torch.rand(10, 32, 512)
--------++++-----+        >>> tgt = torch.rand(20, 32, 512)
--------++++-----+        >>> out = decoder_layer(tgt, memory)
--------++++-----+    """
--------++++-----+    # Adapted from pytorch source
--------++++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------++++-----+        super(TransformerDecoderLayer, self).__init__()
--------++++-----+        #Attention Mechanism
--------++++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------++++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------++++-----+        # Implementation of Feedforward model
--------++++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
--------++++-----+        self.dropout = nn.Dropout(dropout)
--------++++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
--------++++-----+        #Normalization Layer and Dropout Layer
--------++++-----+        self.norm1 = nn.LayerNorm(d_model)
--------++++-----+        self.norm2 = nn.LayerNorm(d_model)
--------++++-----+        self.norm3 = nn.LayerNorm(d_model)
--------++++-----+        self.dropout1 = nn.Dropout(dropout)
--------++++-----+        self.dropout2 = nn.Dropout(dropout)
--------++++-----+        self.dropout3 = nn.Dropout(dropout)
--------++++-----+        #Activation Function
--------++++-----+        self.activation = nn.ReLU()
--------++++-----+    
--------++++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
--------++++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
--------++++-----+        r"""Pass the input through the encoder layer.
--------++++-----+
--------++++-----+        Args:
--------++++-----+            tgt: the sequence to the decoder layer (required).
--------++++-----+            memory: the sequence from the last layer of the encoder (required).
--------++++-----+            tgt_mask: the mask for the tgt sequence (optional).
--------++++-----+            memory_mask: the mask for the memory sequence (optional).
--------++++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
--------++++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
--------++++-----+
--------++++-----+        Shape:
--------++++-----+            see the docs in Transformer class.
--------++++-----+        """
--------++++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
--------++++-----+        tgt = tgt + self.dropout1(tgt2)
--------++++-----+        tgt = self.norm1(tgt)
--------++++-----+
--------++++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
--------++++-----+        tgt = tgt + self.dropout1(tgt2)
--------++++-----+        tgt = self.norm1(tgt)
--------++++-----+
--------++++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
--------++++-----+        tgt = tgt + self.dropout2(tgt2)
--------++++-----+        tgt = self.norm2(tgt)
--------++++-----+        return tgt
--------++++-----+    
--------++++-----+
--------++++----- class MultiHeadAttention(nn.Module):
--------++++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------++++-----     super().__init__()
--------++++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
--------++++-----     else:
--------++++-----         self.relative_positional = None
--------++++----- 
--------++++------  def forward(self, x):
--------++++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------++++-----     """Runs the multi-head self-attention layer.
--------++++----- 
--------++++-----     Args:
--------++++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
--------++++-----       A single tensor containing the output from this layer
--------++++-----     """
--------++++----- 
--------++++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
--------++++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
--------++++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
--------++++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
--------++++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
--------++++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------++++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------++++----- 
--------++++-----     if self.relative_positional is not None:
--------+++ -----
--------+++------['recognition_model.py', '--output_directory', './models/recognition_model/']
--------++++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++ -----train / dev split: 8055 200
--------+++-----diff --git a/output/log.txt b/output/log.txt
--------+++-----index ae42364..1d2cd8e 100644
--------+++-------- a/output/log.txt
--------+++-----+++ b/output/log.txt
--------+++-----@@ -1,3 +1,13 @@
--------+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
--------++++----diff --git a/recognition_model.py b/recognition_model.py
--------++++----index dea6d47..a46dff0 100644
--------++++------- a/recognition_model.py
--------++++----+++ b/recognition_model.py
--------++++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
--------+++ ---- 
--------+++-----+diff --git a/output/log.txt b/output/log.txt
--------+++-----+index ae42364..8563980 100644
--------+++-----+--- a/output/log.txt
--------+++-----++++ b/output/log.txt
--------+++-----+@@ -1,3 +1,2 @@
--------+++-----++57f8139449dd9286c2203ec2eca118a550638a7c
--------+++-----+ 
--------+++-----+-
--------+++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++++----             X = combine_fixed_length(example['emg'], 200).to(device)
--------++++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------++++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+++ ---- 
--------+++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+++-----diff --git a/transformer.py b/transformer.py
--------+++-----index 6743588..ac131be 100644
--------+++-------- a/transformer.py
--------+++-----+++ b/transformer.py
--------+++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
--------+++-----         Shape:
--------+++-----             see the docs in Transformer class.
--------+++-----         """
--------+++------        src2 = self.self_attn(src)
--------+++-----+        src2 = self.self_attn(src, src, src)
--------+++-----         src = src + self.dropout1(src2)
--------+++-----         src = self.norm1(src)
--------+++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------+++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
--------+++-----         src = self.norm2(src)
--------+++-----         return src
--------++++-----            pred = model(X, X_raw, sess)
--------++++----+            pred = model(X, X_raw, tgt, sess)
--------++++----             pred = F.log_softmax(pred, 2)
--------+++ ---- 
--------+++-----+class TransformerDecoderLayer(nn.Module):
--------+++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
--------+++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
--------+++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
--------+++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
--------+++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
--------+++-----+    in a different way during application.
--------+++-----+
--------+++-----+    Args:
--------+++-----+        d_model: the number of expected features in the input (required).
--------+++-----+        nhead: the number of heads in the multiheadattention models (required).
--------+++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
--------+++-----+        dropout: the dropout value (default=0.1).
--------+++-----+        activation: the activation function of the intermediate layer, can be a string
--------+++-----+            ("relu" or "gelu") or a unary callable. Default: relu
--------+++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
--------+++-----+        batch_first: If ``True``, then the input and output tensors are provided
--------+++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
--------+++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
--------+++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
--------+++-----+            Default: ``False`` (after).
--------+++-----+
--------+++-----+    Examples::
--------+++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
--------+++-----+        >>> memory = torch.rand(10, 32, 512)
--------+++-----+        >>> tgt = torch.rand(20, 32, 512)
--------+++-----+        >>> out = decoder_layer(tgt, memory)
--------+++-----+    """
--------+++-----+    # Adapted from pytorch source
--------+++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------+++-----+        super(TransformerDecoderLayer, self).__init__()
--------+++-----+        #Attention Mechanism
--------+++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------+++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
--------+++-----+        # Implementation of Feedforward model
--------+++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
--------+++-----+        self.dropout = nn.Dropout(dropout)
--------+++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
--------+++-----+        #Normalization Layer and Dropout Layer
--------+++-----+        self.norm1 = nn.LayerNorm(d_model)
--------+++-----+        self.norm2 = nn.LayerNorm(d_model)
--------+++-----+        self.norm3 = nn.LayerNorm(d_model)
--------+++-----+        self.dropout1 = nn.Dropout(dropout)
--------+++-----+        self.dropout2 = nn.Dropout(dropout)
--------+++-----+        self.dropout3 = nn.Dropout(dropout)
--------+++-----+        #Activation Function
--------+++-----+        self.activation = nn.ReLU()
--------+++-----+    
--------+++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
--------+++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
--------+++-----+        r"""Pass the input through the encoder layer.
--------+++-----+
--------+++-----+        Args:
--------+++-----+            tgt: the sequence to the decoder layer (required).
--------+++-----+            memory: the sequence from the last layer of the encoder (required).
--------+++-----+            tgt_mask: the mask for the tgt sequence (optional).
--------+++-----+            memory_mask: the mask for the memory sequence (optional).
--------+++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
--------+++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
--------+++-----+
--------+++-----+        Shape:
--------+++-----+            see the docs in Transformer class.
--------+++-----+        """
--------+++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+++-----+        tgt = tgt + self.dropout1(tgt2)
--------+++-----+        tgt = self.norm1(tgt)
--------+++-----+
--------+++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
--------+++-----+        tgt = tgt + self.dropout1(tgt2)
--------+++-----+        tgt = self.norm1(tgt)
--------+++-----+
--------+++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
--------+++-----+        tgt = tgt + self.dropout2(tgt2)
--------+++-----+        tgt = self.norm2(tgt)
--------+++-----+        return tgt
--------+++-----+    
--------+++-----+
--------+++----- class MultiHeadAttention(nn.Module):
--------+++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
--------+++-----     super().__init__()
--------+++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
--------+++-----     else:
--------+++-----         self.relative_positional = None
--------+++----- 
--------+++------  def forward(self, x):
--------+++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------+++-----     """Runs the multi-head self-attention layer.
--------+++----- 
--------+++-----     Args:
--------+++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
--------+++-----       A single tensor containing the output from this layer
--------+++-----     """
--------+++----- 
--------+++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
--------+++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
--------+++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
--------+++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
--------+++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
--------+++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------+++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+++----- 
--------+++-----     if self.relative_positional is not None:
--------++++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+++ ----
--------+++ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++ ----train / dev split: 8055 200
--------+++ ---diff --git a/recognition_model.py b/recognition_model.py
--------+++----index dea6d47..a46dff0 100644
--------++++---index a46dff0..8fd300c 100644
--------+++ ------ a/recognition_model.py
--------+++ ---+++ b/recognition_model.py
--------+++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
--------++++---@@ -6,6 +6,7 @@ import subprocess
--------++++--- from ctcdecode import CTCBeamDecoder
--------++++--- import jiwer
--------++++--- import random
--------++++---+from torch.utils.tensorboard import SummaryWriter
--------++++--- 
--------++++--- import torch
--------++++--- from torch import nn
--------++++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
--------++++--- 
--------++++--- from read_emg import EMGDataset, SizeAwareSampler
--------++++--- from architecture import Model
--------++++----from data_utils import combine_fixed_length, decollate_tensor
--------++++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
--------++++--- from transformer import TransformerEncoderLayer
--------++++--- 
--------++++--- from absl import flags
--------++++---@@ -62,17 +63,21 @@ def test(model, testset, device):
--------++++---     return jiwer.wer(references, predictions)
--------++++--- 
--------++++--- 
--------++++----def train_model(trainset, devset, device, n_epochs=200):
--------++++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++++----
--------++++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------++++---+    #Define Dataloader
--------++++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++++--- 
--------++++---+    #Define model and loss function
--------++++---     n_chars = len(devset.text_transform.chars)
--------++++---     model = Model(devset.num_features, n_chars+1).to(device)
--------++++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------++++--- 
--------++++---     if FLAGS.start_training_from is not None:
--------++++---         state_dict = torch.load(FLAGS.start_training_from)
--------++++---         model.load_state_dict(state_dict, strict=False)
--------++++--- 
--------++++---+    #Define optimizer and scheduler for the learning rate
--------++++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
--------++++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
--------++++--- 
--------++++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
--------++++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
--------+++ --- 
--------++++---     batch_idx = 0
--------++++---+    train_loss= 0
--------++++---+    eval_loss = 0
--------++++---     optim.zero_grad()
--------++++---     for epoch_idx in range(n_epochs):
--------++++---+        model.train()
--------++++---         losses = []
--------++++----        for example in dataloader:
--------++++---+        for example in dataloader_training:
--------++++---             schedule_lr(batch_idx)
--------++++--- 
--------++++---+            #Preprosessing of the input and target for the model
--------+++ ---             X = combine_fixed_length(example['emg'], 200).to(device)
--------+++ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------++++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------+++ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------++++--- 
--------++++---+            #Shifting target for input decoder and loss
--------++++---+            tgt= y[:,:-1]
--------++++---+            target= y[:,1:]
--------++++---+
--------++++---+            #Prediction
--------++++---             pred = model(X, X_raw, tgt, sess)
--------++++----            pred = F.log_softmax(pred, 2)
--------++++--- 
--------++++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------++++---+            #Primary Loss
--------++++---+            pred=pred.permute(0,2,1)
--------++++---+            loss = loss_fn(pred, target)
--------++++---+
--------++++---+            #Auxiliary Loss
--------++++---+            #pred = F.log_softmax(pred, 2)
--------++++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------++++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++++---             losses.append(loss.item())
--------++++---+            train_loss += loss.item()
--------++++--- 
--------++++---             loss.backward()
--------++++---             if (batch_idx+1) % 2 == 0:
--------++++---                 optim.step()
--------++++---                 optim.zero_grad()
--------++++--- 
--------++++---+            #Report plots in tensorboard
--------++++---+            if batch_idx % report_every == report_every - 2:     
--------++++---+                #Evaluation
--------++++---+                model.eval()
--------++++---+                with torch.no_grad():
--------++++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------++++---+                        X_raw = example['raw_emg'].to(device)
--------++++---+                        sess = example['session_ids'].to(device)
--------++++---+                        y = example['text_int'].to(device)
--------++++---+
--------++++---+                        #Shifting target for input decoder and loss
--------++++---+                        tgt= y[:,:-1]
--------++++---+                        target= y[:,1:]
--------++++---+
--------++++---+                        #Prediction without the 197-th batch because of missing label
--------++++---+                        if idx != 197:
--------++++---+                            pred = model(X, X_raw, tgt, sess)
--------++++---+                            #Primary Loss
--------++++---+                            pred=pred.permute(0,2,1)
--------++++---+                            loss = loss_fn(pred, target)
--------++++---+                            eval_loss += loss.item()
--------++++---+
--------++++---+                #Writing on tensorboard
--------++++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------++++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------++++---+                train_loss= 0
--------++++---+                eval_loss= 0
--------++++---+
--------++++---+            #Increment counter        
--------++++---             batch_idx += 1
--------++++----        train_loss = np.mean(losses)
--------++++---+
--------++++---+        #Testing and change learning rate
--------++++---         val = test(model, devset, device)
--------++++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
--------++++---         lr_sched.step()
--------++++---+    
--------++++---+        #Logging
--------++++---+        train_loss = np.mean(losses)
--------++++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
--------++++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
--------+++ --- 
--------+++-----            pred = model(X, X_raw, sess)
--------+++----+            pred = model(X, X_raw, tgt, sess)
--------+++----             pred = F.log_softmax(pred, 2)
--------++++---@@ -148,8 +201,9 @@ def main():
--------++++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
--------+++ --- 
--------+++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
--------++++---+    writer = SummaryWriter(log_dir="./content/runs")
--------++++--- 
--------++++----    model = train_model(trainset, devset, device)
--------++++---+    model = train_model(trainset, devset ,device, writer)
--------++++--- 
--------++++--- if __name__ == '__main__':
--------++++---     FLAGS(sys.argv)
--------++++---diff --git a/transformer.py b/transformer.py
--------++++---index ac131be..51e1f2e 100644
--------++++------ a/transformer.py
--------++++---+++ b/transformer.py
--------++++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
--------++++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------++++---     self.d_qkv = d_qkv
--------++++--- 
--------++++---+    #self.kdim = kdim if kdim is not None else embed_dim
--------++++---+    #self.vdim = vdim if vdim is not None else embed_dim
--------++++---+
--------++++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++ ---
--------+++ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++ ---train / dev split: 8055 200
--------+++ --diff --git a/recognition_model.py b/recognition_model.py
--------+++---index a46dff0..8fd300c 100644
--------++++--index fde5a40..6d5143b 100644
--------+++ ----- a/recognition_model.py
--------+++ --+++ b/recognition_model.py
--------+++---@@ -6,6 +6,7 @@ import subprocess
--------+++--- from ctcdecode import CTCBeamDecoder
--------+++--- import jiwer
--------+++--- import random
--------+++---+from torch.utils.tensorboard import SummaryWriter
--------+++--- 
--------+++--- import torch
--------+++--- from torch import nn
--------+++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
--------+++--- 
--------+++--- from read_emg import EMGDataset, SizeAwareSampler
--------+++--- from architecture import Model
--------+++----from data_utils import combine_fixed_length, decollate_tensor
--------+++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
--------+++--- from transformer import TransformerEncoderLayer
--------+++--- 
--------+++--- from absl import flags
--------+++---@@ -62,17 +63,21 @@ def test(model, testset, device):
--------++++--@@ -63,14 +63,14 @@ def test(model, testset, device):
--------+++ --     return jiwer.wer(references, predictions)
--------+++ -- 
--------+++ -- 
--------+++----def train_model(trainset, devset, device, n_epochs=200):
--------+++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+++----
--------+++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------+++---+    #Define Dataloader
--------+++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------++++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
--------++++--     #Define Dataloader
--------++++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------++++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------+++ -- 
--------+++---+    #Define model and loss function
--------++++--     #Define model and loss function
--------+++ --     n_chars = len(devset.text_transform.chars)
--------+++---     model = Model(devset.num_features, n_chars+1).to(device)
--------+++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------++++---    model = Model(devset.num_features, n_chars+1).to(device)
--------++++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------+++ -- 
--------+++ --     if FLAGS.start_training_from is not None:
--------+++---         state_dict = torch.load(FLAGS.start_training_from)
--------+++---         model.load_state_dict(state_dict, strict=False)
--------++++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------++++--             target= y[:,1:]
--------+++ -- 
--------+++---+    #Define optimizer and scheduler for the learning rate
--------+++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
--------+++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
--------++++--             #Prediction
--------++++---            pred = model(X, X_raw, tgt, sess)
--------++++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
--------+++ -- 
--------+++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
--------+++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
--------++++--             #Primary Loss
--------++++---            pred=pred.permute(0,2,1)
--------++++---            loss = loss_fn(pred, target)
--------++++--+            out_dec=out_dec.permute(0,2,1)
--------++++--+            loss_dec = loss_fn(out_dec, target)
--------+++ -- 
--------+++---     batch_idx = 0
--------+++---+    train_loss= 0
--------+++---+    eval_loss = 0
--------+++---     optim.zero_grad()
--------+++---     for epoch_idx in range(n_epochs):
--------+++---+        model.train()
--------+++---         losses = []
--------+++----        for example in dataloader:
--------+++---+        for example in dataloader_training:
--------+++---             schedule_lr(batch_idx)
--------+++--- 
--------+++---+            #Preprosessing of the input and target for the model
--------+++---             X = combine_fixed_length(example['emg'], 200).to(device)
--------+++---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
--------+++---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+++--- 
--------+++---+            #Shifting target for input decoder and loss
--------+++---+            tgt= y[:,:-1]
--------+++---+            target= y[:,1:]
--------+++---+
--------+++---+            #Prediction
--------+++---             pred = model(X, X_raw, tgt, sess)
--------+++----            pred = F.log_softmax(pred, 2)
--------+++--- 
--------+++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+++---+            #Primary Loss
--------+++---+            pred=pred.permute(0,2,1)
--------+++---+            loss = loss_fn(pred, target)
--------++++--             #Auxiliary Loss
--------++++---            #pred = F.log_softmax(pred, 2)
--------++++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------++++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++++--+            out_enc = F.log_softmax(out_enc, 2)
--------++++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
--------++++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------++++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+++ --+
--------+++---+            #Auxiliary Loss
--------+++---+            #pred = F.log_softmax(pred, 2)
--------+++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------+++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------++++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
--------+++ --             losses.append(loss.item())
--------+++---+            train_loss += loss.item()
--------++++--             train_loss += loss.item()
--------+++ -- 
--------+++---             loss.backward()
--------++++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+++ --             if (batch_idx+1) % 2 == 0:
--------+++ --                 optim.step()
--------+++ --                 optim.zero_grad()
--------++++---
--------++++---            if batch_idx % report_every == report_every - 2:     
--------++++--+            
--------++++--+            if False:
--------++++--+            #if batch_idx % report_every == report_every - 2:     
--------++++--                 #Evaluation
--------++++--                 model.eval()
--------++++--                 with torch.no_grad():
--------++++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------++++---                        X_raw = example['raw_emg'].to(device)
--------++++---                        sess = example['session_ids'].to(device)
--------++++---                        y = example['text_int'].to(device)
--------++++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+++ -- 
--------+++---+            #Report plots in tensorboard
--------+++---+            if batch_idx % report_every == report_every - 2:     
--------+++---+                #Evaluation
--------+++---+                model.eval()
--------+++---+                with torch.no_grad():
--------+++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------+++---+                        X_raw = example['raw_emg'].to(device)
--------+++---+                        sess = example['session_ids'].to(device)
--------+++---+                        y = example['text_int'].to(device)
--------+++---+
--------+++---+                        #Shifting target for input decoder and loss
--------+++---+                        tgt= y[:,:-1]
--------+++---+                        target= y[:,1:]
--------+++---+
--------+++---+                        #Prediction without the 197-th batch because of missing label
--------+++---+                        if idx != 197:
--------+++---+                            pred = model(X, X_raw, tgt, sess)
--------+++---+                            #Primary Loss
--------+++---+                            pred=pred.permute(0,2,1)
--------+++---+                            loss = loss_fn(pred, target)
--------+++---+                            eval_loss += loss.item()
--------+++---+
--------+++---+                #Writing on tensorboard
--------+++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------+++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------+++---+                train_loss= 0
--------+++---+                eval_loss= 0
--------+++---+
--------+++---+            #Increment counter        
--------+++---             batch_idx += 1
--------+++----        train_loss = np.mean(losses)
--------+++---+
--------+++---+        #Testing and change learning rate
--------+++---         val = test(model, devset, device)
--------+++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
--------+++---         lr_sched.step()
--------+++---+    
--------+++---+        #Logging
--------+++---+        train_loss = np.mean(losses)
--------+++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
--------+++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
--------+++--- 
--------+++---@@ -148,8 +201,9 @@ def main():
--------+++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
--------+++--- 
--------+++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
--------+++---+    writer = SummaryWriter(log_dir="./content/runs")
--------++++--                         #Shifting target for input decoder and loss
--------++++--                         tgt= y[:,:-1]
--------++++--                         target= y[:,1:]
--------+++ -- 
--------+++----    model = train_model(trainset, devset, device)
--------+++---+    model = train_model(trainset, devset ,device, writer)
--------++++--+                        print(idx)
--------++++--+
--------++++--                         #Prediction without the 197-th batch because of missing label
--------++++---                        if idx != 197:
--------++++--+                        if idx != 181:
--------++++--                             pred = model(X, X_raw, tgt, sess)
--------++++--                             #Primary Loss
--------++++--                             pred=pred.permute(0,2,1)
--------++++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+++ -- 
--------+++--- if __name__ == '__main__':
--------+++---     FLAGS(sys.argv)
--------+++---diff --git a/transformer.py b/transformer.py
--------+++---index ac131be..51e1f2e 100644
--------+++------ a/transformer.py
--------+++---+++ b/transformer.py
--------+++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
--------+++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------+++---     self.d_qkv = d_qkv
--------++++--             #Increment counter        
--------++++--             batch_idx += 1
--------++++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--------+++ -- 
--------+++---+    #self.kdim = kdim if kdim is not None else embed_dim
--------+++---+    #self.vdim = vdim if vdim is not None else embed_dim
--------+++---+
--------+++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++--         #Testing and change learning rate
--------++++--         val = test(model, devset, device)
--------+++ --
--------+++ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++ --train / dev split: 8055 200
--------+++ -diff --git a/recognition_model.py b/recognition_model.py
--------+++--index fde5a40..6d5143b 100644
--------++++-index 30c5ff2..2672d45 100644
--------+++ ---- a/recognition_model.py
--------+++ -+++ b/recognition_model.py
--------+++--@@ -63,14 +63,14 @@ def test(model, testset, device):
--------+++--     return jiwer.wer(references, predictions)
--------+++-- 
--------+++-- 
--------+++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
--------+++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
--------+++--     #Define Dataloader
--------+++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
--------+++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--------++++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------+++ - 
--------+++ -     #Define model and loss function
--------+++ -     n_chars = len(devset.text_transform.chars)
--------+++---    model = Model(devset.num_features, n_chars+1).to(device)
--------+++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++++--    model = Model(devset.num_features, n_chars+1, True).to(device)
--------++++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
--------+++ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------+++ - 
--------+++ -     if FLAGS.start_training_from is not None:
--------+++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+++--             target= y[:,1:]
--------++++-diff --git a/transformer.py b/transformer.py
--------++++-index 51e1f2e..c125841 100644
--------++++---- a/transformer.py
--------++++-+++ b/transformer.py
--------++++-@@ -1,3 +1,4 @@
--------++++-+import math
--------++++- from typing import Optional
--------+++ - 
--------+++--             #Prediction
--------+++---            pred = model(X, X_raw, tgt, sess)
--------+++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
--------++++- import torch
--------++++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
--------++++-         Shape:
--------++++-             see the docs in Transformer class.
--------++++-         """
--------++++--        src2 = self.self_attn(src, src, src)
--------++++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
--------++++-         src = src + self.dropout1(src2)
--------++++-         src = self.norm1(src)
--------++++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------++++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
--------++++-         Shape:
--------++++-             see the docs in Transformer class.
--------++++-         """
--------++++--        tgt2 = self.self_attn(tgt, tgt, tgt)
--------++++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------++++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------++++-         tgt = tgt + self.dropout1(tgt2)
--------++++-         tgt = self.norm1(tgt)
--------+++ - 
--------+++--             #Primary Loss
--------+++---            pred=pred.permute(0,2,1)
--------+++---            loss = loss_fn(pred, target)
--------+++--+            out_dec=out_dec.permute(0,2,1)
--------+++--+            loss_dec = loss_fn(out_dec, target)
--------++++--        tgt2=self.multihead_attn(tgt, memory, memory)
--------++++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
--------++++-         tgt = tgt + self.dropout1(tgt2)
--------++++-         tgt = self.norm1(tgt)
--------+++ - 
--------+++--             #Auxiliary Loss
--------+++---            #pred = F.log_softmax(pred, 2)
--------+++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
--------+++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
--------+++--+            out_enc = F.log_softmax(out_enc, 2)
--------+++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
--------+++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
--------+++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
--------+++--+
--------+++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
--------+++--             losses.append(loss.item())
--------+++--             train_loss += loss.item()
--------++++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
--------++++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------++++-     self.d_qkv = d_qkv
--------+++ - 
--------+++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+++--             if (batch_idx+1) % 2 == 0:
--------+++--                 optim.step()
--------+++--                 optim.zero_grad()
--------++++--    #self.kdim = kdim if kdim is not None else embed_dim
--------++++--    #self.vdim = vdim if vdim is not None else embed_dim
--------+++ --
--------+++---            if batch_idx % report_every == report_every - 2:     
--------+++--+            
--------+++--+            if False:
--------+++--+            #if batch_idx % report_every == report_every - 2:     
--------+++--                 #Evaluation
--------+++--                 model.eval()
--------+++--                 with torch.no_grad():
--------+++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
--------+++---                        X_raw = example['raw_emg'].to(device)
--------+++---                        sess = example['session_ids'].to(device)
--------+++---                        y = example['text_int'].to(device)
--------+++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------++++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------++++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
--------++++-     else:
--------++++-         self.relative_positional = None
--------+++ - 
--------+++--                         #Shifting target for input decoder and loss
--------+++--                         tgt= y[:,:-1]
--------+++--                         target= y[:,1:]
--------++++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------++++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
--------++++-     """Runs the multi-head self-attention layer.
--------+++ - 
--------+++--+                        print(idx)
--------+++--+
--------+++--                         #Prediction without the 197-th batch because of missing label
--------+++---                        if idx != 197:
--------+++--+                        if idx != 181:
--------+++--                             pred = model(X, X_raw, tgt, sess)
--------+++--                             #Primary Loss
--------+++--                             pred=pred.permute(0,2,1)
--------+++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--------+++-- 
--------+++--             #Increment counter        
--------+++--             batch_idx += 1
--------+++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--------++++-     Args:
--------++++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
--------++++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------++++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+++ - 
--------+++--         #Testing and change learning rate
--------+++--         val = test(model, devset, device)
--------++++-+    if attn_mask is not None:
--------++++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++++-+
--------++++-     if self.relative_positional is not None:
--------++++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--------++++-         l,b,h,d = q_pos.size()
--------++++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
--------++++-             x = x.transpose(0, 1)
--------++++-             x = x.contiguous().view(bsz_heads, length+1, length)
--------++++-             return x[:, 1:, :]
--------++++-+        
--------++++-+
--------++++-+########
--------++++-+# Taken from:
--------++++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
--------++++-+# or also here:
--------++++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
--------++++-+class PositionalEncoding(nn.Module):
--------++++-+
--------++++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
--------++++-+        super(PositionalEncoding, self).__init__()
--------++++-+        self.dropout = nn.Dropout(p=dropout)
--------++++-+        self.max_len = max_len
--------++++-+
--------++++-+        pe = torch.zeros(max_len, d_model)
--------++++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
--------++++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
--------++++-+                             * (-math.log(10000.0) / d_model))
--------++++-+        pe[:, 0::2] = torch.sin(position * div_term)
--------++++-+        pe[:, 1::2] = torch.cos(position * div_term)
--------++++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
--------++++-+        self.register_buffer('pe', pe)  # Will not be trained.
--------++++-+
--------++++-+    def forward(self, x):
--------++++-+        """Inputs of forward function
--------++++-+        Args:
--------++++-+            x: the sequence fed to the positional encoder model (required).
--------++++-+        Shape:
--------++++-+            x: [sequence length, batch size, embed dim]
--------++++-+            output: [sequence length, batch size, embed dim]
--------++++-+        """
--------++++-+        assert x.size(0) < self.max_len, (
--------++++-+            f"Too long sequence length: increase `max_len` of pos encoding")
--------++++-+        # shape of x (len, B, dim)
--------++++-+        x = x + self.pe[:x.size(0), :]
--------++++-+        return self.dropout(x)
--------+++ -
--------+++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++ -train / dev split: 8055 200
--------+++ diff --git a/recognition_model.py b/recognition_model.py
--------+++-index 30c5ff2..2672d45 100644
--------++++index 2672d45..d268f26 100644
--------+++ --- a/recognition_model.py
--------+++ +++ b/recognition_model.py
--------+++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------+++- 
--------+++-     #Define model and loss function
--------+++-     n_chars = len(devset.text_transform.chars)
--------+++--    model = Model(devset.num_features, n_chars+1, True).to(device)
--------+++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
--------+++-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------++++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------++++             X = combine_fixed_length(example['emg'], 200).to(device)
--------++++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+++++            y = combine_fixed_length(example['text_int'], 200).to(device)
--------+++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+++  
--------+++-     if FLAGS.start_training_from is not None:
--------++++             #Shifting target for input decoder and loss
--------++++             tgt= y[:,:-1]
--------+++ diff --git a/transformer.py b/transformer.py
--------+++-index 51e1f2e..c125841 100644
--------++++index c125841..73d805b 100644
--------+++ --- a/transformer.py
--------+++ +++ b/transformer.py
--------+++-@@ -1,3 +1,4 @@
--------+++-+import math
--------+++- from typing import Optional
--------+++- 
--------+++- import torch
--------+++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
--------+++-         Shape:
--------+++-             see the docs in Transformer class.
--------+++-         """
--------+++--        src2 = self.self_attn(src, src, src)
--------+++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
--------+++-         src = src + self.dropout1(src2)
--------+++-         src = self.norm1(src)
--------+++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------+++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
--------++++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
--------+++          Shape:
--------+++              see the docs in Transformer class.
--------+++          """
--------+++--        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------+++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------++++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------++++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------+++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------+++++        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+++          tgt = tgt + self.dropout1(tgt2)
--------+++          tgt = self.norm1(tgt)
--------+++  
--------+++--        tgt2=self.multihead_attn(tgt, memory, memory)
--------+++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
--------+++-         tgt = tgt + self.dropout1(tgt2)
--------+++-         tgt = self.norm1(tgt)
--------+++- 
--------+++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
--------+++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------+++-     self.d_qkv = d_qkv
--------+++- 
--------+++--    #self.kdim = kdim if kdim is not None else embed_dim
--------+++--    #self.vdim = vdim if vdim is not None else embed_dim
--------+++--
--------+++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
--------+++-     else:
--------+++-         self.relative_positional = None
--------+++- 
--------+++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------+++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
--------+++-     """Runs the multi-head self-attention layer.
--------+++- 
--------+++-     Args:
--------+++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
--------++++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
--------+++      v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------+++      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+++  
--------+++-+    if attn_mask is not None:
--------+++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+++-+
--------++++-    if attn_mask is not None:
--------++++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+++++   # if attn_mask is not None:
--------+++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++++ 
--------+++      if self.relative_positional is not None:
--------+++          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--------+++-         l,b,h,d = q_pos.size()
--------+++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
--------+++-             x = x.transpose(0, 1)
--------+++-             x = x.contiguous().view(bsz_heads, length+1, length)
--------+++-             return x[:, 1:, :]
--------+++-+        
--------+++-+
--------+++-+########
--------+++-+# Taken from:
--------+++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
--------+++-+# or also here:
--------+++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
--------+++-+class PositionalEncoding(nn.Module):
--------+++-+
--------+++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
--------+++-+        super(PositionalEncoding, self).__init__()
--------+++-+        self.dropout = nn.Dropout(p=dropout)
--------+++-+        self.max_len = max_len
--------+++-+
--------+++-+        pe = torch.zeros(max_len, d_model)
--------+++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
--------+++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
--------+++-+                             * (-math.log(10000.0) / d_model))
--------+++-+        pe[:, 0::2] = torch.sin(position * div_term)
--------+++-+        pe[:, 1::2] = torch.cos(position * div_term)
--------+++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
--------+++-+        self.register_buffer('pe', pe)  # Will not be trained.
--------+++-+
--------+++-+    def forward(self, x):
--------+++-+        """Inputs of forward function
--------+++-+        Args:
--------+++-+            x: the sequence fed to the positional encoder model (required).
--------+++-+        Shape:
--------+++-+            x: [sequence length, batch size, embed dim]
--------+++-+            output: [sequence length, batch size, embed dim]
--------+++-+        """
--------+++-+        assert x.size(0) < self.max_len, (
--------+++-+            f"Too long sequence length: increase `max_len` of pos encoding")
--------+++-+        # shape of x (len, B, dim)
--------+++-+        x = x + self.pe[:x.size(0), :]
--------+++-+        return self.dropout(x)
--------+++ 
--------+++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+++ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------++ diff --git a/output/log.txt b/output/log.txt
--------++-index ae42364..8563980 100644
--------+++index 1d2cd8e..342fccd 100644
--------++ --- a/output/log.txt
--------++ +++ b/output/log.txt
--------++-@@ -1,3 +1,2 @@
--------++-+57f8139449dd9286c2203ec2eca118a550638a7c
--------+++@@ -1,13 +1,2 @@
--------+++-57f8139449dd9286c2203ec2eca118a550638a7c
--------++++be71135adc89793578f304adb405cea80a5b2b9a
--------++  
--------+++-diff --git a/output/log.txt b/output/log.txt
--------+++-index ae42364..8563980 100644
--------+++---- a/output/log.txt
--------+++-+++ b/output/log.txt
--------+++-@@ -1,3 +1,2 @@
--------+++-+57f8139449dd9286c2203ec2eca118a550638a7c
--------+++- 
--------+++--
--------+++--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------++ -
--------++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+++diff --git a/recognition_model.py b/recognition_model.py
--------+++index 2672d45..d268f26 100644
--------+++--- a/recognition_model.py
--------++++++ b/recognition_model.py
--------+++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------+++             X = combine_fixed_length(example['emg'], 200).to(device)
--------+++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------++++            y = combine_fixed_length(example['text_int'], 200).to(device)
--------++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+++ 
--------+++             #Shifting target for input decoder and loss
--------+++             tgt= y[:,:-1]
--------+++diff --git a/transformer.py b/transformer.py
--------+++index c125841..73d805b 100644
--------+++--- a/transformer.py
--------++++++ b/transformer.py
--------+++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
--------+++         Shape:
--------+++             see the docs in Transformer class.
--------+++         """
--------+++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------+++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------++++        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+++         tgt = tgt + self.dropout1(tgt2)
--------+++         tgt = self.norm1(tgt)
--------+++ 
--------+++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
--------+++     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------+++     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+++ 
--------+++-    if attn_mask is not None:
--------+++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++++   # if attn_mask is not None:
--------++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+++ 
--------+++     if self.relative_positional is not None:
--------+++         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--------++ 
--------++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+++output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+++train / dev split: 8055 200
--------+ diff --git a/recognition_model.py b/recognition_model.py
--------+-index 30c5ff2..2672d45 100644
--------++index 2672d45..517c9a3 100644
--------+ --- a/recognition_model.py
--------+ +++ b/recognition_model.py
--------+-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------++@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------++             X = combine_fixed_length(example['emg'], 200).to(device)
--------++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------+++            y = combine_fixed_length(example['text_int'], 200).to(device)
--------++             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+  
--------+-     #Define model and loss function
--------+-     n_chars = len(devset.text_transform.chars)
--------+--    model = Model(devset.num_features, n_chars+1, True).to(device)
--------+-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
--------+-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--------+- 
--------+-     if FLAGS.start_training_from is not None:
--------++             #Shifting target for input decoder and loss
--------+ diff --git a/transformer.py b/transformer.py
--------+-index 51e1f2e..c125841 100644
--------++index c125841..06e870b 100644
--------+ --- a/transformer.py
--------+ +++ b/transformer.py
--------+-@@ -1,3 +1,4 @@
--------+-+import math
--------+- from typing import Optional
--------+- 
--------+- import torch
--------+-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
--------+-         Shape:
--------+-             see the docs in Transformer class.
--------+-         """
--------+--        src2 = self.self_attn(src, src, src)
--------+-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
--------+-         src = src + self.dropout1(src2)
--------+-         src = self.norm1(src)
--------+-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--------+-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
--------+-         Shape:
--------+-             see the docs in Transformer class.
--------+-         """
--------+--        tgt2 = self.self_attn(tgt, tgt, tgt)
--------+-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--------+-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--------+-         tgt = tgt + self.dropout1(tgt2)
--------+-         tgt = self.norm1(tgt)
--------+- 
--------+--        tgt2=self.multihead_attn(tgt, memory, memory)
--------+-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
--------+-         tgt = tgt + self.dropout1(tgt2)
--------+-         tgt = self.norm1(tgt)
--------+- 
--------+-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
--------+-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--------+-     self.d_qkv = d_qkv
--------+- 
--------+--    #self.kdim = kdim if kdim is not None else embed_dim
--------+--    #self.vdim = vdim if vdim is not None else embed_dim
--------+--
--------+-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--------+-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
--------+-     else:
--------+-         self.relative_positional = None
--------+- 
--------+--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--------+-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
--------+-     """Runs the multi-head self-attention layer.
--------+- 
--------+-     Args:
--------+-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
--------+-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--------++@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
--------+      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+  
--------+-+    if attn_mask is not None:
--------+-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+-+
--------++     if attn_mask is not None:
--------++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++ 
--------+      if self.relative_positional is not None:
--------+          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--------+-         l,b,h,d = q_pos.size()
--------+-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
--------+-             x = x.transpose(0, 1)
--------+-             x = x.contiguous().view(bsz_heads, length+1, length)
--------+-             return x[:, 1:, :]
--------+-+        
--------+-+
--------+-+########
--------+-+# Taken from:
--------+-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
--------+-+# or also here:
--------+-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
--------+-+class PositionalEncoding(nn.Module):
--------+-+
--------+-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
--------+-+        super(PositionalEncoding, self).__init__()
--------+-+        self.dropout = nn.Dropout(p=dropout)
--------+-+        self.max_len = max_len
--------+-+
--------+-+        pe = torch.zeros(max_len, d_model)
--------+-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
--------+-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
--------+-+                             * (-math.log(10000.0) / d_model))
--------+-+        pe[:, 0::2] = torch.sin(position * div_term)
--------+-+        pe[:, 1::2] = torch.cos(position * div_term)
--------+-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
--------+-+        self.register_buffer('pe', pe)  # Will not be trained.
--------+-+
--------+-+    def forward(self, x):
--------+-+        """Inputs of forward function
--------+-+        Args:
--------+-+            x: the sequence fed to the positional encoder model (required).
--------+-+        Shape:
--------+-+            x: [sequence length, batch size, embed dim]
--------+-+            output: [sequence length, batch size, embed dim]
--------+-+        """
--------+-+        assert x.size(0) < self.max_len, (
--------+-+            f"Too long sequence length: increase `max_len` of pos encoding")
--------+-+        # shape of x (len, B, dim)
--------+-+        x = x + self.pe[:x.size(0), :]
--------+-+        return self.dropout(x)
--------+ 
--------+ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------+ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-------- diff --git a/output/log.txt b/output/log.txt
---------index ae42364..8563980 100644
--------+index 1d2cd8e..342fccd 100644
-------- --- a/output/log.txt
-------- +++ b/output/log.txt
---------@@ -1,3 +1,2 @@
---------+57f8139449dd9286c2203ec2eca118a550638a7c
--------+@@ -1,13 +1,2 @@
--------+-57f8139449dd9286c2203ec2eca118a550638a7c
--------++be71135adc89793578f304adb405cea80a5b2b9a
--------  
--------+-diff --git a/output/log.txt b/output/log.txt
--------+-index ae42364..8563980 100644
--------+---- a/output/log.txt
--------+-+++ b/output/log.txt
--------+-@@ -1,3 +1,2 @@
--------+-+57f8139449dd9286c2203ec2eca118a550638a7c
--------+- 
--------+--
--------+--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-------- -
-------- -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+diff --git a/recognition_model.py b/recognition_model.py
--------+index 2672d45..517c9a3 100644
--------+--- a/recognition_model.py
--------++++ b/recognition_model.py
--------+@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------+             X = combine_fixed_length(example['emg'], 200).to(device)
--------+             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--------+             sess = combine_fixed_length(example['session_ids'], 200).to(device)
--------++            y = combine_fixed_length(example['text_int'], 200).to(device)
--------+             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--------+ 
--------+             #Shifting target for input decoder and loss
--------+diff --git a/transformer.py b/transformer.py
--------+index c125841..06e870b 100644
--------+--- a/transformer.py
--------++++ b/transformer.py
--------+@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
--------+     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
--------+ 
--------+     if attn_mask is not None:
--------+-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------+-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--------++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--------+ 
--------+     if self.relative_positional is not None:
--------+         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-------- 
-------- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
--------+output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------+train / dev split: 8055 200
--------diff --git a/recognition_model.py b/recognition_model.py
--------index 62184a7..fdef7e3 100644
----------- a/recognition_model.py
--------+++ b/recognition_model.py
--------@@ -123,7 +123,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
--------             #Encoder Loss
--------             out_enc = F.log_softmax(out_enc, 2)
--------             out_enc = out_enc.transpose(1,0)
---------            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)+1) 
--------+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)) 
-------- 
--------             #Combination the two losses
--------             loss = (1 - alpha) * loss_dec + alpha * loss_enc
--------
--------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--------train / dev split: 8055 200
-------diff --git a/recognition_model.py b/recognition_model.py
-------index fdef7e3..258479a 100644
---------- a/recognition_model.py
-------+++ b/recognition_model.py
-------@@ -3,7 +3,7 @@ import sys
------- import numpy as np
------- import logging
------- import subprocess
--------from ctcdecode import CTCBeamDecoder, DecoderState
-------+from ctcdecode import OnlineCTCBeamDecoder, DecoderState
------- import jiwer
------- import random
------- from torch.utils.tensorboard import SummaryWriter
-------@@ -21,18 +21,19 @@ FLAGS = flags.FLAGS
------- flags.DEFINE_boolean('debug', False, 'debug')
------- flags.DEFINE_string('output_directory', 'output', 'where to save models and outputs')
------- flags.DEFINE_integer('batch_size', 32, 'training batch size')
--------flags.DEFINE_float('learning_rate', 3e-4, 'learning rate')
-------+flags.DEFINE_float('learning_rate', 3e-12, 'learning rate')
------- flags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')
------- flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')
------- flags.DEFINE_string('start_training_from', None, 'start training from this model')
------- flags.DEFINE_float('l2', 0, 'weight decay')
-------+flags.DEFINE_float('alpha_loss', 0.65, 'parameter alpha for the two losses')
------- flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')
------- 
------- def test(model, testset, device):
-------     model.eval()
------- 
--------    blank_id = 1
--------    decoder = CTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
-------+    blank_id = len(testset.text_transform.chars)
-------+    decoder = OnlineCTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,
-------             model_path='lm.binary', alpha=1.5, beta=1.85)
-------     state = DecoderState(decoder)
-------     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
-------@@ -46,10 +47,11 @@ def test(model, testset, device):
------- 
-------             #Prediction without the 197-th batch because of missing label
-------             if batch_idx != 181:
-------+                tgt=tgt[:1,:1]
-------                 out_enc, out_dec = model(X_raw, tgt)
-------                 pred  = F.log_softmax(out_dec, -1)
------- 
--------                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
-------+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred, [state], [True])
-------                 pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
------- 
-------                 pred_text = testset.text_transform.int_to_text(pred_int)
-------@@ -67,10 +69,10 @@ def test(model, testset, device):
-------     return jiwer.wer(references, predictions)
------- 
------- 
--------def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-------+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
-------     #Define Dataloader
--------    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_size=2)
--------    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-------+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, shuffle= True ,collate_fn=EMGDataset.collate_raw, batch_size=2)
-------+    dataloader_evaluation = torch.utils.data.DataLoader(devset, collate_fn=EMGDataset.collate_raw, batch_size=1)
------- 
-------     #Define model and loss function
-------     n_chars = len(devset.text_transform.chars)
-------@@ -85,6 +87,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-------     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-------     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
------- 
-------+
-------     def set_lr(new_lr):
-------         for param_group in optim.param_groups:
-------             param_group['lr'] = new_lr
-------@@ -98,13 +101,14 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-------     batch_idx = 0
-------     train_loss= 0
-------     eval_loss = 0
-------+    run_step_eval=0
-------     optim.zero_grad()
-------     for epoch_idx in range(n_epochs):
-------         model.train()
-------         losses = []
-------         for example in dataloader_training:
-------             schedule_lr(batch_idx)
--------
-------+            
-------             #Preprosessing of the input and target for the model
-------             X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
-------             y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-------@@ -126,7 +130,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-------             loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)) 
------- 
-------             #Combination the two losses
--------            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-------+            loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
-------             losses.append(loss.item())
-------             train_loss += loss.item()
------- 
-------@@ -139,9 +143,11 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-------             
-------             #Increment counter and print the loss training       
-------             batch_idx += 1
--------            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-------+            writer.add_scalar('Loss/Training', train_loss, batch_idx)
-------             train_loss= 0
------- 
-------+            #Debug
-------+            #val = test(model, devset, device)
------- 
-------             if batch_idx % report_every == 0:     
-------                 #Evaluation
-------@@ -159,20 +165,30 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-------                         out_enc, out_dec = model(X_raw, tgt)
-------                         #Decoder Loss
-------                         out_dec=out_dec.permute(0,2,1)
--------                        loss = loss_fn(out_dec, target)
-------+                        loss_dec = loss_fn(out_dec, target)
-------+
-------+                        #Encoder Loss
-------+                        out_enc = F.log_softmax(out_enc, 2)
-------+                        out_enc = out_enc.transpose(1,0)
-------+                        loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)) 
-------+
-------+                        #Combination the two losses
-------+                        loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
-------                         eval_loss += loss.item()
-------+                        run_step_eval += 1
-------                         
-------                         #just for now
-------                         if idx == 10:
-------                             break
------- 
-------                 #Writing on tensorboard
--------                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-------+                writer.add_scalar('Loss/Evaluation', eval_loss/ run_step_eval, batch_idx)
-------                 eval_loss= 0
-------+                run_step_eval=0
------- 
--------                        print(idx)
--------
-------                         #Prediction without the 197-th batch because of missing label
--------                        if idx != 181:
--------                            pred = model(X, X_raw, tgt, sess)
--------                            #Primary Loss
--------                            pred=pred.permute(0,2,1)
--------                            loss = loss_fn(pred, target)
--------                            eval_loss += loss.item()
-------+                        out_enc, out_dec = model(X_raw, tgt)
-------+                        #Decoder Loss
-------+                        out_dec=out_dec.permute(0,2,1)
-------+                        loss = loss_fn(out_dec, target)
-------+                        eval_loss += loss.item()
-------+                        
-------+                        #just for now
-------+                        if idx == 10:
-------+                            break
------- 
-------                 #Writing on tensorboard
-------                 writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
--------                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
--------                train_loss= 0
-------                 eval_loss= 0
------- 
--------            #Increment counter        
--------            batch_idx += 1
--------            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--------            val = test(model, devset, device)
--------
-------         #Testing and change learning rate
--------        val = test(model, devset, device)
--------        writer.add_scalar('WER/Evaluation',val, batch_idx)
-------+        #val = test(model, devset, device)
-------+        #writer.add_scalar('WER/Evaluation',val, batch_idx)
-------         lr_sched.step()
-------     
-------         #Logging
-------diff --git a/transformer.py b/transformer.py
-------index 47a7ec7..899334e 100644
---------- a/transformer.py
-------+++ b/transformer.py
-------@@ -170,6 +170,7 @@ class MultiHeadAttention(nn.Module):
-------     Returns:
-------       A single tensor containing the output from this layer
-------     """
-------+
-------     #Computes projections
-------     q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-------     k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-------
-------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-------train / dev split: 8055 200
------diff --git a/recognition_model.py b/recognition_model.py
------index 258479a..faea23d 100644
--------- a/recognition_model.py
------+++ b/recognition_model.py
------@@ -21,12 +21,13 @@ FLAGS = flags.FLAGS
------ flags.DEFINE_boolean('debug', False, 'debug')
------ flags.DEFINE_string('output_directory', 'output', 'where to save models and outputs')
------ flags.DEFINE_integer('batch_size', 32, 'training batch size')
-------flags.DEFINE_float('learning_rate', 3e-12, 'learning rate')
------+flags.DEFINE_float('learning_rate', 3e-5, 'learning rate')
------ flags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')
------ flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')
------ flags.DEFINE_string('start_training_from', None, 'start training from this model')
------ flags.DEFINE_float('l2', 0, 'weight decay')
-------flags.DEFINE_float('alpha_loss', 0.65, 'parameter alpha for the two losses')
------+flags.DEFINE_float('alpha_loss', 0.8, 'parameter alpha for the two losses')
------+flags.DEFINE_float('report_every', 10, "Reporting parameter of the loss plot")
------ flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')
------ 
------ def test(model, testset, device):
------@@ -69,7 +70,7 @@ def test(model, testset, device):
------     return jiwer.wer(references, predictions)
------ 
------ 
-------def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
------+def train_model(trainset, devset, device, writer, n_epochs=200):
------     #Define Dataloader
------     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, shuffle= True ,collate_fn=EMGDataset.collate_raw, batch_size=2)
------     dataloader_evaluation = torch.utils.data.DataLoader(devset, collate_fn=EMGDataset.collate_raw, batch_size=1)
------@@ -101,7 +102,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
------     batch_idx = 0
------     train_loss= 0
------     eval_loss = 0
-------    run_step_eval=0
------+    run_steps=0
------     optim.zero_grad()
------     for epoch_idx in range(n_epochs):
------         model.train()
------@@ -136,20 +137,17 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
------ 
------             #Gradient Update
------             loss.backward()
-------            if (batch_idx+1) % 2 == 0:
------+            if (batch_idx+1) % 30 == 0:
------                 optim.step()
------                 optim.zero_grad()
-------            
-------            
-------            #Increment counter and print the loss training       
-------            batch_idx += 1
-------            writer.add_scalar('Loss/Training', train_loss, batch_idx)
-------            train_loss= 0
------ 
------             #Debug
------             #val = test(model, devset, device)
------ 
-------            if batch_idx % report_every == 0:     
------+            #Increment counter and print the loss training       
------+            batch_idx += 1
------+
------+            if batch_idx % FLAGS.report_every == FLAGS.report_every - 2:     
------                 #Evaluation
------                 model.eval()
------                 with torch.no_grad():
------@@ -163,8 +161,10 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
------ 
------                         #Prediction without the 197-th batch because of missing label
------                         out_enc, out_dec = model(X_raw, tgt)
------+
------                         #Decoder Loss
------                         out_dec=out_dec.permute(0,2,1)
------+                        loss = loss_fn(out_dec, target)
------                         loss_dec = loss_fn(out_dec, target)
------ 
------                         #Encoder Loss
------@@ -175,16 +175,18 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1):
------                         #Combination the two losses
------                         loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
------                         eval_loss += loss.item()
-------                        run_step_eval += 1
------+                        run_steps += 1
------                         
------                         #just for now
------                         if idx == 10:
------                             break
------ 
------                 #Writing on tensorboard
-------                writer.add_scalar('Loss/Evaluation', eval_loss/ run_step_eval, batch_idx)
------+                writer.add_scalar('Loss/Evaluation', eval_loss / run_steps, batch_idx)
------+                writer.add_scalar('Loss/Training', train_loss / run_steps, batch_idx)
------+                train_loss= 0
------                 eval_loss= 0
-------                run_step_eval=0
------+                run_steps=0
------ 
------         #Testing and change learning rate
------         #val = test(model, devset, device)
------diff --git a/transformer.py b/transformer.py
------index 899334e..aee089e 100644
--------- a/transformer.py
------+++ b/transformer.py
------@@ -181,14 +181,12 @@ class MultiHeadAttention(nn.Module):
------ 
------     # Apply att_mask to the attention weights if provided
------     if attn_mask is not None:
-------        attn_mask=attn_mask.unsqueeze(0)
-------        attn_mask=attn_mask.unsqueeze(1)
-------        logits = logits.masked_fill(attn_mask == float('-inf'), float('-inf'))
------+        logits = logits.masked_fill(attn_mask == float('-inf'), -1e20)
------     
------     
-------    # Apply padding_mask to the attention weights if provided
------+    #Apply padding_mask to the attention weights if provided
------     if key_padding_mask is not None:
-------        logits = logits.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
------+       logits = logits.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(3), -1e20)
------     
------     
------     if self.relative_positional is not None:
------
------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
------train / dev split: 8055 200
-----diff --git a/recognition_model.py b/recognition_model.py
-----index faea23d..24c4207 100644
-------- a/recognition_model.py
-----+++ b/recognition_model.py
-----@@ -3,7 +3,6 @@ import sys
----- import numpy as np
----- import logging
----- import subprocess
------from ctcdecode import OnlineCTCBeamDecoder, DecoderState
----- import jiwer
----- import random
----- from torch.utils.tensorboard import SummaryWriter
-----@@ -31,44 +30,7 @@ flags.DEFINE_float('report_every', 10, "Reporting parameter of the loss plot")
----- flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')
----- 
----- def test(model, testset, device):
------    model.eval()
------
------    blank_id = len(testset.text_transform.chars)
------    decoder = OnlineCTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,
------            model_path='lm.binary', alpha=1.5, beta=1.85)
------    state = DecoderState(decoder)
------    dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
------    references = []
------    predictions = []
------    batch_idx = 0
------    with torch.no_grad():
------        for example in dataloader:
------            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
------            tgt = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
------
------            #Prediction without the 197-th batch because of missing label
------            if batch_idx != 181:
------                tgt=tgt[:1,:1]
------                out_enc, out_dec = model(X_raw, tgt)
------                pred  = F.log_softmax(out_dec, -1)
------
------                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred, [state], [True])
------                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
------
------                pred_text = testset.text_transform.int_to_text(pred_int)
------                target_text = testset.text_transform.clean_text(example['text'][0])
------
------                references.append(target_text)
------                predictions.append(pred_text)
------
------        batch_idx += 1
------        
------    model.train()
------    #remove empty strings because I had an error in the calculation of WER function
------    predictions = [predictions[i] for i in range(len(predictions)) if len(references[i]) > 0]
------    references = [references[i] for i in range(len(references)) if len(references[i]) > 0]
------    return jiwer.wer(references, predictions)
------
-----+    return
----- 
----- def train_model(trainset, devset, device, writer, n_epochs=200):
-----     #Define Dataloader
-----@@ -77,7 +39,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
----- 
-----     #Define model and loss function
-----     n_chars = len(devset.text_transform.chars)
------    model = Model(devset.num_features, n_chars + 1, device, True).to(device)
-----+    model = Model(devset.num_features, n_chars + 1, n_chars, device, True).to(device)
-----     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
----- 
-----     if FLAGS.start_training_from is not None:
-----@@ -195,7 +157,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
-----     
-----         #Logging
-----         train_loss = np.mean(losses)
------        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-----+        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f}')
-----         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
----- 
-----     model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters
-----diff --git a/transformer.py b/transformer.py
-----index aee089e..08fe298 100644
-------- a/transformer.py
-----+++ b/transformer.py
-----@@ -81,13 +81,7 @@ class TransformerDecoderLayer(nn.Module):
-----         norm_first: if ``True``, layer norm is done prior to self attention, multihead
-----             attention and feedforward operations, respectively. Otherwise it's done after.
-----             Default: ``False`` (after).
------
------    Examples::
------        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
------        >>> memory = torch.rand(10, 32, 512)
------        >>> tgt = torch.rand(20, 32, 512)
------        >>> out = decoder_layer(tgt, memory)
------    """
-----+        """
-----     # Adapted from pytorch source
-----     def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-----         super(TransformerDecoderLayer, self).__init__()
-----
-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----train / dev split: 8055 200
----diff --git a/recognition_model.py b/recognition_model.py
----index 24c4207..587f715 100644
------- a/recognition_model.py
----+++ b/recognition_model.py
----@@ -13,7 +13,7 @@ import torch.nn.functional as F
---- 
---- from read_emg import EMGDataset, SizeAwareSampler
---- from architecture import Model
-----from data_utils import combine_fixed_length, decollate_tensor
----+from data_utils import combine_fixed_length
---- 
---- from absl import flags
---- FLAGS = flags.FLAGS
----@@ -73,7 +73,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
----             schedule_lr(batch_idx)
----             
----             #Preprosessing of the input and target for the model
-----            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----+            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
----             y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---- 
----             #Shifting target for input decoder and loss
----@@ -81,7 +81,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
----             target= y[:,1:]
---- 
----             #Prediction
-----            out_enc, out_dec = model(X_raw, tgt)
----+            out_enc, out_dec = model(X_raw, tgt, example['lengths'])
---- 
----             #Decoder Loss
----             out_dec=out_dec.permute(0,2,1)
----@@ -114,7 +114,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
----                 model.eval()
----                 with torch.no_grad():
----                     for idx, example in enumerate(dataloader_evaluation):
-----                        X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
----                         y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----                     
----                         #Shifting target for input decoder and loss
----@@ -122,7 +122,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200):
----                         target= y[:,1:]
---- 
----                         #Prediction without the 197-th batch because of missing label
-----                        out_enc, out_dec = model(X_raw, tgt)
----+                        out_enc, out_dec = model(X_raw, tgt, example['lengths'])
---- 
----                         #Decoder Loss
----                         out_dec=out_dec.permute(0,2,1)
----diff --git a/transformer.py b/transformer.py
----index 08fe298..817ccaf 100644
------- a/transformer.py
----+++ b/transformer.py
----@@ -52,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
----         Shape:
----             see the docs in Transformer class.
----         """
-----        src2 = self.self_attn(src, src, src)
----+        src2 = self.self_attn(src, src, src, src_key_padding_mask=src_key_padding_mask)
----         src = src + self.dropout1(src2)
----         src = self.norm1(src)
----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
----@@ -117,11 +117,11 @@ class TransformerDecoderLayer(nn.Module):
----         Shape:
----             see the docs in Transformer class.
----         """
-----        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_key_padding_mask, tgt_mask)
----+        tgt2 = self.self_attn(tgt, tgt, tgt,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----         tgt = tgt + self.dropout1(tgt2)
----         tgt = self.norm1(tgt)
---- 
-----        tgt2=self.multihead_attn(tgt, memory, memory)
----+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask=memory_key_padding_mask)
----         tgt = tgt + self.dropout1(tgt2)
----         tgt = self.norm1(tgt)
---- 
----@@ -156,7 +156,7 @@ class MultiHeadAttention(nn.Module):
----     else:
----         self.relative_positional = None
---- 
-----  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None, attn_mask: Optional[torch.Tensor] = None):
----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, tgt_key_padding_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None):
----     """Runs the multi-head self-attention layer.
---- 
----     Args:
----@@ -174,13 +174,21 @@ class MultiHeadAttention(nn.Module):
----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
---- 
----     # Apply att_mask to the attention weights if provided
-----    if attn_mask is not None:
-----        logits = logits.masked_fill(attn_mask == float('-inf'), -1e20)
----+    if tgt_mask is not None:
----+        logits = logits.masked_fill(tgt_mask == float('-inf'), -1e8)
----     
----     
----     #Apply padding_mask to the attention weights if provided
-----    if key_padding_mask is not None:
-----       logits = logits.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(3), -1e20)
----+    if tgt_key_padding_mask is not None:
----+       logits = logits.masked_fill(tgt_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
----+
----+    #Apply padding_mask to the attention weights if provided
----+    if src_key_padding_mask is not None:
----+       logits = logits.masked_fill(src_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
----+
----+    #Apply padding_mask to the attention weights if provided
----+    if memory_key_padding_mask is not None:
----+       logits = logits.masked_fill(memory_key_padding_mask.unsqueeze(1).unsqueeze(2), -1e8)
----     
----     
----     if self.relative_positional is not None:
----
----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----train / dev split: 8055 200
---diff --git a/read_emg.py b/read_emg.py
---deleted file mode 100644
---index 27787d5..0000000
------ a/read_emg.py
---+++ /dev/null
---@@ -1,322 +0,0 @@
----import re
----import os
----import numpy as np
----import matplotlib.pyplot as plt
----import random
----from collections import defaultdict
----import scipy
----import json
----import copy
----import sys
----import pickle
----import string
----import logging
----from functools import lru_cache
----from copy import copy
----
----import librosa
----import soundfile as sf
----
----import torch
----
----from data_utils import load_audio, get_emg_features, FeatureNormalizer, phoneme_inventory, read_phonemes, TextTransform
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_list('remove_channels', [], 'channels to remove')
----flags.DEFINE_list('silent_data_directories', ['./emg_data/silent_parallel_data'], 'silent data locations')
----flags.DEFINE_list('voiced_data_directories', ['./emg_data/voiced_parallel_data','./emg_data/nonparallel_data'], 'voiced data locations')
----flags.DEFINE_string('testset_file', 'testset_largedev.json', 'file with testset indices')
----flags.DEFINE_string('text_align_directory', 'text_alignments', 'directory with alignment files')
----
----def remove_drift(signal, fs):
----    b, a = scipy.signal.butter(3, 2, 'highpass', fs=fs)
----    return scipy.signal.filtfilt(b, a, signal)
----
----def notch(signal, freq, sample_frequency):
----    b, a = scipy.signal.iirnotch(freq, 30, sample_frequency)
----    return scipy.signal.filtfilt(b, a, signal)
----
----def notch_harmonics(signal, freq, sample_frequency):
----    for harmonic in range(1,8):
----        signal = notch(signal, freq*harmonic, sample_frequency)
----    return signal
----
----def subsample(signal, new_freq, old_freq):
----    times = np.arange(len(signal))/old_freq
----    sample_times = np.arange(0, times[-1], 1/new_freq)
----    result = np.interp(sample_times, times, signal)
----    return result
----
----def apply_to_all(function, signal_array, *args, **kwargs):
----    results = []
----    for i in range(signal_array.shape[1]):
----        results.append(function(signal_array[:,i], *args, **kwargs))
----    return np.stack(results, 1)
----
----def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):
----    index = int(index)
----    raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))
----    before = os.path.join(base_dir, f'{index-1}_emg.npy')
----    after = os.path.join(base_dir, f'{index+1}_emg.npy')
----    if os.path.exists(before):
----        raw_emg_before = np.load(before)
----    else:
----        raw_emg_before = np.zeros([0,raw_emg.shape[1]])
----    if os.path.exists(after):
----        raw_emg_after = np.load(after)
----    else:
----        raw_emg_after = np.zeros([0,raw_emg.shape[1]])
----
----    x = np.concatenate([raw_emg_before, raw_emg, raw_emg_after], 0)
----    x = apply_to_all(notch_harmonics, x, 60, 1000)
----    x = apply_to_all(remove_drift, x, 1000)
----    x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]
----    emg_orig = apply_to_all(subsample, x, 400.00, 1000)
----    x = apply_to_all(subsample, x, 300, 1000)
----    emg = x
----
----    for c in FLAGS.remove_channels:
----        emg[:,int(c)] = 0
----        emg_orig[:,int(c)] = 0
----
----    emg_features = get_emg_features(emg)
----
----    mfccs = load_audio(os.path.join(base_dir, f'{index}_audio_clean.flac'),
----            max_frames=min(emg_features.shape[0], 800 if limit_length else float('inf')))
----
----    if emg_features.shape[0] > mfccs.shape[0]:
----        emg_features = emg_features[:mfccs.shape[0],:]
----    assert emg_features.shape[0] == mfccs.shape[0]
----    emg = emg[6:6+6*emg_features.shape[0],:]
----    emg_orig = emg_orig[8:8+8*emg_features.shape[0],:]
----    assert emg.shape[0] == emg_features.shape[0]*6
----
----    with open(os.path.join(base_dir, f'{index}_info.json')) as f:
----        info = json.load(f)
----
----    sess = os.path.basename(base_dir)
----    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'
----    if os.path.exists(tg_fname):
----        phonemes = read_phonemes(tg_fname, mfccs.shape[0])
----    else:
----        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')
----
----    return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)
----
----class EMGDirectory(object):
----    def __init__(self, session_index, directory, silent, exclude_from_testset=False):
----        self.session_index = session_index
----        self.directory = directory
----        self.silent = silent
----        self.exclude_from_testset = exclude_from_testset
----
----    def __lt__(self, other):
----        return self.session_index < other.session_index
----
----    def __repr__(self):
----        return self.directory
----
----class SizeAwareSampler(torch.utils.data.Sampler):
----    def __init__(self, emg_dataset, max_len):
----        self.dataset = emg_dataset
----        self.max_len = max_len
----
----    def __iter__(self):
----        indices = list(range(len(self.dataset)))
----        random.shuffle(indices)
----        batch = []
----        batch_length = 0
----        for idx in indices:
----            directory_info, file_idx = self.dataset.example_indices[idx]
----            with open(os.path.join(directory_info.directory, f'{file_idx}_info.json')) as f:
----                info = json.load(f)
----            if not np.any([l in string.ascii_letters for l in info['text']]):
----                continue
----            length = sum([emg_len for emg_len, _, _ in info['chunks']])
----            if length > self.max_len:
----                logging.warning(f'Warning: example {idx} cannot fit within desired batch length')
----            if length + batch_length > self.max_len:
----                yield batch
----                batch = []
----                batch_length = 0
----            batch.append(idx)
----            batch_length += length
----        # dropping last incomplete batch
----
----class EMGDataset(torch.utils.data.Dataset):
----    def __init__(self, base_dir=None, limit_length=False, dev=False, test=False, no_testset=False, no_normalizers=False):
----
----        self.text_align_directory = FLAGS.text_align_directory
----
----        if no_testset:
----            devset = []
----            testset = []
----        else:
----            with open(FLAGS.testset_file) as f:
----                testset_json = json.load(f)
----                devset = testset_json['dev']
----                testset = testset_json['test']
----
----        directories = []
----        if base_dir is not None:
----            directories.append(EMGDirectory(0, base_dir, False))
----        else:
----            for sd in FLAGS.silent_data_directories:
----                for session_dir in sorted(os.listdir(sd)):
----                    directories.append(EMGDirectory(len(directories), os.path.join(sd, session_dir), True))
----
----            has_silent = len(FLAGS.silent_data_directories) > 0
----            for vd in FLAGS.voiced_data_directories:
----                for session_dir in sorted(os.listdir(vd)):
----                    directories.append(EMGDirectory(len(directories), os.path.join(vd, session_dir), False, exclude_from_testset=has_silent))
----
----        self.example_indices = []
----        self.voiced_data_locations = {} # map from book/sentence_index to directory_info/index
----        for directory_info in directories:
----            for fname in os.listdir(directory_info.directory):
----                m = re.match(r'(\d+)_info.json', fname)
----                if m is not None:
----                    idx_str = m.group(1)
----                    with open(os.path.join(directory_info.directory, fname)) as f:
----                        info = json.load(f)
----                        if info['sentence_index'] >= 0: # boundary clips of silence are marked -1
----                            location_in_testset = [info['book'], info['sentence_index']] in testset
----                            location_in_devset = [info['book'], info['sentence_index']] in devset
----                            if (test and location_in_testset and not directory_info.exclude_from_testset) \
----                                    or (dev and location_in_devset and not directory_info.exclude_from_testset) \
----                                    or (not test and not dev and not location_in_testset and not location_in_devset):
----                                self.example_indices.append((directory_info,int(idx_str)))
----
----                            if not directory_info.silent:
----                                location = (info['book'], info['sentence_index'])
----                                self.voiced_data_locations[location] = (directory_info,int(idx_str))
----
----        self.example_indices.sort()
----        random.seed(0)
----        random.shuffle(self.example_indices)
----
----        self.no_normalizers = no_normalizers
----        if not self.no_normalizers:
----            self.mfcc_norm, self.emg_norm = pickle.load(open(FLAGS.normalizers_file,'rb'))
----
----        sample_mfccs, sample_emg, _, _, _, _ = load_utterance(self.example_indices[0][0].directory, self.example_indices[0][1])
----        self.num_speech_features = sample_mfccs.shape[1]
----        self.num_features = sample_emg.shape[1]
----        self.limit_length = limit_length
----        self.num_sessions = len(directories)
----
----        self.text_transform = TextTransform()
----
----    def silent_subset(self):
----        result = copy(self)
----        silent_indices = []
----        for example in self.example_indices:
----            if example[0].silent:
----                silent_indices.append(example)
----        result.example_indices = silent_indices
----        return result
----
----    def subset(self, fraction):
----        result = copy(self)
----        result.example_indices = self.example_indices[:int(fraction*len(self.example_indices))]
----        return result
----
----    def __len__(self):
----        return len(self.example_indices)
----
----    @lru_cache(maxsize=None)
----    def __getitem__(self, i):
----        directory_info, idx = self.example_indices[i]
----        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)
----        raw_emg = raw_emg / 20
----        raw_emg = 50*np.tanh(raw_emg/50.)
----
----        if not self.no_normalizers:
----            mfccs = self.mfcc_norm.normalize(mfccs)
----            emg = self.emg_norm.normalize(emg)
----            emg = 8*np.tanh(emg/8.)
----
----        session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)
----        audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'
----
----        #self.text_transform.add_new_words(text)
----        text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)
----
----        result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}
----
----        if directory_info.silent:
----            voiced_directory, voiced_idx = self.voiced_data_locations[book_location]
----            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)
----
----            if not self.no_normalizers:
----                voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)
----                voiced_emg = self.emg_norm.normalize(voiced_emg)
----                voiced_emg = 8*np.tanh(voiced_emg/8.)
----
----            result['parallel_voiced_audio_features'] = torch.from_numpy(voiced_mfccs).pin_memory()
----            result['parallel_voiced_emg'] = torch.from_numpy(voiced_emg).pin_memory()
----
----            audio_file = f'{voiced_directory.directory}/{voiced_idx}_audio_clean.flac'
----
----        result['phonemes'] = torch.from_numpy(phonemes).pin_memory() # either from this example if vocalized or aligned example if silent
----        result['audio_file'] = audio_file
----
----        return result
----
----    @staticmethod
----    def collate_raw(batch):
----        batch_size = len(batch)
----        audio_features = []
----        audio_feature_lengths = []
----        parallel_emg = []
----        for ex in batch:
----            if ex['silent']:
----                audio_features.append(ex['parallel_voiced_audio_features'])
----                audio_feature_lengths.append(ex['parallel_voiced_audio_features'].shape[0])
----                parallel_emg.append(ex['parallel_voiced_emg'])
----            else:
----                audio_features.append(ex['audio_features'])
----                audio_feature_lengths.append(ex['audio_features'].shape[0])
----                parallel_emg.append(np.zeros(1))
----        phonemes = [ex['phonemes'] for ex in batch]
----        emg = [ex['emg'] for ex in batch]
----        raw_emg = [ex['raw_emg'] for ex in batch]
----        session_ids = [ex['session_ids'] for ex in batch]
----        lengths = [ex['emg'].shape[0] for ex in batch]
----        silent = [ex['silent'] for ex in batch]
----        text_ints = [ex['text_int'] for ex in batch]
----        text_lengths = [ex['text_int'].shape[0] for ex in batch]
----
----        result = {'audio_features':audio_features,
----                  'audio_feature_lengths':audio_feature_lengths,
----                  'emg':emg,
----                  'raw_emg':raw_emg,
----                  'parallel_voiced_emg':parallel_emg,
----                  'phonemes':phonemes,
----                  'session_ids':session_ids,
----                  'lengths':lengths,
----                  'silent':silent,
----                  'text_int':text_ints,
----                  'text_int_lengths':text_lengths}
----        return result
----
----def make_normalizers():
----    dataset = EMGDataset(no_normalizers=True)
----    mfcc_samples = []
----    emg_samples = []
----    for d in dataset:
----        mfcc_samples.append(d['audio_features'])
----        emg_samples.append(d['emg'])
----        if len(emg_samples) > 50:
----            break
----    mfcc_norm = FeatureNormalizer(mfcc_samples, share_scale=True)
----    emg_norm = FeatureNormalizer(emg_samples, share_scale=False)
----    pickle.dump((mfcc_norm, emg_norm), open(FLAGS.normalizers_file, 'wb'))
----
----if __name__ == '__main__':
----    FLAGS(sys.argv)
----    d = EMGDataset()
----    for i in range(1000):
----        d[i]
----
---diff --git a/recognition_model.py b/recognition_model.py
---deleted file mode 100644
---index aed0b22..0000000
------ a/recognition_model.py
---+++ /dev/null
---@@ -1,239 +0,0 @@
----import os
----import sys
----import numpy as np
----import logging
----import subprocess
----from ctcdecode import CTCBeamDecoder, DecoderState
----import jiwer
----import random
----from torch.utils.tensorboard import SummaryWriter
----
----import torch
----from torch import nn
----import torch.nn.functional as F
----
----from read_emg import EMGDataset, SizeAwareSampler
----from architecture import Model
----from data_utils import combine_fixed_length
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_boolean('debug', False, 'debug')
----flags.DEFINE_string('output_directory', 'output', 'where to save models and outputs')
----flags.DEFINE_integer('batch_size', 32, 'training batch size')
----flags.DEFINE_float('learning_rate', 3e-5, 'learning rate')
----flags.DEFINE_integer('learning_rate_warmup', 1000, 'steps of linear warmup')
----flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')
----flags.DEFINE_string('start_training_from', None, 'start training from this model')
----flags.DEFINE_float('l2', 0, 'weight decay')
----flags.DEFINE_float('alpha_loss', 0.8, 'parameter alpha for the two losses')
----flags.DEFINE_float('report_every', 10, "Reporting parameter of the loss plot")
----flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file')
----
----def test(model, testset, device):
----    model.eval()
----
----    blank_id = 1
----    decoder = CTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
----            model_path='lm.binary', alpha=1.5, beta=1.85)
----    state = DecoderState(decoder)
----    dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
----    references = []
----    predictions = []
----    batch_idx = 0
----    with torch.no_grad():
----        for example in dataloader:
----            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
----            tgt = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----
----            #Prediction without the 197-th batch because of missing label
----            if batch_idx != 181:
----                out_enc, out_dec = model(X_raw, tgt)
----                pred  = F.log_softmax(out_dec, -1)
----
----                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
----                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
----
----                pred_text = testset.text_transform.int_to_text(pred_int)
----                target_text = testset.text_transform.clean_text(example['text'][0])
----
----                references.append(target_text)
----                predictions.append(pred_text)
----
----        batch_idx += 1
----        
----    model.train()
----    #remove empty strings because I had an error in the calculation of WER function
----    predictions = [predictions[i] for i in range(len(predictions)) if len(references[i]) > 0]
----    references = [references[i] for i in range(len(references)) if len(references[i]) > 0]
----    return jiwer.wer(references, predictions)
----
----
----def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
----    #Define Dataloader
----    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, shuffle= True ,collate_fn=EMGDataset.collate_raw, batch_size=2)
----    dataloader_evaluation = torch.utils.data.DataLoader(devset, collate_fn=EMGDataset.collate_raw, batch_size=1)
----
----    #Define model and loss function
----    n_chars = len(devset.text_transform.chars)
----    model = Model(devset.num_features, n_chars + 1, n_chars, device, True).to(device)
----    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
----
----    if FLAGS.start_training_from is not None:
----        state_dict = torch.load(FLAGS.start_training_from)
----        model.load_state_dict(state_dict, strict=False)
----
----    #Define optimizer and scheduler for the learning rate
----    optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
----    lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
----
----
----    def set_lr(new_lr):
----        for param_group in optim.param_groups:
----            param_group['lr'] = new_lr
----
----    target_lr = FLAGS.learning_rate
----    def schedule_lr(iteration):
----        iteration = iteration + 1
----        if iteration <= FLAGS.learning_rate_warmup:
----            set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
----
----    batch_idx = 0
----    train_loss= 0
----    eval_loss = 0
----    run_steps=0
----    optim.zero_grad()
----    for epoch_idx in range(n_epochs):
----        model.train()
----        losses = []
----        for example in dataloader_training:
----            schedule_lr(batch_idx)
----            
----            #Preprosessing of the input and target for the model
----            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----
----            #Shifting target for input decoder and loss
----            tgt= y[:,:-1]
----            target= y[:,1:]
----
----            #Prediction
----            out_enc, out_dec = model(X_raw, tgt, example['lengths'])
----
----            #Decoder Loss
----            out_dec=out_dec.permute(0,2,1)
----            loss_dec = loss_fn(out_dec, target)
----
----            #Encoder Loss
----            out_enc = F.log_softmax(out_enc, 2)
----            out_enc = out_enc.transpose(1,0)
----            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)) 
----
----            #Combination the two losses
----            loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
----            losses.append(loss.item())
----            train_loss += loss.item()
----
----            #Gradient Update
----            loss.backward()
----            if (batch_idx+1) % 30 == 0:
----                optim.step()
----                optim.zero_grad()
----
----            #Debug
----            #val = test(model, devset, device)
----
----            #Increment counter and print the loss training       
----            batch_idx += 1
----
----
----            if batch_idx % report_every == 0:     
----                #Evaluation
----                model.eval()
----                with torch.no_grad():
----                    for idx, example in enumerate(dataloader_evaluation):
----                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
----                        y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----                    
----                        #Shifting target for input decoder and loss
----                        tgt= y[:,:-1]
----                        target= y[:,1:]
----
----                        #Prediction without the 197-th batch because of missing label
----                        out_enc, out_dec = model(X_raw, tgt, example['lengths'])
----
----                        #Decoder Loss
----                        out_dec=out_dec.permute(0,2,1)
----                        loss = loss_fn(out_dec, target)
----                        loss_dec = loss_fn(out_dec, target)
----
----                        #Encoder Loss
----                        out_enc = F.log_softmax(out_enc, 2)
----                        out_enc = out_enc.transpose(1,0)
----                        loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = len(devset.text_transform.chars)) 
----
----                        #Combination the two losses
----                        loss = (1 - FLAGS.alpha_loss) * loss_dec + FLAGS.alpha_loss * loss_enc
----                        eval_loss += loss.item()
----                        run_steps += 1
----                        
----                        #just for now
----                        if idx == 10:
----                            break
----
----                #Writing on tensorboard
----                writer.add_scalar('Loss/Evaluation', eval_loss / run_steps, batch_idx)
----                writer.add_scalar('Loss/Training', train_loss / run_steps, batch_idx)
----                train_loss= 0
----                eval_loss= 0
----                run_steps=0
----
----        #Testing and change learning rate
----        #val = test(model, devset, device)
----        #writer.add_scalar('WER/Evaluation',val, batch_idx)
----        lr_sched.step()
----    
----        #Logging
----        train_loss = np.mean(losses)
----        logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f}')
----        torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
----
----    model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters
----    return model
----
----def evaluate_saved():
----    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
----    testset = EMGDataset(test=True)
----    n_chars = len(testset.text_transform.chars)
----    model = Model(testset.num_features, n_chars+1).to(device)
----    model.load_state_dict(torch.load(FLAGS.evaluate_saved))
----    print('WER:', test(model, testset, device))
----
----def main():
----    os.makedirs(FLAGS.output_directory, exist_ok=True)
----    logging.basicConfig(handlers=[
----            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),
----            logging.StreamHandler()
----            ], level=logging.INFO, format="%(message)s")
----
----    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
----    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
----
----    logging.info(sys.argv)
----
----    trainset = EMGDataset(dev=False,test=False)
----    devset = EMGDataset(dev=True)
----    logging.info('output example: %s', devset.example_indices[0])
----    logging.info('train / dev split: %d %d',len(trainset),len(devset))
----
----    device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
----    writer = SummaryWriter(log_dir="./content/runs")
----
----    model = train_model(trainset, devset ,device, writer)
----
----if __name__ == '__main__':
----    FLAGS(sys.argv)
----    if FLAGS.evaluate_saved is not None:
----        evaluate_saved()
----    else:
----        main()
---diff --git a/transduction_model.py b/transduction_model.py
---deleted file mode 100644
---index 70b9bf7..0000000
------ a/transduction_model.py
---+++ /dev/null
---@@ -1,253 +0,0 @@
----import os
----import sys
----import numpy as np
----import logging
----import subprocess
----import random
----
----import soundfile as sf
----
----import torch
----from torch import nn
----import torch.nn.functional as F
----
----from read_emg import EMGDataset, SizeAwareSampler
----from architecture import Model
----from align import align_from_distances
----from asr_evaluation import evaluate
----from data_utils import phoneme_inventory, decollate_tensor, combine_fixed_length
----from vocoder import Vocoder
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_integer('batch_size', 32, 'training batch size')
----flags.DEFINE_integer('epochs', 80, 'number of training epochs')
----flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')
----flags.DEFINE_integer('learning_rate_patience', 5, 'learning rate decay patience')
----flags.DEFINE_integer('learning_rate_warmup', 500, 'steps of linear warmup')
----flags.DEFINE_string('start_training_from', None, 'start training from this model')
----flags.DEFINE_float('data_size_fraction', 1.0, 'fraction of training data to use')
----flags.DEFINE_float('phoneme_loss_weight', 0.5, 'weight of auxiliary phoneme prediction loss')
----flags.DEFINE_float('l2', 1e-7, 'weight decay')
----flags.DEFINE_string('output_directory', 'output', 'output directory')
----
----def test(model, testset, device):
----    model.eval()
----
----    dataloader = torch.utils.data.DataLoader(testset, batch_size=32, collate_fn=testset.collate_raw)
----    losses = []
----    accuracies = []
----    phoneme_confusion = np.zeros((len(phoneme_inventory),len(phoneme_inventory)))
----    seq_len = 200
----    with torch.no_grad():
----        for batch in dataloader:
----            X = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['emg']], seq_len)
----            X_raw = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['raw_emg']], seq_len*8)
----            sess = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['session_ids']], seq_len)
----
----            pred, phoneme_pred = model(X, X_raw, sess)
----
----            loss, phon_acc = dtw_loss(pred, phoneme_pred, batch, True, phoneme_confusion)
----            losses.append(loss.item())
----
----            accuracies.append(phon_acc)
----
----    model.train()
----    return np.mean(losses), np.mean(accuracies), phoneme_confusion #TODO size-weight average
----
----def save_output(model, datapoint, filename, device, audio_normalizer, vocoder):
----    model.eval()
----    with torch.no_grad():
----        sess = torch.tensor(datapoint['session_ids'], device=device).unsqueeze(0)
----        X = torch.tensor(datapoint['emg'], dtype=torch.float32, device=device).unsqueeze(0)
----        X_raw = torch.tensor(datapoint['raw_emg'], dtype=torch.float32, device=device).unsqueeze(0)
----
----        pred, _ = model(X, X_raw, sess)
----        y = pred.squeeze(0)
----
----        y = audio_normalizer.inverse(y.cpu()).to(device)
----
----        audio = vocoder(y).cpu().numpy()
----
----    sf.write(filename, audio, 22050)
----
----    model.train()
----
----def get_aligned_prediction(model, datapoint, device, audio_normalizer):
----    model.eval()
----    with torch.no_grad():
----        silent = datapoint['silent']
----        sess = datapoint['session_ids'].to(device).unsqueeze(0)
----        X = datapoint['emg'].to(device).unsqueeze(0)
----        X_raw = datapoint['raw_emg'].to(device).unsqueeze(0)
----        y = datapoint['parallel_voiced_audio_features' if silent else 'audio_features'].to(device).unsqueeze(0)
----
----        pred, _ = model(X, X_raw, sess) # (1, seq, dim)
----
----        if silent:
----            costs = torch.cdist(pred, y).squeeze(0)
----            alignment = align_from_distances(costs.T.detach().cpu().numpy())
----            pred_aligned = pred.squeeze(0)[alignment]
----        else:
----            pred_aligned = pred.squeeze(0)
----
----        pred_aligned = audio_normalizer.inverse(pred_aligned.cpu())
----
----    model.train()
----    return pred_aligned
----
----def dtw_loss(predictions, phoneme_predictions, example, phoneme_eval=False, phoneme_confusion=None):
----    device = predictions.device
----
----    predictions = decollate_tensor(predictions, example['lengths'])
----    phoneme_predictions = decollate_tensor(phoneme_predictions, example['lengths'])
----
----    audio_features = [t.to(device, non_blocking=True) for t in example['audio_features']]
----
----    phoneme_targets = example['phonemes']
----
----    losses = []
----    correct_phones = 0
----    total_length = 0
----    for pred, y, pred_phone, y_phone, silent in zip(predictions, audio_features, phoneme_predictions, phoneme_targets, example['silent']):
----        assert len(pred.size()) == 2 and len(y.size()) == 2
----        y_phone = y_phone.to(device)
----
----        if silent:
----            dists = torch.cdist(pred.unsqueeze(0), y.unsqueeze(0))
----            costs = dists.squeeze(0)
----
----            # pred_phone (seq1_len, 48), y_phone (seq2_len)
----            # phone_probs (seq1_len, seq2_len)
----            pred_phone = F.log_softmax(pred_phone, -1)
----            phone_lprobs = pred_phone[:,y_phone]
----
----            costs = costs + FLAGS.phoneme_loss_weight * -phone_lprobs
----
----            alignment = align_from_distances(costs.T.cpu().detach().numpy())
----
----            loss = costs[alignment,range(len(alignment))].sum()
----
----            if phoneme_eval:
----                alignment = align_from_distances(costs.T.cpu().detach().numpy())
----
----                pred_phone = pred_phone.argmax(-1)
----                correct_phones += (pred_phone[alignment] == y_phone).sum().item()
----
----                for p, t in zip(pred_phone[alignment].tolist(), y_phone.tolist()):
----                    phoneme_confusion[p, t] += 1
----        else:
----            assert y.size(0) == pred.size(0)
----
----            dists = F.pairwise_distance(y, pred)
----
----            assert len(pred_phone.size()) == 2 and len(y_phone.size()) == 1
----            phoneme_loss = F.cross_entropy(pred_phone, y_phone, reduction='sum')
----            loss = dists.sum() + FLAGS.phoneme_loss_weight * phoneme_loss
----
----            if phoneme_eval:
----                pred_phone = pred_phone.argmax(-1)
----                correct_phones += (pred_phone == y_phone).sum().item()
----
----                for p, t in zip(pred_phone.tolist(), y_phone.tolist()):
----                    phoneme_confusion[p, t] += 1
----
----        losses.append(loss)
----        total_length += y.size(0)
----
----    return sum(losses)/total_length, correct_phones/total_length
----
----def train_model(trainset, devset, device, save_sound_outputs=True):
----    n_epochs = FLAGS.epochs
----
----    if FLAGS.data_size_fraction >= 1:
----        training_subset = trainset
----    else:
----        training_subset = trainset.subset(FLAGS.data_size_fraction)
----    dataloader = torch.utils.data.DataLoader(training_subset, pin_memory=(device=='cuda'), collate_fn=devset.collate_raw, num_workers=0, batch_sampler=SizeAwareSampler(training_subset, 256000))
----
----    n_phones = len(phoneme_inventory)
----    model = Model(devset.num_features, devset.num_speech_features, n_phones).to(device)
----
----    if FLAGS.start_training_from is not None:
----        state_dict = torch.load(FLAGS.start_training_from)
----        model.load_state_dict(state_dict, strict=False)
----
----    if save_sound_outputs:
----        vocoder = Vocoder()
----
----    optim = torch.optim.AdamW(model.parameters(), weight_decay=FLAGS.l2)
----    lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', 0.5, patience=FLAGS.learning_rate_patience)
----
----    def set_lr(new_lr):
----        for param_group in optim.param_groups:
----            param_group['lr'] = new_lr
----
----    target_lr = FLAGS.learning_rate
----    def schedule_lr(iteration):
----        iteration = iteration + 1
----        if iteration <= FLAGS.learning_rate_warmup:
----            set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
----
----    seq_len = 200
----
----    batch_idx = 0
----    for epoch_idx in range(n_epochs):
----        losses = []
----        for batch in dataloader:
----            optim.zero_grad()
----            schedule_lr(batch_idx)
----
----            X = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['emg']], seq_len)
----            X_raw = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['raw_emg']], seq_len*8)
----            sess = combine_fixed_length([t.to(device, non_blocking=True) for t in batch['session_ids']], seq_len)
----
----            pred, phoneme_pred = model(X, X_raw, sess)
----
----            loss, _ = dtw_loss(pred, phoneme_pred, batch)
----            losses.append(loss.item())
----
----            loss.backward()
----            optim.step()
----
----            batch_idx += 1
----        train_loss = np.mean(losses)
----        val, phoneme_acc, _ = test(model, devset, device)
----        lr_sched.step(val)
----        logging.info(f'finished epoch {epoch_idx+1} - validation loss: {val:.4f} training loss: {train_loss:.4f} phoneme accuracy: {phoneme_acc*100:.2f}')
----        torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
----        if save_sound_outputs:
----            save_output(model, devset[0], os.path.join(FLAGS.output_directory, f'epoch_{epoch_idx}_output.wav'), device, devset.mfcc_norm, vocoder)
----
----    if save_sound_outputs:
----        for i, datapoint in enumerate(devset):
----            save_output(model, datapoint, os.path.join(FLAGS.output_directory, f'example_output_{i}.wav'), device, devset.mfcc_norm, vocoder)
----
----        evaluate(devset, FLAGS.output_directory)
----
----    return model
----
----def main():
----    os.makedirs(FLAGS.output_directory, exist_ok=True)
----    logging.basicConfig(handlers=[
----            logging.FileHandler(os.path.join(FLAGS.output_directory, 'log.txt'), 'w'),
----            logging.StreamHandler()
----            ], level=logging.INFO, format="%(message)s")
----
----    logging.info(subprocess.run(['git','rev-parse','HEAD'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
----    logging.info(subprocess.run(['git','diff'], stdout=subprocess.PIPE, universal_newlines=True).stdout)
----
----    logging.info(sys.argv)
----
----    trainset = EMGDataset(dev=False,test=False)
----    devset = EMGDataset(dev=True)
----    logging.info('output example: %s', devset.example_indices[0])
----    logging.info('train / dev split: %d %d',len(trainset),len(devset))
----
----    device = 'cuda' if torch.cuda.is_available() else 'cpu'
----
----    model = train_model(trainset, devset, device, save_sound_outputs=(FLAGS.hifigan_checkpoint is not None))
----
----if __name__ == '__main__':
----    FLAGS(sys.argv)
----    main()
---diff --git a/transformer.py b/transformer.py
---deleted file mode 100644
---index 817ccaf..0000000
------ a/transformer.py
---+++ /dev/null
---@@ -1,434 +0,0 @@
----import math
----from typing import Optional
----
----import torch
----from torch import nn
----import torch.nn.functional as F
----
----class TransformerEncoderLayer(nn.Module):
----    # Adapted from pytorch source
----    r"""TransformerEncoderLayer is made up of self-attn and feedforward network.
----    This standard encoder layer is based on the paper "Attention Is All You Need".
----    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
----    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
----    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
----    in a different way during application.
----
----    Args:
----        d_model: the number of expected features in the input (required).
----        nhead: the number of heads in the multiheadattention models (required).
----        dim_feedforward: the dimension of the feedforward network model (default=2048).
----        dropout: the dropout value (default=0.1).
----
----    Examples::
----        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
----        >>> src = torch.rand(10, 32, 512)
----        >>> out = encoder_layer(src)
----    """
----
----    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
----        super(TransformerEncoderLayer, self).__init__()
----        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
----        # Implementation of Feedforward model
----        self.linear1 = nn.Linear(d_model, dim_feedforward)
----        self.dropout = nn.Dropout(dropout)
----        self.linear2 = nn.Linear(dim_feedforward, d_model)
----
----        self.norm1 = nn.LayerNorm(d_model)
----        self.norm2 = nn.LayerNorm(d_model)
----        self.dropout1 = nn.Dropout(dropout)
----        self.dropout2 = nn.Dropout(dropout)
----
----        self.activation = nn.ReLU()
----
----    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
----        r"""Pass the input through the encoder layer.
----
----        Args:
----            src: the sequence to the encoder layer (required).
----            src_mask: the mask for the src sequence (optional).
----            src_key_padding_mask: the mask for the src keys per batch (optional).
----
----        Shape:
----            see the docs in Transformer class.
----        """
----        src2 = self.self_attn(src, src, src, src_key_padding_mask=src_key_padding_mask)
----        src = src + self.dropout1(src2)
----        src = self.norm1(src)
----        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
----        src = src + self.dropout2(src2)
----        src = self.norm2(src)
----        return src
----
----class TransformerDecoderLayer(nn.Module):
----    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
----    This standard decoder layer is based on the paper "Attention Is All You Need".
----    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
----    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
----    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
----    in a different way during application.
----
----    Args:
----        d_model: the number of expected features in the input (required).
----        nhead: the number of heads in the multiheadattention models (required).
----        dim_feedforward: the dimension of the feedforward network model (default=2048).
----        dropout: the dropout value (default=0.1).
----        activation: the activation function of the intermediate layer, can be a string
----            ("relu" or "gelu") or a unary callable. Default: relu
----        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
----        batch_first: If ``True``, then the input and output tensors are provided
----            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
----        norm_first: if ``True``, layer norm is done prior to self attention, multihead
----            attention and feedforward operations, respectively. Otherwise it's done after.
----            Default: ``False`` (after).
----        """
----    # Adapted from pytorch source
----    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
----        super(TransformerDecoderLayer, self).__init__()
----        #Attention Mechanism
----        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
----        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
----        # Implementation of Feedforward model
----        self.linear1 = nn.Linear(d_model, dim_feedforward)
----        self.dropout = nn.Dropout(dropout)
----        self.linear2 = nn.Linear(dim_feedforward, d_model)
----        #Normalization Layer and Dropout Layer
----        self.norm1 = nn.LayerNorm(d_model)
----        self.norm2 = nn.LayerNorm(d_model)
----        self.norm3 = nn.LayerNorm(d_model)
----        self.dropout1 = nn.Dropout(dropout)
----        self.dropout2 = nn.Dropout(dropout)
----        self.dropout3 = nn.Dropout(dropout)
----        #Activation Function
----        self.activation = nn.ReLU()
----    
----    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
----                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
----        r"""Pass the input through the encoder layer.
----
----        Args:
----            tgt: the sequence to the decoder layer (required).
----            memory: the sequence from the last layer of the encoder (required).
----            tgt_mask: the mask for the tgt sequence (optional).
----            memory_mask: the mask for the memory sequence (optional).
----            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
----            memory_key_padding_mask: the mask for the memory keys per batch (optional).
----
----        Shape:
----            see the docs in Transformer class.
----        """
----        tgt2 = self.self_attn(tgt, tgt, tgt,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
----        tgt = tgt + self.dropout1(tgt2)
----        tgt = self.norm1(tgt)
----
----        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask=memory_key_padding_mask)
----        tgt = tgt + self.dropout1(tgt2)
----        tgt = self.norm1(tgt)
----
----        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
----        tgt = tgt + self.dropout2(tgt2)
----        tgt = self.norm2(tgt)
----        return tgt
----    
----
----class MultiHeadAttention(nn.Module):
----  def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
----    super().__init__()
----    self.d_model = d_model
----    self.n_head = n_head
----    d_qkv = d_model // n_head
----    assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
----    self.d_qkv = d_qkv
----
----    self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----    self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----    self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----    self.w_o = nn.Parameter(torch.Tensor(n_head, d_qkv, d_model))
----    nn.init.xavier_normal_(self.w_q)
----    nn.init.xavier_normal_(self.w_k)
----    nn.init.xavier_normal_(self.w_v)
----    nn.init.xavier_normal_(self.w_o)
----
----    self.dropout = nn.Dropout(dropout)
----
----    if relative_positional:
----        self.relative_positional = LearnedRelativePositionalEmbedding(relative_positional_distance, n_head, d_qkv, True)
----    else:
----        self.relative_positional = None
----
----  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, tgt_key_padding_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None):
----    """Runs the multi-head self-attention layer.
----
----    Args:
----      x: the input to the layer, a tensor of shape [length, batch_size, d_model]
----    Returns:
----      A single tensor containing the output from this layer
----    """
----
----    #Computes projections
----    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
----    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
----    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
----     
----    # Compute scaled dot-product attention
----    logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
----
----    # Apply att_mask to the attention weights if provided
----    if tgt_mask is not None:
----        logits = logits.masked_fill(tgt_mask == float('-inf'), -1e8)
----    
----    
----    #Apply padding_mask to the attention weights if provided
----    if tgt_key_padding_mask is not None:
----       logits = logits.masked_fill(tgt_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
----
----    #Apply padding_mask to the attention weights if provided
----    if src_key_padding_mask is not None:
----       logits = logits.masked_fill(src_key_padding_mask.unsqueeze(1).unsqueeze(3), -1e8)
----
----    #Apply padding_mask to the attention weights if provided
----    if memory_key_padding_mask is not None:
----       logits = logits.masked_fill(memory_key_padding_mask.unsqueeze(1).unsqueeze(2), -1e8)
----    
----    
----    if self.relative_positional is not None:
----        q_pos = q.permute(2,0,1,3) #bhqd->qbhd
----        l,b,h,d = q_pos.size()
----        position_logits, _ = self.relative_positional(q_pos.reshape(l,b*h,d))
----        # (bh)qk
----        logits = logits + position_logits.view(b,h,l,l)
----
----    probs = F.softmax(logits, dim=-1)
----    probs = self.dropout(probs)
----    o = torch.einsum('bhqk,bhka->bhqa', probs, v)
----    out = torch.einsum('bhta,haf->tbf', o, self.w_o)
----    return out
----
----class LearnedRelativePositionalEmbedding(nn.Module):
----    # from https://github.com/pytorch/fairseq/pull/2225/commits/a7fb63f2b84d5b20c8855e9c3372a95e5d0ea073
----    """
----    This module learns relative positional embeddings up to a fixed
----    maximum size. These are masked for decoder and unmasked for encoder
----    self attention.
----    By default the embeddings are added to keys, but could be added to
----    values as well.
----    Args:
----        max_relative_pos (int): the maximum relative positions to compute embeddings for
----        num_heads (int): number of attention heads
----        embedding_dim (int): depth of embeddings
----        unmasked (bool): if the attention is unmasked (for transformer encoder)
----        heads_share_embeddings (bool): if heads share the same relative positional embeddings
----        add_to_values (bool): compute embeddings to be added to values as well
----    """
----
----    def __init__(
----            self,
----            max_relative_pos: int,
----            num_heads: int,
----            embedding_dim: int,
----            unmasked: bool = False,
----            heads_share_embeddings: bool = False,
----            add_to_values: bool = False):
----        super().__init__()
----        self.max_relative_pos = max_relative_pos
----        self.num_heads = num_heads
----        self.embedding_dim = embedding_dim
----        self.unmasked = unmasked
----        self.heads_share_embeddings = heads_share_embeddings
----        self.add_to_values = add_to_values
----        num_embeddings = (
----            2 * max_relative_pos - 1
----            if unmasked
----            else max_relative_pos
----        )
----        embedding_size = (
----            [num_embeddings, embedding_dim, 1]
----            if heads_share_embeddings
----            else [num_heads, num_embeddings, embedding_dim, 1]
----        )
----        if add_to_values:
----            embedding_size[-1] = 2
----        initial_stddev = embedding_dim**(-0.5)
----        self.embeddings = nn.Parameter(torch.zeros(*embedding_size))
----        nn.init.normal_(self.embeddings, mean=0.0, std=initial_stddev)
----
----    def forward(self, query, saved_state=None):
----        """
----        Computes relative positional embeddings to be added to keys (and optionally values),
----        multiplies the embeddings for keys with queries to create positional logits,
----        returns the positional logits, along with embeddings for values (optionally)
----        which could be added to values outside this module.
----        Args:
----            query (torch.Tensor): query tensor
----            saved_state (dict): saved state from previous time step
----        Shapes:
----            query: `(length, batch_size*num_heads, embed_dim)`
----        Returns:
----            tuple(torch.Tensor):
----                - positional logits
----                - relative positional embeddings to be added to values
----        """
----        # During inference when previous states are cached
----        if saved_state is not None and "prev_key" in saved_state:
----            assert not self.unmasked, "This should only be for decoder attention"
----            length = saved_state["prev_key"].shape[-2] + 1  # `length - 1` keys are cached,
----                                                            # `+ 1` for the current time step
----            decoder_step = True
----        else:
----            length = query.shape[0]
----            decoder_step = False
----
----        used_embeddings = self.get_embeddings_for_query(length)
----
----        values_embeddings = (
----            used_embeddings[..., 1]
----            if self.add_to_values
----            else None
----        )
----        positional_logits = self.calculate_positional_logits(query, used_embeddings[..., 0])
----        positional_logits = self.relative_to_absolute_indexing(positional_logits, decoder_step)
----        return (positional_logits, values_embeddings)
----
----    def get_embeddings_for_query(self, length):
----        """
----        Extract the required embeddings. The maximum relative position between two time steps is
----        `length` for masked case or `2*length - 1` for the unmasked case. If `length` is greater than
----        `max_relative_pos`, we first pad the embeddings tensor with zero-embeddings, which represent
----        embeddings when relative position is greater than `max_relative_pos`. In case `length` is
----        less than `max_relative_pos`, we don't use the first `max_relative_pos - length embeddings`.
----        Args:
----            length (int): length of the query
----        Returns:
----            torch.Tensor: embeddings used by the query
----        """
----        pad_length = max(length - self.max_relative_pos, 0)
----        start_pos = max(self.max_relative_pos - length, 0)
----        if self.unmasked:
----            with torch.no_grad():
----                padded_embeddings = nn.functional.pad(
----                    self.embeddings,
----                    (0, 0, 0, 0, pad_length, pad_length)
----                )
----            used_embeddings = padded_embeddings.narrow(-3, start_pos, 2*length - 1)
----        else:
----            with torch.no_grad():
----                padded_embeddings = nn.functional.pad(
----                    self.embeddings,
----                    (0, 0, 0, 0, pad_length, 0)
----                )
----            used_embeddings = padded_embeddings.narrow(-3, start_pos, length)
----        return used_embeddings
----
----    def calculate_positional_logits(self, query, relative_embeddings):
----        """
----        Multiplies query with the relative positional embeddings to create relative
----        positional logits
----        Args:
----            query (torch.Tensor): Input tensor representing queries
----            relative_embeddings (torch.Tensor): relative embeddings compatible with query
----        Shapes:
----            query: `(length, batch_size*num_heads, embed_dim)` if heads share embeddings
----                   else `(length, batch_size, num_heads, embed_dim)`
----            relative_embeddings: `(max_allowed_relative_positions, embed_dim)` if heads share embeddings
----                                 else `(num_heads, max_allowed_relative_positions, embed_dim)`
----                                 where `max_allowed_relative_positions` is `length` if masked
----                                 else `2*length - 1`
----        Returns:
----            torch.Tensor: relative positional logits
----        """
----        if self.heads_share_embeddings:
----            positional_logits = torch.einsum("lbd,md->lbm", query, relative_embeddings)
----        else:
----            query = query.view(query.shape[0], -1, self.num_heads, self.embedding_dim)
----            positional_logits = torch.einsum("lbhd,hmd->lbhm", query, relative_embeddings)
----            positional_logits = positional_logits.contiguous().view(
----                positional_logits.shape[0], -1, positional_logits.shape[-1]
----            )
----        # mask out tokens out of range
----        length = query.size(0)
----        if length > self.max_relative_pos:
----            # there is some padding
----            pad_length = length - self.max_relative_pos
----            positional_logits[:,:,:pad_length] -= 1e8
----            if self.unmasked:
----                positional_logits[:,:,-pad_length:] -= 1e8
----        return positional_logits
----
----    def relative_to_absolute_indexing(self, x, decoder_step):
----        """
----        Index tensor x (relative positional logits) in terms of absolute positions
----        rather than relative positions. Last dimension of x represents relative position
----        with respect to the first dimension, whereas returned tensor has both the first
----        and last dimension indexed with absolute positions.
----        Args:
----            x (torch.Tensor): positional logits indexed by relative positions
----            decoder_step (bool): is this is a single decoder step (during inference)
----        Shapes:
----            x: `(length, batch_size*num_heads, length)` for masked case or
----               `(length, batch_size*num_heads, 2*length - 1)` for unmasked
----        Returns:
----            torch.Tensor: positional logits represented using absolute positions
----        """
----        length, bsz_heads, _ = x.shape
----
----        if decoder_step:
----            return x.contiguous().view(bsz_heads, 1, -1)
----
----        if self.unmasked:
----            x = nn.functional.pad(
----                x,
----                (0, 1)
----            )
----            x = x.transpose(0, 1)
----            x = x.contiguous().view(bsz_heads, length * 2 * length)
----            x = nn.functional.pad(
----                x,
----                (0, length - 1)
----            )
----            # Reshape and slice out the padded elements.
----            x = x.view(bsz_heads, length + 1, 2*length - 1)
----            return x[:, :length, length-1:]
----        else:
----            x = nn.functional.pad(
----                x,
----                (1, 0)
----            )
----            x = x.transpose(0, 1)
----            x = x.contiguous().view(bsz_heads, length+1, length)
----            return x[:, 1:, :]
----        
----
----########
----# Taken from:
----# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
----# or also here:
----# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
----class PositionalEncoding(nn.Module):
----
----    def __init__(self, d_model, dropout=0.0, max_len=5000):
----        super(PositionalEncoding, self).__init__()
----        self.dropout = nn.Dropout(p=dropout)
----        self.max_len = max_len
----
----        pe = torch.zeros(max_len, d_model)
----        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
----        div_term = torch.exp(torch.arange(0, d_model, 2).float()
----                             * (-math.log(10000.0) / d_model))
----        pe[:, 0::2] = torch.sin(position * div_term)
----        pe[:, 1::2] = torch.cos(position * div_term)
----        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
----        self.register_buffer('pe', pe)  # Will not be trained.
----
----    def forward(self, x):
----        """Inputs of forward function
----        Args:
----            x: the sequence fed to the positional encoder model (required).
----        Shape:
----            x: [sequence length, batch size, embed dim]
----            output: [sequence length, batch size, embed dim]
----        """
----        assert x.size(0) < self.max_len, (
----            f"Too long sequence length: increase `max_len` of pos encoding")
----        # shape of x (len, B, dim)
----        x = x + self.pe[:x.size(0), :]
----        return self.dropout(x)
---diff --git a/vocoder.py b/vocoder.py
---deleted file mode 100644
---index c10a523..0000000
------ a/vocoder.py
---+++ /dev/null
---@@ -1,36 +0,0 @@
----import os
----import json
----import sys
----import numpy as np
----
----import torch
----
----sys.path.append('./hifi_gan')
----from env import AttrDict
----from models import Generator
----
----from absl import flags
----FLAGS = flags.FLAGS
----flags.DEFINE_string('hifigan_checkpoint', None, 'filename of hifi-gan generator checkpoint')
----
----class Vocoder(object):
----    def __init__(self, device='cuda'):
----        assert FLAGS.hifigan_checkpoint is not None
----        checkpoint_file = FLAGS.hifigan_checkpoint
----        config_file = os.path.join(os.path.split(checkpoint_file)[0], 'config.json')
----        with open(config_file) as f:
----            hparams = AttrDict(json.load(f))
----        self.generator = Generator(hparams).to(device)
----        self.generator.load_state_dict(torch.load(checkpoint_file)['generator'])
----        self.generator.eval()
----        self.generator.remove_weight_norm()
----
----    def __call__(self, mel_spectrogram):
----        '''
----            mel_spectrogram should be a tensor of shape (seq_len, 80)
----            returns 1d tensor of audio
----        '''
----        with torch.no_grad():
----            mel_spectrogram = mel_spectrogram.T[np.newaxis,:,:]
----            audio = self.generator(mel_spectrogram)
----        return audio.squeeze()
---
---['/home/christian/storage/EMG-christian/silent_speech-main/speech_recognition/recognition_model.py', '--output_directory', './models/recognition_model/']
---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
---train / dev split: 8055 200
--diff --git a/speech_recognition/data_utils.py b/speech_recognition/data_utils.py
--index 6f7fe93..2b30b9f 100644
----- a/speech_recognition/data_utils.py
--+++ b/speech_recognition/data_utils.py
--@@ -4,8 +4,8 @@ import string
-- import numpy as np
-- import librosa
-- import soundfile as sf
---from textgrids import TextGrid
-- import jiwer
--+from num2words import num2words
-- from unidecode import unidecode
-- 
-- import torch
--@@ -15,7 +15,9 @@ from absl import flags
-- FLAGS = flags.FLAGS
-- flags.DEFINE_string('normalizers_file', 'normalizers.pkl', 'file with pickled feature normalizers')
-- 
---phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']
--+#phoneme_inventory = ['aa','ae','ah','ao','aw','ax','axr','ay','b','ch','d','dh','dx','eh','el','em','en','er','ey','f','g','hh','hv','ih','iy','jh','k','l','m','n','nx','ng','ow','oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil']
--+phoneme_inventory = ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0', 'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH', 'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH', 'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1', 'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']
--+pron_dct= { line.split()[0] : line.split()[1:] for line in open('descriptions/librispeech-lexicon.txt') if line.split() != [] }
-- 
-- def normalize_volume(audio):
--     rms = librosa.feature.rms(audio)
--@@ -221,82 +223,30 @@ def print_confusion(confusion_mat, n=10):
--         p2s = phoneme_inventory[p2]
--         print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')
-- 
---def read_phonemes(textgrid_fname, max_len=None):
---    tg = TextGrid(textgrid_fname)
---    phone_ids = np.zeros(int(tg['phones'][-1].xmax*86.133)+1, dtype=np.int64)
---    phone_ids[:] = -1
---    phone_ids[-1] = phoneme_inventory.index('sil') # make sure list is long enough to cover full length of original sequence
---    for interval in tg['phones']:
---        phone = interval.text.lower()
---        if phone in ['', 'sp', 'spn']:
---            phone = 'sil'
---        if phone[-1] in string.digits:
---            phone = phone[:-1]
---        ph_id = phoneme_inventory.index(phone)
---        phone_ids[int(interval.xmin*86.133):int(interval.xmax*86.133)] = ph_id
---    assert (phone_ids >= 0).all(), 'missing aligned phones'
---
---    if max_len is not None:
---        phone_ids = phone_ids[:max_len]
---        assert phone_ids.shape[0] == max_len
---    return phone_ids
---
---class TextTransform(object):
---    '''
---    def __init__(self, pad_token="<pad>", blank_token='_', eos_token='<eos>', sos_token='<sos>'):
---        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
---        self.id_to_string = {}
---        self.string_to_id = {}
---        
---        # add the default pad token
---        self.id_to_string[0] = pad_token
---        self.string_to_id[pad_token] = 0
---        
---        # add the default unknown token
---        self.id_to_string[1] = blank_token
---        self.string_to_id[blank_token] = 1
---        
---        # add the default unknown token
---        self.id_to_string[2] = eos_token
---        self.string_to_id[eos_token] = 2   
---
---        # add the default unknown token
---        self.id_to_string[3] = sos_token
---        self.string_to_id[sos_token] = 3
---
---        # shortcut access
---        self.pad_id = 0
---        self.blank_id = 1
---        self.eos_id = 2
---        self.sos_id = 3
---
---    def __len__(self):
---        return len('<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' ')
---    
---    def clean_text(self, text):
---        text = unidecode(text)
---        text = self.transformation(text)
---        return text
---    
---    def add_new_words(self, text):
---        text = self.clean_text(text)
---        for c in text:
---            if c not in self.string_to_id:
---                self.string_to_id[c] = len(self.string_to_id)
---                self.id_to_string[len(self.id_to_string)] = c
---    
---    def get_labels(self):
---        return  '<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' '
--+def read_phonemes(sentence):
--+    #Transform digits into words
--+    digits=[]
--+    new_sentence=""
--+    for unit in sentence:
--+        if unit.isdigit():
--+            digits.append(unit)
--+        elif unit == ' ' and digits:
--+            new_sentence += num2words(int(''.join(digits))) + ' '
--+            digits=[]  
--+        elif unit.isalpha() or unit == ' ':
--+            new_sentence += unit     
--             
---    def text_to_int(self, text):
---        text = self.clean_text(text)
---        return [self.string_to_id[c] for c in text]
---
---    def int_to_text(self, ints):
---        return ''.join(self.id_to_string[i] for i in ints)
---    '''
--+    #String manipulation before being proccesed by the dictionary     
--+    new_sentence=jiwer.Compose([jiwer.SubstituteRegexes({r"-": r" ",}),jiwer.RemovePunctuation(), jiwer.ToUpperCase()])(new_sentence).split()
--     
--+    #Transform the words into sequences of phones
--+    phones = []
--+    for n in new_sentence:
--+        p = pron_dct[n]
--+        phones.append(p)
--+    return np.array([np.array([phoneme_inventory.index(phone) for phone in word_phone], dtype=np.int32) for word_phone in phones], dtype=np.int32)
-- 
--+class TextTransform(object):
--     def __init__(self):
--         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
--         self.chars = "*" + string.ascii_lowercase+string.digits+ ' '
--diff --git a/speech_recognition/read_emg.py b/speech_recognition/read_emg.py
--index 27787d5..2f30f17 100644
----- a/speech_recognition/read_emg.py
--+++ b/speech_recognition/read_emg.py
--@@ -54,7 +54,7 @@ def apply_to_all(function, signal_array, *args, **kwargs):
--         results.append(function(signal_array[:,i], *args, **kwargs))
--     return np.stack(results, 1)
-- 
---def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_directory=None):
--+def load_utterance(base_dir, index, limit_length=False, debug=False):
--     index = int(index)
--     raw_emg = np.load(os.path.join(base_dir, f'{index}_emg.npy'))
--     before = os.path.join(base_dir, f'{index-1}_emg.npy')
--@@ -96,12 +96,9 @@ def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_
--         info = json.load(f)
-- 
--     sess = os.path.basename(base_dir)
---    tg_fname = f'{text_align_directory}/{sess}/{sess}_{index}_audio.TextGrid'
---    if os.path.exists(tg_fname):
---        phonemes = read_phonemes(tg_fname, mfccs.shape[0])
---    else:
---        phonemes = np.zeros(mfccs.shape[0], dtype=np.int64)+phoneme_inventory.index('sil')
---
--+    
--+    phonemes=read_phonemes(info['text'])
--+    
--     return mfccs, emg_features, info['text'], (info['book'],info['sentence_index']), phonemes, emg_orig.astype(np.float32)
-- 
-- class EMGDirectory(object):
--@@ -228,7 +225,7 @@ class EMGDataset(torch.utils.data.Dataset):
--     @lru_cache(maxsize=None)
--     def __getitem__(self, i):
--         directory_info, idx = self.example_indices[i]
---        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length, text_align_directory=self.text_align_directory)
--+        mfccs, emg, text, book_location, phonemes, raw_emg = load_utterance(directory_info.directory, idx, self.limit_length)
--         raw_emg = raw_emg / 20
--         raw_emg = 50*np.tanh(raw_emg/50.)
-- 
--@@ -247,7 +244,7 @@ class EMGDataset(torch.utils.data.Dataset):
-- 
--         if directory_info.silent:
--             voiced_directory, voiced_idx = self.voiced_data_locations[book_location]
---            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False, text_align_directory=self.text_align_directory)
--+            voiced_mfccs, voiced_emg, _, _, phonemes, _ = load_utterance(voiced_directory.directory, voiced_idx, False)
-- 
--             if not self.no_normalizers:
--                 voiced_mfccs = self.mfcc_norm.normalize(voiced_mfccs)
--
--['/home/christian/storage/EMG-christian/silent_speech-main/speech_recognition/recognition_model.py', '--output_directory', './models/recognition_model/']
-diff --git a/speech_recognition/data_utils.py b/speech_recognition/data_utils.py
-index 2b30b9f..76c2b36 100644
---- a/speech_recognition/data_utils.py
-+++ b/speech_recognition/data_utils.py
-@@ -1,5 +1,6 @@
- import math
- import string
-+import logging
- 
- import numpy as np
- import librosa
-@@ -224,6 +225,10 @@ def print_confusion(confusion_mat, n=10):
-         print(f'{p1s} {p2s} {v*100:.1f} {(confusion_mat[p1,p1]+confusion_mat[p2,p2])/(target_counts[p1]+target_counts[p2])*100:.1f}')
- 
- def read_phonemes(sentence):
-+    
-+    #Premanipulation to avoid issues with num2words
-+    sentence=jiwer.SubstituteRegexes({r"_": r" ", r"th": r" th", r"£": r"pound "})(sentence)
-+    
-     #Transform digits into words
-     digits=[]
-     new_sentence=""
-@@ -233,18 +238,21 @@ def read_phonemes(sentence):
-         elif unit == ' ' and digits:
-             new_sentence += num2words(int(''.join(digits))) + ' '
-             digits=[]  
--        elif unit.isalpha() or unit == ' ':
-+        else:
-             new_sentence += unit     
-             
-     #String manipulation before being proccesed by the dictionary     
--    new_sentence=jiwer.Compose([jiwer.SubstituteRegexes({r"-": r" ",}),jiwer.RemovePunctuation(), jiwer.ToUpperCase()])(new_sentence).split()
-+    new_sentence=jiwer.Compose([jiwer.SubstituteRegexes({r"—": r" ", r"-": r" ",r"’s": r"'s",r"[.!?,\“\”;:‘’\[\]\(\)]": r""}), jiwer.ToUpperCase()])(new_sentence).split()
-     
-     #Transform the words into sequences of phones
-     phones = []
-     for n in new_sentence:
--        p = pron_dct[n]
--        phones.append(p)
--    return np.array([np.array([phoneme_inventory.index(phone) for phone in word_phone], dtype=np.int32) for word_phone in phones], dtype=np.int32)
-+        try:
-+            p = pron_dct[n]
-+            phones.append(p)
-+        except KeyError as e:
-+            logging.warning('Dictionary error for the word %s in the phrase: %s', e, sentence)
-+    return np.array([ phoneme_inventory.index(phone) for word_phone in phones for phone in word_phone], dtype=np.int64)
- 
- class TextTransform(object):
-     def __init__(self):
-
-['/home/christian/storage/EMG-christian/silent_speech-main/speech_recognition/recognition_model.py', '--output_directory', './models/recognition_model/']
-Dictionary error for the word 'BENEA' in the phrase: The latter led to ano ther broad passage, and, just as we reached it, we heard  the sound of running feet and  the shouting of two voices, one answering  the o ther, from  the floor on which we were and from  the one benea th.
-output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-train / dev split: 8055 200
-Dictionary error for the word 'THORITIES' in the phrase: I did not succeed in getting a glimpse of  the common, for even Horsell and Chobham church towers were in  the hands of  the military au thorities.
-Dictionary error for the word 'STREA' in the phrase: I have been out to Strea tham since I saw you last, but I did not call at  the house.
-Dictionary error for the word 'ZENI' in the phrase: It was starlight and I explained  the Signs of  the Zodiac to her, and pointed out Mars, a bright dot of light creeping zeni thward, towards which so many telescopes were pointed.
-Dictionary error for the word 'THWARD' in the phrase: It was starlight and I explained  the Signs of  the Zodiac to her, and pointed out Mars, a bright dot of light creeping zeni thward, towards which so many telescopes were pointed.
-Dictionary error for the word 'THUSIASTIC' in the phrase: He came up to  the fence and extended a handful of strawberries, for his gardening was as generous as it was en thusiastic.
-Dictionary error for the word 'MRS' in the phrase: Bo th Mr. and Mrs. Rucastle expressed a delight at  the look of it, which seemed quite exaggerated in its vehemence.
-Dictionary error for the word 'MRS' in the phrase: I am glad to see  that Mrs. Hudson has had  the good sense to light  the fire.
-Dictionary error for the word 'LIGH' in the phrase: This intense heat  they project in a parallel beam against any object  they choose, by means of a polished parabolic mirror of unknown composition, much as  the parabolic mirror of a ligh thouse projects a beam of light.
-Dictionary error for the word 'THOUSE' in the phrase: This intense heat  they project in a parallel beam against any object  they choose, by means of a polished parabolic mirror of unknown composition, much as  the parabolic mirror of a ligh thouse projects a beam of light.
-Dictionary error for the word 'THERHEAD' in the phrase: I was too shaken to go to bed again, however, so I dressed, and as soon as it was daylight I slipped down, got a dog-cart at  the ‘Crown Inn,’ which is opposite, and drove to Lea therhead, from whence I have come on  this morning wi th  the one object of seeing you and asking your advice.”
-Dictionary error for the word 'THDRAWN' in the phrase: Then it was wi thdrawn as suddenly as it appeared, and all was dark again save  the single lurid spark which marked a chink between  the stones.
-Dictionary error for the word 'THERWISE' in the phrase: “But Mr. Fowler being a persevering man, as a good seaman should be, blockaded  the house, and, having met you, succeeded by certain arguments, metallic or o therwise, in convincing you  that your interests were  the same as his.”
-Dictionary error for the word 'THERED' in the phrase: “The fur ther points,  that he is middle-aged,  that his hair is grizzled,  that it has been recently cut, and  that he uses lime-cream, are all to be ga thered from a close examination of  the lower part of  the lining.
-Dictionary error for the word 'MÉTIER' in the phrase: Therein lies my  métier , and it is just possible  that it may be of some service in  the investigation which lies before us.
-Dictionary error for the word 'THERHEAD' in the phrase: It is a curious  thing  that I felt angry wi th my wife; I cannot account for it, but my impotent desire to reach Lea therhead worried me excessively.
-Dictionary error for the word 'THERHEAD' in the phrase: It is a curious  thing  that I felt angry wi th my wife; I cannot account for it, but my impotent desire to reach Lea therhead worried me excessively.
-Dictionary error for the word 'TOGE' in the phrase: It hung toge ther in banks, flowing sluggishly down  the slope of  the land and driving reluctantly before  the wind, and very slowly it combined wi th  the mist and moisture of  the air, and sank to  the ear th in  the form of dust.
-Dictionary error for the word 'MCCAR' in the phrase: Does it not strike you as a little singular  that  this McCar thy, who appears to have had little of his own, and to have been under such obligations to Turner, should still talk of marrying his son to Turner’s daughter, who is, presumably, heiress to  the estate, and  that in such a very cocksure manner, as if it were merely a case of a proposal and all else would follow?
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word 'WRI' in the phrase: I have felt like one of  those poor rabbits when  the snake is wri thing towards it.
-Dictionary error for the word 'ALOO' in the phrase: Aloo!”—and in ano ther minute it was wi th its companion, half a mile away, stooping over some thing in  the field.
-Dictionary error for the word 'ALOO' in the phrase: Aloo!”—and in ano ther minute it was wi th its companion, half a mile away, stooping over some thing in  the field.
-Dictionary error for the word 'MCCAR' in the phrase: Bo th  these witnesses depose  that Mr. McCar thy was walking alone.
-Dictionary error for the word 'THORITIES' in the phrase: The body exhibited no traces of violence, and  there can be no doubt  that  the deceased had been  the victim of an unfortunate accident, which should have  the effect of calling  the attention of  the au thorities to  the condition of  the river-side landing-stages.’”
-Dictionary error for the word 'THWI' in the phrase: For thwi th  the six guns which, unknown to anyone on  the right bank, had been hidden behind  the outskirts of  that village, fired simultaneously.
-Dictionary error for the word 'BENEA' in the phrase: Beside  this table, on  the wooden chair, sat Dr. Grimesby Roylott, clad in a long gray dressing-gown, his bare ankles protruding benea th, and his feet  thrust into red heelless Turkish slippers.
-Dictionary error for the word 'STREA' in the phrase: Wi th  these I journeyed down to Strea tham, and saw  that  they exactly fitted  the tracks.”
-Dictionary error for the word 'THERING' in the phrase: For a long time he remained  there, turning over  the leaves and dried sticks, ga thering up what seemed to me to be dust into an envelope, and examining wi th his lens not only  the ground, but even  the bark of  the tree as far as he could reach.
-Dictionary error for the word 'THORITIES' in the phrase: And meanwhile  the military and naval au thorities, now fully alive to  the tremendous power of  their antagonists, worked wi th furious energy.
-Dictionary error for the word 'SERIOCOMIC' in the phrase: The seriocomic periodical  Punch , I remember, made a happy use of it in  the political cartoon.
-Dictionary error for the word 'BENEA' in the phrase: This barricaded door corresponded clearly wi th  the shuttered window outside, and yet I could see by  the glimmer from benea th it  that  the room was not in darkness.
-Dictionary error for the word 'THODS' in the phrase: On glancing over my notes of  the seventy odd cases in which I have during  the last eight years studied  the me thods of my friend Sherlock Holmes, I find many tragic, some comic, a large number merely strange, but none commonplace; for, working as he did ra ther for  the love of his art  than for  the acquirement of weal th, he refused to associate himself wi th any investigation which did not tend towards  the unusual, and even  the fantastic.
-Dictionary error for the word 'SERIOCOMIC' in the phrase: The seriocomic periodical  Punch , I remember, made a happy use of it in  the political cartoon.
-Dictionary error for the word 'SERIOCOMIC' in the phrase: The seriocomic periodical  Punch , I remember, made a happy use of it in  the political cartoon.
-Dictionary error for the word 'TOGE' in the phrase: Everybody yelled toge ther, and  the guns were reloaded in frantic haste.
-Dictionary error for the word 'TOGE' in the phrase: Toge ther we rushed into  the room.
-Dictionary error for the word 'THAMPTON' in the phrase: They were busy making  the necessary arrangements to alter  the route of  the Sou thampton and Portsmou th Sunday League excursions.
-Dictionary error for the word 'PORTSMOU' in the phrase: They were busy making  the necessary arrangements to alter  the route of  the Sou thampton and Portsmou th Sunday League excursions.
-Dictionary error for the word 'THAMPTON' in the phrase: They were busy making  the necessary arrangements to alter  the route of  the Sou thampton and Portsmou th Sunday League excursions.
-Dictionary error for the word 'PORTSMOU' in the phrase: They were busy making  the necessary arrangements to alter  the route of  the Sou thampton and Portsmou th Sunday League excursions.
-Dictionary error for the word 'BTWO' in the phrase: Mr. Henry Baker can have  the same by applying at 6.30  this evening at 221B, Baker Street.’ That is clear and concise.”
-Dictionary error for the word 'ALTOGE' in the phrase: Altoge ther one hundred and sixteen were in position or being hastily placed, chiefly covering London.
-Dictionary error for the word 'TOGE' in the phrase: You see all  these isolated facts, toge ther wi th many minor ones, all pointed in  the same direction.”
-Dictionary error for the word "THERLAND'S" in the phrase: A flush stole over Miss Su therland’s face, and she picked nervously at  the fringe of her jacket.
-Dictionary error for the word 'THERLEY' in the phrase: Seeing  that his passion was becoming ungovernable, I left him, and returned towards Ha therley Farm.
-Dictionary error for the word 'THERHEAD' in the phrase: Had it not been for my promise to  the innkeeper, she would, I  think, have urged me to stay in Lea therhead  that night.
-Dictionary error for the word 'THERHEAD' in the phrase: Had it not been for my promise to  the innkeeper, she would, I  think, have urged me to stay in Lea therhead  that night.
-Dictionary error for the word 'ENCYCLOPÆDIA' in the phrase: “You see, Watson,” he explained, in  the early hours of  the morning, as we sat over a glass of whiskey-and-soda in Baker Street, “it was perfectly obvious from  the first  that  the only possible object of  this ra ther fantastic business of  the advertisement of  the League, and  the copying of  the ‘Encyclopædia,’ must be to get  this not over-bright pawnbroker out of  the way for a number of hours every day.
-Dictionary error for the word 'MCCAR' in the phrase: Young McCar thy must be got off, however.”
-Dictionary error for the word 'THWEST' in the phrase: Before  the guns on  the Richmond and Kingston line of hills began,  there was a fitful cannonade far away in  the sou thwest, due, I believe, to guns being fired haphazard before  the black vapour could overwhelm  the gunners.
-Dictionary error for the word 'THWEST' in the phrase: Before  the guns on  the Richmond and Kingston line of hills began,  there was a fitful cannonade far away in  the sou thwest, due, I believe, to guns being fired haphazard before  the black vapour could overwhelm  the gunners.
-Dictionary error for the word 'BENEA' in the phrase: The peculiar V-shaped mou th wi th its pointed upper lip,  the absence of brow ridges,  the absence of a chin benea th  the wedgelike lower lip,  the incessant quivering of  this mou th,  the Gorgon groups of tentacles,  the tumultuous brea thing of  the lungs in a strange atmosphere,  the evident heaviness and painfulness of movement due to  the greater gravitational energy of  the ear th—above all,  the extraordinary intensity of  the immense eyes—were at once vital, intense, inhuman, crippled and monstrous.
-Dictionary error for the word 'WEDGELIKE' in the phrase: The peculiar V-shaped mou th wi th its pointed upper lip,  the absence of brow ridges,  the absence of a chin benea th  the wedgelike lower lip,  the incessant quivering of  this mou th,  the Gorgon groups of tentacles,  the tumultuous brea thing of  the lungs in a strange atmosphere,  the evident heaviness and painfulness of movement due to  the greater gravitational energy of  the ear th—above all,  the extraordinary intensity of  the immense eyes—were at once vital, intense, inhuman, crippled and monstrous.
-Dictionary error for the word 'BENEA' in the phrase: The peculiar V-shaped mou th wi th its pointed upper lip,  the absence of brow ridges,  the absence of a chin benea th  the wedgelike lower lip,  the incessant quivering of  this mou th,  the Gorgon groups of tentacles,  the tumultuous brea thing of  the lungs in a strange atmosphere,  the evident heaviness and painfulness of movement due to  the greater gravitational energy of  the ear th—above all,  the extraordinary intensity of  the immense eyes—were at once vital, intense, inhuman, crippled and monstrous.
-Dictionary error for the word 'WEDGELIKE' in the phrase: The peculiar V-shaped mou th wi th its pointed upper lip,  the absence of brow ridges,  the absence of a chin benea th  the wedgelike lower lip,  the incessant quivering of  this mou th,  the Gorgon groups of tentacles,  the tumultuous brea thing of  the lungs in a strange atmosphere,  the evident heaviness and painfulness of movement due to  the greater gravitational energy of  the ear th—above all,  the extraordinary intensity of  the immense eyes—were at once vital, intense, inhuman, crippled and monstrous.
-Dictionary error for the word "ELEY'S" in the phrase: An Eley’s No.
-Dictionary error for the word 'WRI' in the phrase: It seemed indeed as if  the whole country in  that direction was on fire—a broad hillside set wi th minute tongues of flame, swaying and wri thing wi th  the gusts of  the dying storm, and  throwing a red reflection upon  the cloud scud above.
-Dictionary error for the word 'WRI' in the phrase: It seemed indeed as if  the whole country in  that direction was on fire—a broad hillside set wi th minute tongues of flame, swaying and wri thing wi th  the gusts of  the dying storm, and  throwing a red reflection upon  the cloud scud above.
-Dictionary error for the word 'ROYLOTTS' in the phrase: “My name is Helen Stoner, and I am living wi th my step-fa ther, who is  the last survivor of one of  the oldest Saxon families in England,  the Roylotts of Stoke Moran, on  the western border of Surrey.”
-Dictionary error for the word 'BEQUEA' in the phrase: She had a considerable sum of money—not less  than pound 1000 a year—and  this she bequea thed to Dr. Roylott entirely while we resided wi th him, wi th a provision  that a certain annual sum should be allowed to each of us in  the event of our marriage.
-Dictionary error for the word 'THUSIASM' in the phrase: “You have done well indeed!” cried Holmes, wi th en thusiasm.
-Dictionary error for the word 'THWESTWARD' in the phrase: The officers who were not actively engaged stood and stared over  the treetops sou thwestward, and  the men digging would stop every now and again to stare in  the same direction.
-Dictionary error for the word 'THWESTWARD' in the phrase: The officers who were not actively engaged stood and stared over  the treetops sou thwestward, and  the men digging would stop every now and again to stare in  the same direction.
-Dictionary error for the word 'THYST' in the phrase: He held out his snuffbox of old gold, wi th a great ame thyst in  the centre of  the lid.
-Dictionary error for the word 'SCALDINGLY' in the phrase: In ano ther moment a huge wave, like a muddy tidal bore but almost scaldingly hot, came sweeping round  the bend upstream.
-Dictionary error for the word 'SIRENLIKE' in the phrase: They communicated wi th one ano ther by means of sirenlike howls, running up and down  the scale from one note to ano ther.
-Dictionary error for the word 'LENG' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'THENED' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'LENG' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'THENED' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'BENEA' in the phrase: When it was concluded he settled our new acquaintance upon  the sofa, placed a pillow benea th his head, and laid a glass of brandy-and-water wi thin his reach.
-Dictionary error for the word 'THODS' in the phrase: You know my me thods.
-Dictionary error for the word '/FOUR' in the phrase: It is in New Zealand stock, paying 4-1/2 per cent.
-Dictionary error for the word "RUCASTLE'S" in the phrase: There was  the huge famished brute, its black muzzle buried in Rucastle’s  throat, while he wri thed and screamed upon  the ground.
-Dictionary error for the word 'WRI' in the phrase: There was  the huge famished brute, its black muzzle buried in Rucastle’s  throat, while he wri thed and screamed upon  the ground.
-Dictionary error for the word 'THEAST' in the phrase: Far away to  the sou theast, marking  the quiet, we heard  the Martians hooting to one ano ther, and  then  the air quivered again wi th  the distant  thud of  their guns.
-Dictionary error for the word 'BENEA' in the phrase: His brows were drawn into two hard, black lines, while his eyes shone out from benea th  them wi th a steely glitter.
-Dictionary error for the word 'THERLEY' in the phrase: Victor Ha therley, hydraulic engineer, 16A, Victoria Street (3d floor).” That was  the name, style, and abode of my morning visitor.
-Dictionary error for the word 'ASIXTEEN' in the phrase: Victor Ha therley, hydraulic engineer, 16A, Victoria Street (3d floor).” That was  the name, style, and abode of my morning visitor.
-Dictionary error for the word 'DTHREE' in the phrase: Victor Ha therley, hydraulic engineer, 16A, Victoria Street (3d floor).” That was  the name, style, and abode of my morning visitor.
-Dictionary error for the word 'FAIRBANK' in the phrase: Fairbank was a good-sized square house of white stone, standing back a little from  the road.
-Dictionary error for the word 'THERING' in the phrase: The driving clouds of  the ga thering  thunderstorm mingled  there wi th masses of black and red smoke.
-Dictionary error for the word "WESTAWAY'S" in the phrase: “There is a well-known agency for governesses in  the West End called Westaway’s, and  there I used to call about once a week in order to see whe ther any thing had turned up which might suit me.
-Dictionary error for the word 'LENG' in the phrase: Sherlock Holmes sat moodily at one side of  the fireplace cross-indexing his records of crime, while I at  the o ther was deep in one of Clark Russell’s fine sea-stories, until  the howl of  the gale from wi thout seemed to blend wi th  the text, and  the splash of  the rain to leng then out into  the long swash of  the sea waves.
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word '*' in the phrase: *       *       *       *       *
-Dictionary error for the word 'DOTTELS' in the phrase: Sherlock Holmes was, as I expected, lounging about his sitting-room in his dressing-gown, reading  the agony column of  The Times , and smoking his before-breakfast pipe, which was composed of all  the plugs and dottels left from his smokes of  the day before, all carefully dried and collected on  the corner of  the mantel-piece.
-Dictionary error for the word 'THOUSE' in the phrase: The same post brought me a letter from Wes thouse & Marbank, of Fenchurch Street, to say  that  the description tallied in every respect wi th  that of  their employé, James Windibank.
-Dictionary error for the word '&' in the phrase: The same post brought me a letter from Wes thouse & Marbank, of Fenchurch Street, to say  that  the description tallied in every respect wi th  that of  their employé, James Windibank.
-Dictionary error for the word 'MARBANK' in the phrase: The same post brought me a letter from Wes thouse & Marbank, of Fenchurch Street, to say  that  the description tallied in every respect wi th  that of  their employé, James Windibank.
-Dictionary error for the word 'EMPLOYÉ' in the phrase: The same post brought me a letter from Wes thouse & Marbank, of Fenchurch Street, to say  that  the description tallied in every respect wi th  that of  their employé, James Windibank.
-Dictionary error for the word 'THWARD' in the phrase: The whole population of  the great six-million city was stirring, slipping, running; presently it would be pouring  en masse  nor thward.
-Dictionary error for the word 'PYRFORD' in the phrase: From Ripley until I came  through Pyrford I was in  the valley of  the Wey, and  the red glare was hidden from me.
-Dictionary error for the word 'DTWENTY' in the phrase: John Horner, 26, plumber, was brought up upon  the charge of having upon  the 22d inst.
-Dictionary error for the word 'THODS' in the phrase: So perfect was  the organization of  the society, and so systematic its me thods,  that  there is hardly a case upon record where any man succeeded in braving it wi th impunity, or in which any of its outrages were traced home to  the perpetrators.
-Dictionary error for the word 'THERLAND' in the phrase: James Windibank wished Miss Su therland to be so bound to Hosmer Angel, and so uncertain as to his fate,  that for ten years to come, at any rate, she would not listen to ano ther man.
-Dictionary error for the word 'TOGE' in the phrase: “Why did you come away to consult me in such a hurry?” asked Sherlock Holmes, wi th his finger-tips toge ther, and his eyes to  the ceiling.
-Dictionary error for the word 'GUNWISE' in the phrase: And  then  the Martian beside us raised his tube on high and discharged it, gunwise, wi th a heavy report  that made  the ground heave.
-Dictionary error for the word 'GUNWISE' in the phrase: And  then  the Martian beside us raised his tube on high and discharged it, gunwise, wi th a heavy report  that made  the ground heave.
-Dictionary error for the word 'PYRFORD' in the phrase: The scent of hay was in  the air  through  the lush meadows beyond Pyrford, and  the hedges on ei ther side were sweet and gay wi th multitudes of dog-roses.
-Dictionary error for the word 'PYRFORD' in the phrase: The scent of hay was in  the air  through  the lush meadows beyond Pyrford, and  the hedges on ei ther side were sweet and gay wi th multitudes of dog-roses.
-Dictionary error for the word 'ENCYCLOPÆDIAS' in the phrase: To carry  the art, however, to its highest pitch, it is necessary  that  the reasoner should be able to utilize all  the facts which have come to his knowledge; and  this in itself implies, as you will readily see, a possession of all knowledge, which, even in  these days of free education and encyclopædias, is a somewhat rare accomplishment.
-Dictionary error for the word 'THERHEAD' in the phrase: “Lea therhead!” I shouted above  the sudden noise.
-Dictionary error for the word 'RUCASTLES' in the phrase: I returned  the strange hair to  the drawer, and I said no thing of  the matter to  the Rucastles, as I felt  that I had put myself in  the wrong by opening a drawer which  they had locked.
-Dictionary error for the word 'PYRFORD' in the phrase: As I ascended  the little hill beyond Pyrford Church  the glare came into view again, and  the trees about me shivered wi th  the first intimation of  the storm  that was upon me.
-Dictionary error for the word 'BURNWELL' in the phrase: “And, indeed, I could not wonder  that such a man as Sir George Burnwell should gain an influence over him, for he has frequently brought him to my house, and I have found myself  that I could hardly resist  the fascination of his manner.
-Dictionary error for the word 'MRS' in the phrase: You are endeavoring to trace some geese which were sold by Mrs. Oakshott, of Brixton Road, to a salesman named Breckinridge, by him in turn to Mr. Windigate, of  the ‘Alpha,’ and by him to his club, of which Mr. Henry Baker is a member.”
-Dictionary error for the word 'WINDIGATE' in the phrase: You are endeavoring to trace some geese which were sold by Mrs. Oakshott, of Brixton Road, to a salesman named Breckinridge, by him in turn to Mr. Windigate, of  the ‘Alpha,’ and by him to his club, of which Mr. Henry Baker is a member.”
-Dictionary error for the word 'ANSTRU' in the phrase: “Oh, Anstru ther would do your work for you.
-Dictionary error for the word 'THORITY' in the phrase: Denning, our greatest au thority on meteorites, stated  that  the height of its first appearance was about ninety or one hundred miles.
-Dictionary error for the word 'WESTPHAIL' in the phrase: We had, however, an aunt, my mo ther’s maiden sister, Miss Honoria Westphail, who lives near Harrow, and we were occasionally allowed to pay short visits at  this lady’s house.
-Dictionary error for the word 'THERHEAD' in the phrase: No thing more of  the fighting was known  that night,  the night of my drive to Lea therhead and back.
-Dictionary error for the word 'THERHEAD' in the phrase: No thing more of  the fighting was known  that night,  the night of my drive to Lea therhead and back.
-Dictionary error for the word 'TOGE' in the phrase: We got into a cab toge ther, and away we drove to some lodgings he had taken in Gordon Square, and  that was my true wedding after all  those years of waiting.
-Dictionary error for the word 'THWARD' in the phrase: So you understand  the roaring wave of fear  that swept  through  the greatest city in  the world just as Monday was dawning— the stream of flight rising swiftly to a torrent, lashing in a foaming tumult round  the railway stations, banked up into a horrible struggle about  the shipping in  the Thames, and hurrying by every available channel nor thward and eastward.
-Dictionary error for the word 'THWARD' in the phrase: So you understand  the roaring wave of fear  that swept  through  the greatest city in  the world just as Monday was dawning— the stream of flight rising swiftly to a torrent, lashing in a foaming tumult round  the railway stations, banked up into a horrible struggle about  the shipping in  the Thames, and hurrying by every available channel nor thward and eastward.
-Dictionary error for the word 'SIRENLIKE' in the phrase: They communicated wi th one ano ther by means of sirenlike howls, running up and down  the scale from one note to ano ther.
-Dictionary error for the word 'SIRENLIKE' in the phrase: They communicated wi th one ano ther by means of sirenlike howls, running up and down  the scale from one note to ano ther.
-Dictionary error for the word 'THWARD' in the phrase: And as  the day advanced and  the engine drivers and stokers refused to return to London,  the pressure of  the flight drove  the people in an ever- thickening multitude away from  the stations and along  the nor thward-running roads.
-Dictionary error for the word 'STOPER' in the phrase: “‘If you please, Miss Stoper.’
-Dictionary error for the word 'FT' in the phrase: About 5 ft. 7 in.
-Dictionary error for the word 'BENEA' in the phrase: On  the inside of  the cover was a paper label, wi th  the initials of K. K. K. repeated upon it, and ‘Letters, memoranda, receipts, and a register’ written benea th.
-Dictionary error for the word 'XIV' in the phrase: XIV.
-Dictionary error for the word 'TOGE' in the phrase: “We’ll call a cab and go toge ther.
-Dictionary error for the word 'MRS' in the phrase: Fowler was a very kind-spoken, free-handed gentleman,” said Mrs. Toller, serenely.
-Dictionary error for the word 'MRS' in the phrase: Well, Watson, we are, I fancy, nearing  the end of our quest, and  the only point which remains to be determined is whe ther we should go on to  this Mrs. Oakshott to-night, or whe ther we should reserve it for to-morrow.
-Dictionary error for the word 'LENG' in the phrase: As he spoke he drew  the dog-whip swiftly from  the dead man’s lap, and  throwing  the noose round  the reptile’s neck, he drew it from its horrid perch, and carrying it at arm’s leng th,  threw it into  the iron safe, which he closed upon it.
-Dictionary error for the word 'MCCAR' in the phrase: Mr. McCar thy was  the only man alive who had known dad in  the old days in Victoria.”
-Dictionary error for the word 'COUNTERPANED' in the phrase: A brown chest of drawers stood in one corner, a narrow white-counterpaned bed in ano ther, and a dressing-table on  the left-hand side of  the window.
-Dictionary error for the word 'TOGE' in the phrase: Sherlock Holmes closed his eyes and placed his elbows upon  the arms of his chair, wi th his finger-tips toge ther.
-Dictionary error for the word 'THWI' in the phrase: For thwi th flashes of actual flame, a bright glare leaping from one to ano ther, sprang from  the scattered group of men.
-Dictionary error for the word 'TOGE' in the phrase: On  the four th day after  the new year I heard my fa ther give a sharp cry of surprise as we sat toge ther at  the breakfast-table.
-Dictionary error for the word 'THWEST' in the phrase: A similar tube was handed to each of  the  three, and  the seven proceeded to distribute  themselves at equal distances along a curved line between St. George’s Hill, Weybridge, and  the village of Send, sou thwest of Ripley.
-Dictionary error for the word 'THWEST' in the phrase: A similar tube was handed to each of  the  three, and  the seven proceeded to distribute  themselves at equal distances along a curved line between St. George’s Hill, Weybridge, and  the village of Send, sou thwest of Ripley.
-Dictionary error for the word 'WREA' in the phrase: “Then perhaps you will kindly explain how it is  that we found  this in it?” He opened his bag as he spoke, and tumbled onto  the floor a wedding-dress of watered silk, a pair of white satin shoes, and a bride’s wrea th and veil, all discolored and soaked in water.
-Dictionary error for the word 'THERHEAD' in the phrase: Had it not been for my promise to  the innkeeper, she would, I  think, have urged me to stay in Lea therhead  that night.
-Dictionary error for the word 'THWEST' in the phrase: A similar tube was handed to each of  the  three, and  the seven proceeded to distribute  themselves at equal distances along a curved line between St. George’s Hill, Weybridge, and  the village of Send, sou thwest of Ripley.
-Dictionary error for the word 'OUTRÉ' in the phrase: If we could fly out of  that window hand in hand, hover over  this great city, gently remove  the roofs, and peep in at  the queer  things which are going on,  the strange coincidences,  the plannings,  the cross-purposes,  the wonderful chains of events, working  through generations, and leading to  the most  outré  results, it would make all fiction wi th its conventionalities and foreseen conclusions most stale and unprofitable.”
-Dictionary error for the word 'RÉPERTOIRE' in the phrase: Again I changed my dress, again I sat in  the window, and again I laughed very heartily at  the funny stories of which my employer had an immense  répertoire , and which he told inimitably.
-Dictionary error for the word 'ARAT' in the phrase: “ARAT,” I read.
-Dictionary error for the word 'KNAPHILL' in the phrase: And far away towards Knaphill I saw  the flashes of trees and hedges and wooden buildings suddenly set alight.
-Dictionary error for the word 'KNAPHILL' in the phrase: And far away towards Knaphill I saw  the flashes of trees and hedges and wooden buildings suddenly set alight.
-Dictionary error for the word "BECHER'S" in the phrase: Becher’s.”
-Dictionary error for the word 'SHOLTOS' in the phrase: And yet  this John Openshaw seems to me to be walking amid even greater perils  than did  the Sholtos.”
-Dictionary error for the word 'JEPHRO' in the phrase: “‘Jephro,’ said she, ‘ there is an impertinent fellow upon  the road  there who stares up at Miss Hunter.’
-Dictionary error for the word 'SITTE' in the phrase: When men shall call upon  the mountains and  the rocks to fall upon  them and hide  them—hide  them from  the face of Him  that sitte th upon  the  throne!”
-Dictionary error for the word 'THWARD' in the phrase: So you understand  the roaring wave of fear  that swept  through  the greatest city in  the world just as Monday was dawning— the stream of flight rising swiftly to a torrent, lashing in a foaming tumult round  the railway stations, banked up into a horrible struggle about  the shipping in  the Thames, and hurrying by every available channel nor thward and eastward.
-Dictionary error for the word 'THUMBERLAND' in the phrase: In  the second one which I visited in Nor thumberland Avenue, I learned by an inspection of  the book  that Francis H. Moulton, an American gentleman, had left only  the day before, and on looking over  the entries against him, I came upon  the very items which I had seen in  the duplicate bill.
-Dictionary error for the word 'THINGNESS' in the phrase: The man who escaped at  the former place tells a wonderful story of  the strangeness of its coiling flow, and how he looked down from  the church spire and saw  the houses of  the village rising like ghosts out of its inky no thingness.
-Dictionary error for the word 'MCCAR' in the phrase: No one but Mr. McCar thy was in favor of it.” A quick blush passed over her fresh young face as Holmes shot one of his keen, questioning glances at her.
-Dictionary error for the word 'THWORKS' in the phrase: Yonder, I take it are  the Martians, and Londonward, where  those hills rise about Richmond and Kingston and  the trees give cover, ear thworks are being  thrown up and guns are being placed.
-Dictionary error for the word 'THWORKS' in the phrase: Yonder, I take it are  the Martians, and Londonward, where  those hills rise about Richmond and Kingston and  the trees give cover, ear thworks are being  thrown up and guns are being placed.
-Dictionary error for the word 'STREA' in the phrase: Wi th  this intention, I called a cab, and drove out to my house at Strea tham, carrying  the jewel wi th me.
-Dictionary error for the word 'MCCAR' in the phrase: All was going well when McCar thy laid his grip upon me.
-Dictionary error for the word "MCQUIRE'S" in the phrase: “Frank here and I met in ’84, in McQuire’s camp, near  the Rockies, where pa was working a claim.
-Dictionary error for the word 'THARGY' in the phrase: I heard a slight scraping at  the fence, and rousing myself from  the le thargy  that had fallen upon me, I looked down and saw him dimly, clambering over  the palings.
-Dictionary error for the word 'WRI' in the phrase: For a minute or more  the hand, wi th its wri thing fingers, protruded out of  the floor.
-Dictionary error for the word 'TOGE' in the phrase: Just as I arrived,  the door was opened, and we were shown up toge ther to Holmes’s room.
-Dictionary error for the word 'THWARD' in the phrase: But  the artilleryman dissuaded me: “It’s no kindness to  the right sort of wife,” he said, “to make her a widow”; and in  the end I agreed to go wi th him, under cover of  the woods, nor thward as far as Street Cobham before I parted wi th him.
-Dictionary error for the word 'THWARD' in the phrase: But  the artilleryman dissuaded me: “It’s no kindness to  the right sort of wife,” he said, “to make her a widow”; and in  the end I agreed to go wi th him, under cover of  the woods, nor thward as far as Street Cobham before I parted wi th him.
-Dictionary error for the word 'THERINE' in the phrase: Ca therine Cusack, maid to  the countess, deposed to having heard Ryder’s cry of dismay on discovering  the robbery, and to having rushed into  the room, where she found matters as described by  the last witness.
-Dictionary error for the word 'UNHEAL' in the phrase: There were no carpets and no signs of any furniture above  the ground floor, while  the plaster was peeling off  the walls, and  the damp was breaking  through in green, unheal thy blotches.
-Dictionary error for the word 'SMOO' in the phrase: At last, however,  the bumping of  the road was exchanged for  the crisp smoo thness of a gravel-drive, and  the carriage came to a stand.
-Dictionary error for the word 'THNESS' in the phrase: At last, however,  the bumping of  the road was exchanged for  the crisp smoo thness of a gravel-drive, and  the carriage came to a stand.
-Dictionary error for the word 'THERWISE' in the phrase: I can hardly see how  the lady could have acted o therwise,  though her abrupt me thod of doing it was undoubtedly to be regretted.
-Dictionary error for the word 'THOD' in the phrase: I can hardly see how  the lady could have acted o therwise,  though her abrupt me thod of doing it was undoubtedly to be regretted.
-Dictionary error for the word 'THORITIES' in the phrase: I did not succeed in getting a glimpse of  the common, for even Horsell and Chobham church towers were in  the hands of  the military au thorities.
-Dictionary error for the word 'THORITIES' in the phrase: I did not succeed in getting a glimpse of  the common, for even Horsell and Chobham church towers were in  the hands of  the military au thorities.
-Dictionary error for the word 'UNDERNEA' in the phrase: And undernea th?”
-Dictionary error for the word 'THERWISE' in the phrase: Now, when you see  that a young lady, o therwise neatly dressed, has come away from home wi th odd boots, half-buttoned, it is no great deduction to say  that she came away in a hurry.”
-Dictionary error for the word 'THERHEAD' in the phrase: Then I remembered her cousins at Lea therhead.
-Dictionary error for the word 'STOPER' in the phrase: “‘DEAR MISS HUNTER,—Miss Stoper has very kindly given me your     address, and I write from here to ask you whe ther you have     reconsidered your decision.
-Dictionary error for the word 'GASFITTERS' in the phrase: Then at  the gasfitters’ ball you met, as I understand, a gentleman called Mr. Hosmer Angel.”
-Dictionary error for the word 'XVI' in the phrase: XVI.
-Dictionary error for the word 'XVI' in the phrase: XVI.
-Dictionary error for the word 'FIANCÉ' in the phrase: The man in  the road was, undoubtedly, some friend of hers—possibly her  fiancé —and no doubt, as you wore  the girl’s dress and were so like her, he was convinced from your laughter, whenever he saw you, and afterwards from your gesture,  that Miss Rucastle was perfectly happy, and  that she no longer desired his attentions.
-Dictionary error for the word 'TOGE' in the phrase: “I wish you were all at  the devil toge ther.
-Dictionary error for the word 'THQUAKE' in the phrase: “And suddenly—fire, ear thquake, dea th!”
-Dictionary error for the word 'THQUAKE' in the phrase: “And suddenly—fire, ear thquake, dea th!”
-Dictionary error for the word 'ALTOGE' in the phrase: And  through  the charred and desolated area—perhaps twenty square miles altoge ther— that encircled  the Martian encampment on Horsell Common,  through charred and ruined villages among  the green trees,  through  the blackened and smoking arcades  that had been but a day ago pine spinneys, crawled  the devoted scouts wi th  the heliographs  that were presently to warn  the gunners of  the Martian approach.
-Dictionary error for the word 'ALTOGE' in the phrase: And  through  the charred and desolated area—perhaps twenty square miles altoge ther— that encircled  the Martian encampment on Horsell Common,  through charred and ruined villages among  the green trees,  through  the blackened and smoking arcades  that had been but a day ago pine spinneys, crawled  the devoted scouts wi th  the heliographs  that were presently to warn  the gunners of  the Martian approach.
-Dictionary error for the word 'PYRFORD' in the phrase: But  three certainly came out about eight o’clock and, advancing slowly and cautiously, made  their way  through Byfleet and Pyrford towards Ripley and Weybridge, and so came in sight of  the expectant batteries against  the setting sun.
-Dictionary error for the word 'PYRFORD' in the phrase: But  three certainly came out about eight o’clock and, advancing slowly and cautiously, made  their way  through Byfleet and Pyrford towards Ripley and Weybridge, and so came in sight of  the expectant batteries against  the setting sun.
-Dictionary error for the word 'DUSTCOAT' in the phrase: In spite of  the light brown dustcoat and lea ther-leggings which he wore in deference to his rustic surroundings, I had no difficulty in recognizing Lestrade, of Scotland Yard.
-Dictionary error for the word 'LENG' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'THENED' in the phrase: The silence was restored;  the minute leng thened to  three.
-Dictionary error for the word 'THWEST' in the phrase: A few seconds after midnight  the crowd in  the Chertsey road, Woking, saw a star fall from heaven into  the pine woods to  the nor thwest.
-Dictionary error for the word 'THWEST' in the phrase: A few seconds after midnight  the crowd in  the Chertsey road, Woking, saw a star fall from heaven into  the pine woods to  the nor thwest.
diff --git a/speech_recognition/data_utils.py b/speech_recognition/data_utils.py
index 7c5c270..6da6db0 100644
--- a/speech_recognition/data_utils.py
+++ b/speech_recognition/data_utils.py
@@ -251,7 +251,7 @@ def read_phonemes(sentence):
             p = pron_dct[n]
             phones.append(p)
         except KeyError as e:
-            logging.warning('Dictionary error for the word %s in the phrase: %s', e, sentence)
+            logging.warning(e)
     return np.array([ phoneme_inventory.index(phone) for word_phone in phones for phone in word_phone], dtype=np.int64)
 
 class TextTransform(object):
diff --git a/speech_recognition/recognition_model.py b/speech_recognition/recognition_model.py
index 7db967e..9ca25b6 100644
--- a/speech_recognition/recognition_model.py
+++ b/speech_recognition/recognition_model.py
@@ -143,7 +143,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
             #Increment counter and print the loss training       
             batch_idx += 1
 
-
+            '''
             if batch_idx % report_every == 0:     
                 #Evaluation
                 model.eval()
@@ -177,14 +177,14 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
                         #just for now
                         if idx == 10:
                             break
-
+                
                 #Writing on tensorboard
                 writer.add_scalar('Loss/Evaluation', eval_loss / run_steps, batch_idx)
                 writer.add_scalar('Loss/Training', train_loss / run_steps, batch_idx)
                 train_loss= 0
                 eval_loss= 0
                 run_steps=0
-
+                '''
         #Testing and change learning rate
         #val = test(model, devset, device)
         #writer.add_scalar('WER/Evaluation',val, batch_idx)
@@ -194,7 +194,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
         train_loss = np.mean(losses)
         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f}')
         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-
+        return
     model.load_state_dict(torch.load(os.path.join(FLAGS.output_directory,'model.pt'))) # re-load best parameters
     return model
 

['/home/christian/storage/EMG-christian/silent_speech-main/speech_recognition/recognition_model.py', '--output_directory', './models/recognition_model/']
'BENEA'
output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
train / dev split: 8055 200
'PYRFORD'
'THEST'
'THOUSE'
'&'
'MARBANK'
'WESTPHAIL'
'LUMINIUM'
'THWARD'
'THAMPTON'
'DD'
'WREA'
'MÉTIER'
'ISLEWOR'
'ISLEWOR'
'THERY'
'THLESSLY'
'MRS'
'THWI'
'THWI'
'THUMBERLAND'
'THERHEAD'
'BURNWELL'
'THEART'
'MCCAR'
'THERLEY'
'THILY'
'UNCOU'
'RÉPERTOIRE'
'PAINSHILL'
'THERING'
'THERING'
'WRI'
'MRS'
'THERLEY'
'MRS'
'THWARD'
"MCQUIRE'S"
'THWARD'
'THWARD'
'DTWENTY'
'TOGE'
'TOGE'
'THDREW'
'WREA'
'BENEA'
'THODS'
'ZENI'
'THWARD'
'THEART'
'TOGE'
'TOGE'
'LOTLL'
'LOTLL'
'TOGE'
'ROYLOTTS'
'BENEA'
'WEDGELIKE'
'BENEA'
'WEDGELIKE'
"THERTON'S"
'TOGE'
'TOGE'
'DÉNOUEMENT'
'THERINE'
'GROUNDLL'
'GROUNDLL'
'THWI'
'THWI'
'SWANDAM'
'BURNWELL'
'TOGE'
'BENEA'
'THERED'
'THERED'
'XIV'
'THWARD'
'THWARD'
'THDREW'
'THDREW'
'BURNWELL'
'SWANDAM'
'THYST'
'MRS'
'THORITIES'
'THERLEY'
'MCCAR'
'THWARD'
'THWARD'
'THERED'
'THERED'
'MCCAR'
'BROADCLO'
'SWANDAM'
'ALTOGE'
'ALTOGE'
'MCCAR'
'MCCAR'
'MRS'
'THARGY'
'MRS'
'AONE'
'GRANDFA'
'TOGE'
'WRI'
'WRI'
"OPENSHAW'S"
'THERED'
"BACKWATER'S"
'THQUAKE'
'THQUAKE'
'THARGY'
'THARGY'
'THERHEAD'
'MCCAR'
'THEDRAL'
'THWAY'
'THWEST'
'PAINSHILL'
'PAINSHILL'
'ZENI'
'THWARD'
'ZENI'
'THWARD'
'THWEST'
'THWEST'
'GASFITTERS'
'THQUAKES'
'THQUAKES'
'STREA'
'LENG'
'THWI'
'MRS'
'THERHEAD'
'THERHEAD'
'ACETONES'
'UNFA'
'THOMABLE'
'THWI'
'MCCAR'
'DYOU'
'STOPER'
'BENEA'
'MCCAULEY'
'THERHEAD'
'THERHEAD'
'MRS'
'MRS'
'THWARD'
'SCALDINGLY'
'SCALDINGLY'
'SWANDAM'
'MCCAR'
'THORISED'
'MERRYWEA'
'NOTWI'
'THSTANDING'
'THERHEAD'
'FT'
'TOGE'
'MRS'
'TOGE'
'RUCASTLES'
'MRS'
'SWANDAM'
'SMO'
'THERED'
'SMO'
'THERED'
'SYMPA'
'THUSIASM'
'THERLEY'
'SCALDINGLY'
'THDRAWN'
'GUNWISE'
'GUNWISE'
'THUSIASTIC'
'THUSIASTIC'
'THERLAND'
'THWEST'
'THWEST'
'THLESS'
'WRI'
'MCCAR'
"BECHER'S"
'MRS'
'MCCAR'
'THOUSES'
"RUCASTLE'S"
'WRI'
'BLACKSMI'
'TOGE'
'THERHEAD'
'&'
'SOJERSLL'
'SOJERSLL'
'THERLEY'
'THWARD'
'TOGE'
'MRS'
'MRS'
'AVENT'
'AVENT'
'THERED'
'ALOO'
'ALOO'
'DYOU'
'WRI'
'THFULLY'
'THWI'
'THQUAKE'
"THERLEY'S"
'TOGE'
'GASFITTERS'
'MRS'
'MCCAR'
'THERLEY'
'THERINE'
'THOD'
'TOGE'
'MCCAR'
'CARBOLIZED'
'BENEA'
'RUCASTLES'
'THLESS'
'THORISED'
'THORISED'
'THQUAKE'
'THQUAKE'
'THERLEY'
'DTWENTY'
'TOGE'
'HEADLIKE'
'THWARD'
'THWARD'
'THERLAND'
'THERLEY'
'BURNWELL'
'SERIOCOMIC'
'SERIOCOMIC'
'THWARD'
'HAGGERSTON'
'PYRFORD'
'WINDIGATE'
'THORITY'
'THORITY'
'TOGE'
'TOGE'
'TOGE'
'THERING'
'NOTWI'
'THSTANDING'
'NOTWI'
'THSTANDING'
'BENEA'
'PYRFORD'
'PYRFORD'
'THERHEAD'
'THWI'
'THERHEAD'
'THWI'
'THERWISE'
'MEREDI'
'ALTOGE'
'LENG'
'THENED'
'*'
'*'
'*'
'*'
'*'
'MRS'
'SWANDAM'
'MRS'
'THERLAND'
'ALTOGE'
'STREA'
'THWARD'
'THOD'
'MCCAR'
'MCCAR'
'ALTOGE'
'STOPER'
'SNOWCAPS'
'THERHEAD'
'ISLEWOR'
'THEMATICAL'
'THEMATICAL'
'THWARD'
'THWARD'
'KNAPHILL'
'BIRCHMOOR'
'RUCASTLES'
'THODS'
'THUSIASM'
'MERRYWEA'
'SITTE'
'TOGE'
'THORITIES'
'THOMED'
'MCCAR'
"THY'S"
'THERING'
'SIRENLIKE'
'BISULPHATE'
'BARYTA'
'THERHEAD'
'THERHEAD'
'DYOU'
'THODICALLY'
'THODICALLY'
'SITTE'
'SITTE'
'SNOWCAPS'
'SNOWCAPS'
'STOPER'
'THERY'
'THERY'
'TOGE'
'THWI'
'TOGE'
'THERHEAD'
'THAMPTON'
'PORTSMOU'
'THAMPTON'
'PORTSMOU'
'PÂTÉ'
'THWI'
'TABLECLO'
'TABLECLO'
'COUNTERPANED'
'LENG'
'THILY'
'MCCAR'
'MRS'
'THERING'
'THDRAW'
'TOGE'
'TOGE'
'ALTOGE'
'PYRFORD'
'*'
'*'
'*'
'*'
'*'
'JEPHRO'
'THLESSLY'
'THLESSLY'
'SWANDAM'
'MRS'
'ALTOGE'
'ALTOGE'
'FAIRBANK'
"RUCASTLE'S"
'TOGE'
'MCCAR'
'TOGE'
'THERWISE'
'THFULLY'
'JEPHRO'
'THERED'
'XVI'
'XVI'
'MRS'
'SIRENLIKE'
'SIRENLIKE'
'TOGE'
'TOGE'
'MRS'
'WINDIGATE'
'MRS'
'THORITIES'
'MRS'
'MCCAR'
'PYRFORD'
'PYRFORD'
'THLESSLY'
'THAMPTON'
'STOPER'
'CŒUR'
'THODS'
'THERLEY'
'CÉLÈBRES'
'THWARD'
'THWARD'
'SYMPA'
'TOGE'
'TOGE'
'THWARD'
'THINGNESS'
'THERLAND'
'THWESTWARD'
'TOGE'
'TOGE'
'ARAT'
'BENEA'
'THODS'
'MCCAR'
'MCCAR'
'THORITIES'
'ALTOGE'
'ALTOGE'
'ALTOGE'
'ALTOGE'
'WRI'
'THWI'
'THWI'
'SYMPA'
'MRS'
'ENCYCLOPÆDIAS'
'THORITIES'
'THORITIES'
'THEMATICAL'
'BTWO'
'SERIOCOMIC'
'BENEA'
'BENEA'
'LENG'
'SOJERSLL'
'THWI'
'THWI'
'TOLLERS'
"LESTRADE'S"
'MCCAR'
"THY'S"
'MCCAR'
'SMOO'
'THERLAND'
'THQUAKES'
'THODS'
'TOGE'
'PATERSONS'
'UFFA'
'THEST'
'ENCYCLOPÆDIA'
'THODICALLY'
'FIANCÉ'
'THUSIASTIC'
'TOGE'
'THORITIES'
'THORITIES'
'THWESTWARD'
'THWESTWARD'
'SMO'
'THERED'
'THERLEY'
'MERRYWEA'
'TOGE'
"RUCASTLE'S"
'ALTOGE'
'ALTOGE'
'PYRFORD'
'PYRFORD'
'PAINSHILL'
'THORITIES'
'ALOO'
'ALOO'
'THWARD'
'THWARD'
'THERLAND'
'SMOO'
'THNESS'
'MRS'
'THOD'
'MCCAR'
"THY'S"
'THERLEY'
'THEART'
'THERHEAD'
'THERHEAD'
'THQUAKE'
'THERLAND'
'GUNWISE'
'KNAPHILL'
'KNAPHILL'
'THOUSES'
'THOUSES'
'MCCAR'
'THORITIES'
'THORITIES'
'LUMINIUM'
'LUMINIUM'
'HEADLIKE'
'HEADLIKE'
'MCCAR'
'MCCAR'
"THY'S"
'UNFA'
'THOMABLE'
'UNFA'
'THOMABLE'
'BURNWELL'
'PHILAN'
'THROPIST'
'STOPER'
'MRS'
'THEREGE'
'THUSIASTIC'
'TOGE'
'SMOO'
'PORTSMOU'
'PORTSMOU'
'BENEA'
'THERHEAD'
'THERING'
'LENG'
'THENED'
'LENG'
'THENED'
'THWI'
'THWI'
'MRS'
'MRS'
'THERHEAD'
'THERHEAD'
'MRS'
'DOTTELS'
'THWI'
'THQUAKE'
'THWI'
'THQUAKE'
'UNCLASPINGS'
'MRS'
'MRS'
'MRS'
'FAIRBANK'
'THERED'
'THERED'
'WRI'
'DÉTOUR'
'LHOMME'
'LOEUVRE'
'THFULLY'
'THERHEAD'
'THERHEAD'
'THERHEAD'
'GUNLIKE'
'SYMPA'
'THETIC'
'THWEST'
'TOGE'
'XVI'
'THERED'
'THERED'
'THDRAWN'
'THDRAWN'
'THERED'
"RUCASTLE'S"
'PYRFORD'
'PYRFORD'
'TABLECLO'
'TOGE'
'TOGE'
'AVENT'
'THERHEAD'
'MCCAR'
'THERLEY'
'ALTOGE'
'LENG'
'LENG'
'THENED'
'MERRYWEA'
'PYRFORD'
'PYRFORD'
'GUNLIKE'
'GUNLIKE'
'ANSTRU'
'THERING'
'THERING'
'XIV'
'XIV'
'THOUSE'
'&'
'MARBANK'
'EMPLOYÉ'
'THERHEAD'
'SMI'
'SMI'
'THERTO'
'DUSTCOAT'
'PYRFORD'
'MRS'
'WINDIGATE'
'MCCAR'
'MCCAR'
'TOGE'
'ALTOGE'
'THERING'
'THERING'
'LABYRIN'
'PROOSIA'
'SWANDAM'
'ENCYCLOPÆDIA'
'THERLAND'
'MCCAR'
'THSOME'
'TOGE'
'THWARD'
'THWARD'
'THWARD'
'TOGE'
'*'
'*'
'*'
'*'
'*'
'TOGE'
'MCCAR'
'THERING'
'PYRFORD'
'THWI'
'THWI'
'*'
'*'
'*'
'*'
'*'
'THERED'
'BENEA'
'WEDGELIKE'
'THORITIES'
'THORITIES'
'OUTRÉ'
'BENEA'
'THODS'
'TOGE'
'BROADCLO'
'BROADCLO'
'MRS'
'THWARD'
'THWARD'
'CASEFUL'
'TOGE'
'LIGH'
'THOUSE'
'LIGH'
'THOUSE'
'THORITIES'
'LABYRIN'
'THORITIES'
'WRI'
'ALOO'
'STOPER'
'PYRFORD'
'ALTOGE'
'ALTOGE'
'TOGE'
'THWI'
'GROUNDLL'
'MRS'
'THARGY'
'OCT'
'ALOO'
'MRS'
'*'
'*'
'*'
'*'
'*'
'THWARD'
'HAGGERSTON'
'THWARD'
'HAGGERSTON'
'MRS'
'THWARD'
'THERLAND'
'TOGE'
'THWORKS'
'THWORKS'
'TOGE'
'BEQUEA'
'PYRFORD'
'MCCAR'
'THORITIES'
'THORITIES'
'TOGE'
'&'
'THESON'
'ALTOGE'
'THAMPTON'
'STREA'
'THWORKS'
'BENEA'
'THEAST'
'THEAST'
'THERING'
'THERING'
'TOGE'
'RUCASTLES'
'MCCAR'
'THERING'
'THWI'
'MCCAR'
'THWARD'
'THORITIES'
'THERWISE'
'THOD'
'BURNWELL'
'THWEST'
'PAINSHILL'
'PAINSHILL'
'MCCAR'
'WREA'
'THERHEAD'
'THWI'
'MCCAULEY'
'SYMPA'
'OUTRÉ'
'MCCAR'
'LENG'
'BENEA'
'LIGH'
'THOUSE'
'BELLYINGS'
'WRI'
'BELLYINGS'
'WRI'
'THERHEAD'
'THERHEAD'
'THDRAWN'
'THAMPTON'
'PORTSMOU'
'THEAST'
'THORITY'
'MRS'
'THERLEY'
'BELLYINGS'
'WRI'
'THERHEAD'
'THERHEAD'
'THWEST'
'THWEST'
'THERING'
'THERHEAD'
'TOGE'
'THERHEAD'
'WRITIST'
'THERWISE'
'THOD'
'TOGE'
'MRS'
'THLESS'
'THLESS'
'THERHEAD'
'THERHEAD'
'TOGE'
'STREA'
'TOGE'
'ROYLOTTS'
'WRI'
'*'
'*'
'*'
'*'
'*'
'THORITY'
'FOOTPA'
'FOOTPA'
'SMOO'
'MCCAR'
'THERED'
'THERED'
'SHOLTOS'
'THLESS'
'FOOTPA'
'PYRFORD'
'PYRFORD'
'MRS'
"THERLAND'S"
'THERLEY'
'PYRFORD'
'PYRFORD'
'THERLEY'
'ASIXTEEN'
'DTHREE'
'THERING'
'THERING'
'UNHEAL'
'MCCAR'
'THWORKS'
'THWORKS'
'CARRÉE'
"WESTAWAY'S"
'MCCAR'
'ALTOGE'
'THQUAKE'
'LOTLL'
'MRS'
"ELEY'S"
'THORITIES'
'THORITIES'
'THWORKS'
'PORTSMOU'
'MRS'
'MRS'
'MCCAR'
'WRI'
'WRI'
'THORITIES'
'THERHEAD'
'SMI'
'*'
'*'
'*'
'*'
'*'
'BENEA'
'THERLEY'
'BURNWELL'
'TOGE'
'LENG'
'LENG'
'THERLEY'
'THINGNESS'
'THINGNESS'
'THINGLY'
'UNDERNEA'
finished epoch 1 - training loss: 4.2481
