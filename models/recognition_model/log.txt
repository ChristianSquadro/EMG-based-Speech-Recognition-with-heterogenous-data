897f691543960d43991a13846d6b09c92f89eb8c

diff --git a/architecture.py b/architecture.py
index 91fd2da..8671340 100644
--- a/architecture.py
+++ b/architecture.py
@@ -9,7 +9,7 @@ from transformer import TransformerEncoderLayer, TransformerDecoderLayer, Positi
 from absl import flags
 FLAGS = flags.FLAGS
 flags.DEFINE_integer('model_size', 768, 'number of hidden dimensions')
-flags.DEFINE_integer('num_layers', 6, 'number of layers')
+flags.DEFINE_integer('num_layers', 3, 'number of layers')
 flags.DEFINE_float('dropout', .2, 'dropout')
 
 class ResBlock(nn.Module):
@@ -54,8 +54,8 @@ class Model(nn.Module):
         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)
         self.pos_encoder = PositionalEncoding(FLAGS.model_size)
 
-        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
+        encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=4, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
@@ -65,12 +65,6 @@ class Model(nn.Module):
             self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
         self.device=device
 
-
-    def create_src_padding_mask(self, src):
-        # input src of shape ()
-        src_padding_mask = src == 0
-        return src_padding_mask
-
     def create_tgt_padding_mask(self, tgt):
         # input tgt of shape ()
         tgt_padding_mask = tgt == 0
@@ -83,8 +77,10 @@ class Model(nn.Module):
         if self.training:
             r = random.randrange(8)
             if r > 0:
-                x_raw[:,:-r,:] = x_raw[:,r:,:] # shift left r
-                x_raw[:,-r:,:] = 0
+                x_raw_clone = x_raw.clone()
+                x_raw_clone[:,:-r,:] = x_raw[:,r:,:] # shift left r
+                x_raw_clone[:,-r:,:] = 0
+                x_raw = x_raw_clone
 
         x_raw = x_raw.transpose(1,2) # put channel before time for conv
         x_raw = self.conv_blocks(x_raw)
@@ -92,15 +88,16 @@ class Model(nn.Module):
         x_raw = self.w_raw_in(x_raw)
         x = x_raw
 
+        #Padding Target Mask and attention mask
+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
+
         #Embedding and positional encoding of tgt
         tgt=self.embedding_tgt(y)
         tgt=self.pos_encoder(tgt)
-        #Padding Target
-        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).transpose(0,1).to(self.device)
-        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, tgt.shape[1]).to(self.device)
-
+        
         x = x.transpose(0,1) # put time first
-        tgt = tgt.transpose(0,1) # put channel after
+        tgt = tgt.transpose(0,1) # put sequence_length first
         x_encoder = self.transformerEncoder(x)
         x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
 
diff --git a/data_utils.py b/data_utils.py
index 8b05213..33a96f7 100644
--- a/data_utils.py
+++ b/data_utils.py
@@ -167,17 +167,6 @@ def combine_fixed_length(tensor_list, length):
     n = total_length // length
     return tensor.view(n, length, *tensor.size()[1:])
 
-def combine_fixed_length_tgt(tensor_list, n_batch):
-    total_length = sum(t.size(0) for t in tensor_list)
-    tensor_list = list(tensor_list) # copy
-    if total_length % n_batch != 0:
-        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-        total_length += pad_length
-    tensor = torch.cat(tensor_list, 0)
-    length = total_length // n_batch
-    return tensor.view(n_batch, length, *tensor.size()[1:])
-
 def decollate_tensor(tensor, lengths):
     b, s, d = tensor.size()
     tensor = tensor.view(b*s, d)
@@ -253,6 +242,61 @@ def read_phonemes(textgrid_fname, max_len=None):
     return phone_ids
 
 class TextTransform(object):
+    '''
+    def __init__(self, pad_token="<pad>", blank_token='_', eos_token='<eos>', sos_token='<sos>'):
+        self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
+        self.id_to_string = {}
+        self.string_to_id = {}
+        
+        # add the default pad token
+        self.id_to_string[0] = pad_token
+        self.string_to_id[pad_token] = 0
+        
+        # add the default unknown token
+        self.id_to_string[1] = blank_token
+        self.string_to_id[blank_token] = 1
+        
+        # add the default unknown token
+        self.id_to_string[2] = eos_token
+        self.string_to_id[eos_token] = 2   
+
+        # add the default unknown token
+        self.id_to_string[3] = sos_token
+        self.string_to_id[sos_token] = 3
+
+        # shortcut access
+        self.pad_id = 0
+        self.blank_id = 1
+        self.eos_id = 2
+        self.sos_id = 3
+
+    def __len__(self):
+        return len('<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' ')
+    
+    def clean_text(self, text):
+        text = unidecode(text)
+        text = self.transformation(text)
+        return text
+    
+    def add_new_words(self, text):
+        text = self.clean_text(text)
+        for c in text:
+            if c not in self.string_to_id:
+                self.string_to_id[c] = len(self.string_to_id)
+                self.id_to_string[len(self.id_to_string)] = c
+    
+    def get_labels(self):
+        return  '<pad>'+'_'+'<eos>'+'<sos>'+string.ascii_lowercase+string.digits+' '
+            
+    def text_to_int(self, text):
+        text = self.clean_text(text)
+        return [self.string_to_id[c] for c in text]
+
+    def int_to_text(self, ints):
+        return ''.join(self.id_to_string[i] for i in ints)
+    '''
+    
+
     def __init__(self):
         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
         self.chars = string.ascii_lowercase+string.digits+' '
@@ -269,3 +313,4 @@ class TextTransform(object):
 
     def int_to_text(self, ints):
         return ''.join(self.chars[i] for i in ints)
+    
diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
index 96f39d1..76c276d 100644
--- a/models/recognition_model/log.txt
+++ b/models/recognition_model/log.txt
@@ -1,4433 +1,2 @@
-be71135adc89793578f304adb405cea80a5b2b9a
+897f691543960d43991a13846d6b09c92f89eb8c
 
-diff --git a/architecture.py b/architecture.py
-index 94d0de0..91fd2da 100644
---- a/architecture.py
-+++ b/architecture.py
-@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-         return F.relu(x + res)
- 
- class Model(nn.Module):
--    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-+    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-         super().__init__()
- 
-         self.conv_blocks = nn.Sequential(
-@@ -51,7 +51,7 @@ class Model(nn.Module):
-         )
-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
- 
--        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)
-         self.pos_encoder = PositionalEncoding(FLAGS.model_size)
- 
-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-@@ -61,25 +61,24 @@ class Model(nn.Module):
-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
- 
-         self.has_aux_loss = has_aux_loss
-+        if self.has_aux_loss:
-+            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-         self.device=device
- 
-+
-     def create_src_padding_mask(self, src):
-         # input src of shape ()
--        src_padding_mask = src.transpose(1, 0) == 0
-+        src_padding_mask = src == 0
-         return src_padding_mask
- 
-     def create_tgt_padding_mask(self, tgt):
-         # input tgt of shape ()
--        tgt_padding_mask = tgt.transpose(1, 0) == 0
-+        tgt_padding_mask = tgt == 0
-         return tgt_padding_mask
-     
--    def forward(self, x_feat, x_raw, y, session_ids):
-+    def forward(self, x_raw, y):
-         # x shape is (batch, time, electrode)
-         # y shape is (batch, sequence_length)
--        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--        memory_key_padding_mask = src_key_padding_mask
--        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
- 
-         if self.training:
-             r = random.randrange(8)
-@@ -96,17 +95,20 @@ class Model(nn.Module):
-         #Embedding and positional encoding of tgt
-         tgt=self.embedding_tgt(y)
-         tgt=self.pos_encoder(tgt)
-+        #Padding Target
-+        tgt_key_padding_mask = self.create_tgt_padding_mask(tgt).transpose(0,1).to(self.device)
-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, tgt.shape[1]).to(self.device)
- 
-         x = x.transpose(0,1) # put time first
-         tgt = tgt.transpose(0,1) # put channel after
--        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-+        x_encoder = self.transformerEncoder(x)
-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, tgt_mask=tgt_mask)
- 
-         x_encoder = x_encoder.transpose(0,1)
-         x_decoder = x_decoder.transpose(0,1)
- 
-         if self.has_aux_loss:
--            return self.w_out(x_encoder), self.w_out(x_decoder)
-+            return self.w_aux(x_encoder), self.w_out(x_decoder)
-         else:
-             return self.w_out(x)
- 
-diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-index 2243f2d..342fccd 100644
---- a/models/recognition_model/log.txt
-+++ b/models/recognition_model/log.txt
-@@ -1,845 +1,2 @@
--dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-+be71135adc89793578f304adb405cea80a5b2b9a
- 
--diff --git a/architecture.py b/architecture.py
--index 2413a8a..94d0de0 100644
----- a/architecture.py
--+++ b/architecture.py
--@@ -4,7 +4,7 @@ import torch
-- from torch import nn
-- import torch.nn.functional as F
-- 
---from transformer import TransformerEncoderLayer, TransformerDecoderLayer
--+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-- 
-- from absl import flags
-- FLAGS = flags.FLAGS
--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
--         return F.relu(x + res)
-- 
-- class Model(nn.Module):
---    def __init__(self, num_features, num_outs, has_aux_loss=False):
--+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
--         super().__init__()
-- 
--         self.conv_blocks = nn.Sequential(
--@@ -52,6 +52,7 @@ class Model(nn.Module):
--         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-- 
--         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
--+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-- 
--         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
--@@ -60,9 +61,25 @@ class Model(nn.Module):
--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-- 
--         self.has_aux_loss = has_aux_loss
---
---    def forward(self, x_feat, x_raw, y,session_ids):
--+        self.device=device
--+
--+    def create_src_padding_mask(self, src):
--+        # input src of shape ()
--+        src_padding_mask = src.transpose(1, 0) == 0
--+        return src_padding_mask
--+
--+    def create_tgt_padding_mask(self, tgt):
--+        # input tgt of shape ()
--+        tgt_padding_mask = tgt.transpose(1, 0) == 0
--+        return tgt_padding_mask
--+    
--+    def forward(self, x_feat, x_raw, y, session_ids):
--         # x shape is (batch, time, electrode)
--+        # y shape is (batch, sequence_length)
--+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
--+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
--+        memory_key_padding_mask = src_key_padding_mask
--+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-- 
--         if self.training:
--             r = random.randrange(8)
--@@ -74,14 +91,16 @@ class Model(nn.Module):
--         x_raw = self.conv_blocks(x_raw)
--         x_raw = x_raw.transpose(1,2)
--         x_raw = self.w_raw_in(x_raw)
---
--         x = x_raw
--+
--+        #Embedding and positional encoding of tgt
--         tgt=self.embedding_tgt(y)
--+        tgt=self.pos_encoder(tgt)
-- 
--         x = x.transpose(0,1) # put time first
--         tgt = tgt.transpose(0,1) # put channel after
---        x_encoder = self.transformerEncoder(x)
---        x_decoder = self.transformerDecoder(tgt, x_encoder)
--+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
--+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-- 
--         x_encoder = x_encoder.transpose(0,1)
--         x_decoder = x_decoder.transpose(0,1)
--diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
--index 53839a0..1343d7d 100644
----- a/models/recognition_model/log.txt
--+++ b/models/recognition_model/log.txt
--@@ -1,639 +1,2 @@
---2fa943cd85263a152b6be80d502eda27932ebb27
--+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-- 
---diff --git a/architecture.py b/architecture.py
---index a8c70f3..2413a8a 100644
------ a/architecture.py
---+++ b/architecture.py
---@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
---         return F.relu(x + res)
--- 
--- class Model(nn.Module):
----    def __init__(self, num_features, num_outs, num_aux_outs=None):
---+    def __init__(self, num_features, num_outs, has_aux_loss=False):
---         super().__init__()
--- 
---         self.conv_blocks = nn.Sequential(
---@@ -59,9 +59,7 @@ class Model(nn.Module):
---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
--- 
----        self.has_aux_out = num_aux_outs is not None
----        if self.has_aux_out:
----            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
---+        self.has_aux_loss = has_aux_loss
--- 
---     def forward(self, x_feat, x_raw, y,session_ids):
---         # x shape is (batch, time, electrode)
---@@ -82,12 +80,14 @@ class Model(nn.Module):
--- 
---         x = x.transpose(0,1) # put time first
---         tgt = tgt.transpose(0,1) # put channel after
----        x = self.transformerEncoder(x)
----        x = self.transformerDecoder(tgt, x)
----        x = x.transpose(0,1)
---+        x_encoder = self.transformerEncoder(x)
---+        x_decoder = self.transformerDecoder(tgt, x_encoder)
--- 
----        if self.has_aux_out:
----            return self.w_out(x), self.w_aux(x)
---+        x_encoder = x_encoder.transpose(0,1)
---+        x_decoder = x_decoder.transpose(0,1)
---+
---+        if self.has_aux_loss:
---+            return self.w_out(x_encoder), self.w_out(x_decoder)
---         else:
---             return self.w_out(x)
--- 
---diff --git a/data_utils.py b/data_utils.py
---index e2632e8..8b05213 100644
------ a/data_utils.py
---+++ b/data_utils.py
---@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
--- 
--- def combine_fixed_length_tgt(tensor_list, n_batch):
---     total_length = sum(t.size(0) for t in tensor_list)
---+    tensor_list = list(tensor_list) # copy
---     if total_length % n_batch != 0:
---         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
----        tensor_list = list(tensor_list) # copy
---         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
---         total_length += pad_length
---     tensor = torch.cat(tensor_list, 0)
---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
---index b8f7791..617dd85 100644
------ a/models/recognition_model/log.txt
---+++ b/models/recognition_model/log.txt
---@@ -1,480 +1,2 @@
----902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
---+2fa943cd85263a152b6be80d502eda27932ebb27
--- 
----diff --git a/architecture.py b/architecture.py
----index d6e99b4..a8c70f3 100644
------- a/architecture.py
----+++ b/architecture.py
----@@ -54,7 +54,7 @@ class Model(nn.Module):
----         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
---- 
----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
----         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
----diff --git a/data_utils.py b/data_utils.py
----index e4ac852..e2632e8 100644
------- a/data_utils.py
----+++ b/data_utils.py
----@@ -1,3 +1,4 @@
----+import math
---- import string
---- 
---- import numpy as np
----@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
----     n = total_length // length
----     return tensor.view(n, length, *tensor.size()[1:])
---- 
----+def combine_fixed_length_tgt(tensor_list, n_batch):
----+    total_length = sum(t.size(0) for t in tensor_list)
----+    if total_length % n_batch != 0:
----+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
----+        tensor_list = list(tensor_list) # copy
----+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
----+        total_length += pad_length
----+    tensor = torch.cat(tensor_list, 0)
----+    length = total_length // n_batch
----+    return tensor.view(n_batch, length, *tensor.size()[1:])
----+
---- def decollate_tensor(tensor, lengths):
----     b, s, d = tensor.size()
----     tensor = tensor.view(b*s, d)
----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
----index e890f0f..1ee3421 100644
------- a/models/recognition_model/log.txt
----+++ b/models/recognition_model/log.txt
----@@ -1,265 +1,2 @@
-----031b80598b18e602b7f2b8d237d6b2f8d1246c05
----+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
---- 
-----diff --git a/architecture.py b/architecture.py
-----index b22af61..d6e99b4 100644
-------- a/architecture.py
-----+++ b/architecture.py
-----@@ -51,6 +51,8 @@ class Model(nn.Module):
-----         )
-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
----- 
-----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-----+
-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-----@@ -61,7 +63,7 @@ class Model(nn.Module):
-----         if self.has_aux_out:
-----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
----- 
------    def forward(self, x_feat, x_raw, session_ids):
-----+    def forward(self, x_feat, x_raw, y,session_ids):
-----         # x shape is (batch, time, electrode)
----- 
-----         if self.training:
-----@@ -76,10 +78,12 @@ class Model(nn.Module):
-----         x_raw = self.w_raw_in(x_raw)
----- 
-----         x = x_raw
-----+        tgt=self.embedding_tgt(y)
----- 
-----         x = x.transpose(0,1) # put time first
-----+        tgt = tgt.transpose(0,1) # put channel after
-----         x = self.transformerEncoder(x)
------        x = self.transformerDecoder(x) #TODO I need the target EMG
-----+        x = self.transformerDecoder(tgt, x)
-----         x = x.transpose(0,1)
----- 
-----         if self.has_aux_out:
-----diff --git a/data_utils.py b/data_utils.py
-----index 11d4805..e4ac852 100644
-------- a/data_utils.py
-----+++ b/data_utils.py
-----@@ -244,6 +244,7 @@ class TextTransform(object):
-----     def __init__(self):
-----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-----         self.chars = string.ascii_lowercase+string.digits+' '
-----+        self.vocabulary_size=len(self.chars)
----- 
-----     def clean_text(self, text):
-----         text = unidecode(text)
-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-----index fbc0abb..400061a 100644
-------- a/models/recognition_model/log.txt
-----+++ b/models/recognition_model/log.txt
-----@@ -1,188 +1,2 @@
------57f8139449dd9286c2203ec2eca118a550638a7c
-----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
----- 
------diff --git a/architecture.py b/architecture.py
------index 4fc3793..b22af61 100644
--------- a/architecture.py
------+++ b/architecture.py
------@@ -4,7 +4,7 @@ import torch
------ from torch import nn
------ import torch.nn.functional as F
------ 
-------from transformer import TransformerEncoderLayer
------+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
------ 
------ from absl import flags
------ FLAGS = flags.FLAGS
------@@ -52,7 +52,9 @@ class Model(nn.Module):
------         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
------ 
------         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
------+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
------+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
------+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
------         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
------ 
------         self.has_aux_out = num_aux_outs is not None
------@@ -76,7 +78,8 @@ class Model(nn.Module):
------         x = x_raw
------ 
------         x = x.transpose(0,1) # put time first
-------        x = self.transformer(x)
------+        x = self.transformerEncoder(x)
------+        x = self.transformerDecoder(x) #TODO I need the target EMG
------         x = x.transpose(0,1)
------ 
------         if self.has_aux_out:
------diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
------index 571de9d..8563980 100644
--------- a/models/recognition_model/log.txt
------+++ b/models/recognition_model/log.txt
------@@ -1,5 +1,2 @@
------+57f8139449dd9286c2203ec2eca118a550638a7c
------ 
-------
-------['recognition_model.py', '--output_directory', './models/recognition_model/']
-------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-------train / dev split: 8055 200
------diff --git a/output/log.txt b/output/log.txt
------index ae42364..1d2cd8e 100644
--------- a/output/log.txt
------+++ b/output/log.txt
------@@ -1,3 +1,13 @@
------+57f8139449dd9286c2203ec2eca118a550638a7c
------ 
------+diff --git a/output/log.txt b/output/log.txt
------+index ae42364..8563980 100644
------+--- a/output/log.txt
------++++ b/output/log.txt
------+@@ -1,3 +1,2 @@
------++57f8139449dd9286c2203ec2eca118a550638a7c
------+ 
------+-
------+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
------ 
------ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
------diff --git a/transformer.py b/transformer.py
------index 6743588..ac131be 100644
--------- a/transformer.py
------+++ b/transformer.py
------@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
------         Shape:
------             see the docs in Transformer class.
------         """
-------        src2 = self.self_attn(src)
------+        src2 = self.self_attn(src, src, src)
------         src = src + self.dropout1(src2)
------         src = self.norm1(src)
------         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
------@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
------         src = self.norm2(src)
------         return src
------ 
------+class TransformerDecoderLayer(nn.Module):
------+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
------+    This standard decoder layer is based on the paper "Attention Is All You Need".
------+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
------+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
------+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
------+    in a different way during application.
------+
------+    Args:
------+        d_model: the number of expected features in the input (required).
------+        nhead: the number of heads in the multiheadattention models (required).
------+        dim_feedforward: the dimension of the feedforward network model (default=2048).
------+        dropout: the dropout value (default=0.1).
------+        activation: the activation function of the intermediate layer, can be a string
------+            ("relu" or "gelu") or a unary callable. Default: relu
------+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
------+        batch_first: If ``True``, then the input and output tensors are provided
------+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
------+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
------+            attention and feedforward operations, respectively. Otherwise it's done after.
------+            Default: ``False`` (after).
------+
------+    Examples::
------+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
------+        >>> memory = torch.rand(10, 32, 512)
------+        >>> tgt = torch.rand(20, 32, 512)
------+        >>> out = decoder_layer(tgt, memory)
------+    """
------+    # Adapted from pytorch source
------+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
------+        super(TransformerDecoderLayer, self).__init__()
------+        #Attention Mechanism
------+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
------+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
------+        # Implementation of Feedforward model
------+        self.linear1 = nn.Linear(d_model, dim_feedforward)
------+        self.dropout = nn.Dropout(dropout)
------+        self.linear2 = nn.Linear(dim_feedforward, d_model)
------+        #Normalization Layer and Dropout Layer
------+        self.norm1 = nn.LayerNorm(d_model)
------+        self.norm2 = nn.LayerNorm(d_model)
------+        self.norm3 = nn.LayerNorm(d_model)
------+        self.dropout1 = nn.Dropout(dropout)
------+        self.dropout2 = nn.Dropout(dropout)
------+        self.dropout3 = nn.Dropout(dropout)
------+        #Activation Function
------+        self.activation = nn.ReLU()
------+    
------+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
------+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
------+        r"""Pass the input through the encoder layer.
------+
------+        Args:
------+            tgt: the sequence to the decoder layer (required).
------+            memory: the sequence from the last layer of the encoder (required).
------+            tgt_mask: the mask for the tgt sequence (optional).
------+            memory_mask: the mask for the memory sequence (optional).
------+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
------+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
------+
------+        Shape:
------+            see the docs in Transformer class.
------+        """
------+        tgt2 = self.self_attn(tgt, tgt, tgt)
------+        tgt = tgt + self.dropout1(tgt2)
------+        tgt = self.norm1(tgt)
------+
------+        tgt2=self.multihead_attn(tgt, memory, memory)
------+        tgt = tgt + self.dropout1(tgt2)
------+        tgt = self.norm1(tgt)
------+
------+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
------+        tgt = tgt + self.dropout2(tgt2)
------+        tgt = self.norm2(tgt)
------+        return tgt
------+    
------+
------ class MultiHeadAttention(nn.Module):
------   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
------     super().__init__()
------@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
------     else:
------         self.relative_positional = None
------ 
-------  def forward(self, x):
------+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
------     """Runs the multi-head self-attention layer.
------ 
------     Args:
------@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
------       A single tensor containing the output from this layer
------     """
------ 
-------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
------+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
------+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
------+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
------     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
------ 
------     if self.relative_positional is not None:
------
------['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
------train / dev split: 8055 200
-----diff --git a/recognition_model.py b/recognition_model.py
-----index dea6d47..a46dff0 100644
-------- a/recognition_model.py
-----+++ b/recognition_model.py
-----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
----- 
-----             X = combine_fixed_length(example['emg'], 200).to(device)
-----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
----- 
------            pred = model(X, X_raw, sess)
-----+            pred = model(X, X_raw, tgt, sess)
-----             pred = F.log_softmax(pred, 2)
----- 
-----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----
-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-----train / dev split: 8055 200
----diff --git a/recognition_model.py b/recognition_model.py
----index a46dff0..8fd300c 100644
------- a/recognition_model.py
----+++ b/recognition_model.py
----@@ -6,6 +6,7 @@ import subprocess
---- from ctcdecode import CTCBeamDecoder
---- import jiwer
---- import random
----+from torch.utils.tensorboard import SummaryWriter
---- 
---- import torch
---- from torch import nn
----@@ -13,7 +14,7 @@ import torch.nn.functional as F
---- 
---- from read_emg import EMGDataset, SizeAwareSampler
---- from architecture import Model
-----from data_utils import combine_fixed_length, decollate_tensor
----+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
---- from transformer import TransformerEncoderLayer
---- 
---- from absl import flags
----@@ -62,17 +63,21 @@ def test(model, testset, device):
----     return jiwer.wer(references, predictions)
---- 
---- 
-----def train_model(trainset, devset, device, n_epochs=200):
-----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-----
----+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
----+    #Define Dataloader
----+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
----+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
---- 
----+    #Define model and loss function
----     n_chars = len(devset.text_transform.chars)
----     model = Model(devset.num_features, n_chars+1).to(device)
----+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
---- 
----     if FLAGS.start_training_from is not None:
----         state_dict = torch.load(FLAGS.start_training_from)
----         model.load_state_dict(state_dict, strict=False)
---- 
----+    #Define optimizer and scheduler for the learning rate
----     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
----     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
---- 
----@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
----             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
---- 
----     batch_idx = 0
----+    train_loss= 0
----+    eval_loss = 0
----     optim.zero_grad()
----     for epoch_idx in range(n_epochs):
----+        model.train()
----         losses = []
-----        for example in dataloader:
----+        for example in dataloader_training:
----             schedule_lr(batch_idx)
---- 
----+            #Preprosessing of the input and target for the model
----             X = combine_fixed_length(example['emg'], 200).to(device)
----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
----+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
---- 
----+            #Shifting target for input decoder and loss
----+            tgt= y[:,:-1]
----+            target= y[:,1:]
----+
----+            #Prediction
----             pred = model(X, X_raw, tgt, sess)
-----            pred = F.log_softmax(pred, 2)
---- 
-----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
----+            #Primary Loss
----+            pred=pred.permute(0,2,1)
----+            loss = loss_fn(pred, target)
----+
----+            #Auxiliary Loss
----+            #pred = F.log_softmax(pred, 2)
----+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
----+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
----             losses.append(loss.item())
----+            train_loss += loss.item()
---- 
----             loss.backward()
----             if (batch_idx+1) % 2 == 0:
----                 optim.step()
----                 optim.zero_grad()
---- 
----+            #Report plots in tensorboard
----+            if batch_idx % report_every == report_every - 2:     
----+                #Evaluation
----+                model.eval()
----+                with torch.no_grad():
----+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
----+                        X_raw = example['raw_emg'].to(device)
----+                        sess = example['session_ids'].to(device)
----+                        y = example['text_int'].to(device)
----+
----+                        #Shifting target for input decoder and loss
----+                        tgt= y[:,:-1]
----+                        target= y[:,1:]
----+
----+                        #Prediction without the 197-th batch because of missing label
----+                        if idx != 197:
----+                            pred = model(X, X_raw, tgt, sess)
----+                            #Primary Loss
----+                            pred=pred.permute(0,2,1)
----+                            loss = loss_fn(pred, target)
----+                            eval_loss += loss.item()
----+
----+                #Writing on tensorboard
----+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
----+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
----+                train_loss= 0
----+                eval_loss= 0
----+
----+            #Increment counter        
----             batch_idx += 1
-----        train_loss = np.mean(losses)
----+
----+        #Testing and change learning rate
----         val = test(model, devset, device)
----+        writer.add_scalar('WER/Evaluation',val, batch_idx)
----         lr_sched.step()
----+    
----+        #Logging
----+        train_loss = np.mean(losses)
----         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
----         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
---- 
----@@ -148,8 +201,9 @@ def main():
----     logging.info('train / dev split: %d %d',len(trainset),len(devset))
---- 
----     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
----+    writer = SummaryWriter(log_dir="./content/runs")
---- 
-----    model = train_model(trainset, devset, device)
----+    model = train_model(trainset, devset ,device, writer)
---- 
---- if __name__ == '__main__':
----     FLAGS(sys.argv)
----diff --git a/transformer.py b/transformer.py
----index ac131be..51e1f2e 100644
------- a/transformer.py
----+++ b/transformer.py
----@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
----     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
----     self.d_qkv = d_qkv
---- 
----+    #self.kdim = kdim if kdim is not None else embed_dim
----+    #self.vdim = vdim if vdim is not None else embed_dim
----+
----     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
----
----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
----train / dev split: 8055 200
---diff --git a/recognition_model.py b/recognition_model.py
---index fde5a40..6d5143b 100644
------ a/recognition_model.py
---+++ b/recognition_model.py
---@@ -63,14 +63,14 @@ def test(model, testset, device):
---     return jiwer.wer(references, predictions)
--- 
--- 
----def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
---     #Define Dataloader
---     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
---     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
--- 
---     #Define model and loss function
---     n_chars = len(devset.text_transform.chars)
----    model = Model(devset.num_features, n_chars+1).to(device)
---+    model = Model(devset.num_features, n_chars+1, True).to(device)
---     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
--- 
---     if FLAGS.start_training_from is not None:
---@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
---             target= y[:,1:]
--- 
---             #Prediction
----            pred = model(X, X_raw, tgt, sess)
---+            out_enc, out_dec = model(X, X_raw, tgt, sess)
--- 
---             #Primary Loss
----            pred=pred.permute(0,2,1)
----            loss = loss_fn(pred, target)
---+            out_dec=out_dec.permute(0,2,1)
---+            loss_dec = loss_fn(out_dec, target)
--- 
---             #Auxiliary Loss
----            #pred = F.log_softmax(pred, 2)
----            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
----            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
----            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
---+            out_enc = F.log_softmax(out_enc, 2)
---+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
---+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
---+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
---+
---+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
---             losses.append(loss.item())
---             train_loss += loss.item()
--- 
---@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
---             if (batch_idx+1) % 2 == 0:
---                 optim.step()
---                 optim.zero_grad()
----
----            if batch_idx % report_every == report_every - 2:     
---+            
---+            if False:
---+            #if batch_idx % report_every == report_every - 2:     
---                 #Evaluation
---                 model.eval()
---                 with torch.no_grad():
---                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
----                        X_raw = example['raw_emg'].to(device)
----                        sess = example['session_ids'].to(device)
----                        y = example['text_int'].to(device)
---+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
---+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
---+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
--- 
---                         #Shifting target for input decoder and loss
---                         tgt= y[:,:-1]
---                         target= y[:,1:]
--- 
---+                        print(idx)
---+
---                         #Prediction without the 197-th batch because of missing label
----                        if idx != 197:
---+                        if idx != 181:
---                             pred = model(X, X_raw, tgt, sess)
---                             #Primary Loss
---                             pred=pred.permute(0,2,1)
---@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
--- 
---             #Increment counter        
---             batch_idx += 1
---+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
--- 
---         #Testing and change learning rate
---         val = test(model, devset, device)
---
---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
---train / dev split: 8055 200
--diff --git a/recognition_model.py b/recognition_model.py
--index 30c5ff2..2672d45 100644
----- a/recognition_model.py
--+++ b/recognition_model.py
--@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-- 
--     #Define model and loss function
--     n_chars = len(devset.text_transform.chars)
---    model = Model(devset.num_features, n_chars+1, True).to(device)
--+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-- 
--     if FLAGS.start_training_from is not None:
--diff --git a/transformer.py b/transformer.py
--index 51e1f2e..c125841 100644
----- a/transformer.py
--+++ b/transformer.py
--@@ -1,3 +1,4 @@
--+import math
-- from typing import Optional
-- 
-- import torch
--@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
--         Shape:
--             see the docs in Transformer class.
--         """
---        src2 = self.self_attn(src, src, src)
--+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
--         src = src + self.dropout1(src2)
--         src = self.norm1(src)
--         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
--@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
--         Shape:
--             see the docs in Transformer class.
--         """
---        tgt2 = self.self_attn(tgt, tgt, tgt)
--+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
--         tgt = tgt + self.dropout1(tgt2)
--         tgt = self.norm1(tgt)
-- 
---        tgt2=self.multihead_attn(tgt, memory, memory)
--+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
--         tgt = tgt + self.dropout1(tgt2)
--         tgt = self.norm1(tgt)
-- 
--@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
--     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
--     self.d_qkv = d_qkv
-- 
---    #self.kdim = kdim if kdim is not None else embed_dim
---    #self.vdim = vdim if vdim is not None else embed_dim
---
--     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
--@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
--     else:
--         self.relative_positional = None
-- 
---  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
--+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
--     """Runs the multi-head self-attention layer.
-- 
--     Args:
--@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
--     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
--     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-- 
--+    if attn_mask is not None:
--+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
--+
--     if self.relative_positional is not None:
--         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
--         l,b,h,d = q_pos.size()
--@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
--             x = x.transpose(0, 1)
--             x = x.contiguous().view(bsz_heads, length+1, length)
--             return x[:, 1:, :]
--+        
--+
--+########
--+# Taken from:
--+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
--+# or also here:
--+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
--+class PositionalEncoding(nn.Module):
--+
--+    def __init__(self, d_model, dropout=0.0, max_len=5000):
--+        super(PositionalEncoding, self).__init__()
--+        self.dropout = nn.Dropout(p=dropout)
--+        self.max_len = max_len
--+
--+        pe = torch.zeros(max_len, d_model)
--+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
--+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
--+                             * (-math.log(10000.0) / d_model))
--+        pe[:, 0::2] = torch.sin(position * div_term)
--+        pe[:, 1::2] = torch.cos(position * div_term)
--+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
--+        self.register_buffer('pe', pe)  # Will not be trained.
--+
--+    def forward(self, x):
--+        """Inputs of forward function
--+        Args:
--+            x: the sequence fed to the positional encoder model (required).
--+        Shape:
--+            x: [sequence length, batch size, embed dim]
--+            output: [sequence length, batch size, embed dim]
--+        """
--+        assert x.size(0) < self.max_len, (
--+            f"Too long sequence length: increase `max_len` of pos encoding")
--+        # shape of x (len, B, dim)
--+        x = x + self.pe[:x.size(0), :]
--+        return self.dropout(x)
--
--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
--output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
--train / dev split: 8055 200
-diff --git a/output/log.txt b/output/log.txt
-index 1d2cd8e..42b3343 100644
---- a/output/log.txt
-+++ b/output/log.txt
-@@ -1,13 +1,3339 @@
--57f8139449dd9286c2203ec2eca118a550638a7c
-+be71135adc89793578f304adb405cea80a5b2b9a
- 
-+diff --git a/architecture.py b/architecture.py
-+index 94d0de0..222c88e 100644
-+--- a/architecture.py
-++++ b/architecture.py
-+@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-+         return F.relu(x + res)
-+ 
-+ class Model(nn.Module):
-+-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-+         super().__init__()
-+ 
-+         self.conv_blocks = nn.Sequential(
-+@@ -61,8 +61,11 @@ class Model(nn.Module):
-+         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+ 
-+         self.has_aux_loss = has_aux_loss
-++        if self.has_aux_loss:
-++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-+         self.device=device
-+ 
-++
-+     def create_src_padding_mask(self, src):
-+         # input src of shape ()
-+         src_padding_mask = src.transpose(1, 0) == 0
-+@@ -106,7 +109,7 @@ class Model(nn.Module):
-+         x_decoder = x_decoder.transpose(0,1)
-+ 
-+         if self.has_aux_loss:
-+-            return self.w_out(x_encoder), self.w_out(x_decoder)
-++            return self.w_aux(x_encoder), self.w_out(x_decoder)
-+         else:
-+             return self.w_out(x)
-+ 
-+diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+index 2243f2d..a8cb5ee 100644
-+--- a/models/recognition_model/log.txt
-++++ b/models/recognition_model/log.txt
-+@@ -1,844 +1,2577 @@
-+-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-++be71135adc89793578f304adb405cea80a5b2b9a
-+ 
-+ diff --git a/architecture.py b/architecture.py
-+-index 2413a8a..94d0de0 100644
-++index 94d0de0..7170c48 100644
-+ --- a/architecture.py
-+ +++ b/architecture.py
-+-@@ -4,7 +4,7 @@ import torch
-+- from torch import nn
-+- import torch.nn.functional as F
-+- 
-+--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-+-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-+- 
-+- from absl import flags
-+- FLAGS = flags.FLAGS
-+ @@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-+          return F.relu(x + res)
-+  
-+  class Model(nn.Module):
-+--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-+-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-++-    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-+++    def __init__(self, num_features, num_outs, device , has_aux_loss=False):
-+          super().__init__()
-+  
-+          self.conv_blocks = nn.Sequential(
-+-@@ -52,6 +52,7 @@ class Model(nn.Module):
-+-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+- 
-+-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-+- 
-+-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+-@@ -60,9 +61,25 @@ class Model(nn.Module):
-++@@ -61,8 +61,11 @@ class Model(nn.Module):
-+          self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+  
-+          self.has_aux_loss = has_aux_loss
-+--
-+--    def forward(self, x_feat, x_raw, y,session_ids):
-+-+        self.device=device
-+-+
-+-+    def create_src_padding_mask(self, src):
-+-+        # input src of shape ()
-+-+        src_padding_mask = src.transpose(1, 0) == 0
-+-+        return src_padding_mask
-+-+
-+-+    def create_tgt_padding_mask(self, tgt):
-+-+        # input tgt of shape ()
-+-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-+-+        return tgt_padding_mask
-+-+    
-+-+    def forward(self, x_feat, x_raw, y, session_ids):
-+-         # x shape is (batch, time, electrode)
-+-+        # y shape is (batch, sequence_length)
-+-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-+-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-+-+        memory_key_padding_mask = src_key_padding_mask
-+-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-+++        if self.has_aux_out:
-+++            self.w_aux = nn.Linear(FLAGS.model_size, num_outs)
-++         self.device=device
-+  
-+-         if self.training:
-+-             r = random.randrange(8)
-+-@@ -74,14 +91,16 @@ class Model(nn.Module):
-+-         x_raw = self.conv_blocks(x_raw)
-+-         x_raw = x_raw.transpose(1,2)
-+-         x_raw = self.w_raw_in(x_raw)
-+--
-+-         x = x_raw
-+ +
-+-+        #Embedding and positional encoding of tgt
-+-         tgt=self.embedding_tgt(y)
-+-+        tgt=self.pos_encoder(tgt)
-++     def create_src_padding_mask(self, src):
-++         # input src of shape ()
-++         src_padding_mask = src.transpose(1, 0) == 0
-++@@ -106,7 +109,7 @@ class Model(nn.Module):
-++         x_decoder = x_decoder.transpose(0,1)
-+  
-+-         x = x.transpose(0,1) # put time first
-+-         tgt = tgt.transpose(0,1) # put channel after
-+--        x_encoder = self.transformerEncoder(x)
-+--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-+-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-+-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-++         if self.has_aux_loss:
-++-            return self.w_out(x_encoder), self.w_out(x_decoder)
-+++            return self.w_aux(x_encoder), self.w_out(x_decoder)
-++         else:
-++             return self.w_out(x)
-+  
-+-         x_encoder = x_encoder.transpose(0,1)
-+-         x_decoder = x_decoder.transpose(0,1)
-+ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+-index 53839a0..1343d7d 100644
-++index 2243f2d..342fccd 100644
-+ --- a/models/recognition_model/log.txt
-+ +++ b/models/recognition_model/log.txt
-+-@@ -1,639 +1,2 @@
-+--2fa943cd85263a152b6be80d502eda27932ebb27
-+-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-++@@ -1,845 +1,2 @@
-++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-+++be71135adc89793578f304adb405cea80a5b2b9a
-+  
-+ -diff --git a/architecture.py b/architecture.py
-+--index a8c70f3..2413a8a 100644
-++-index 2413a8a..94d0de0 100644
-+ ---- a/architecture.py
-+ -+++ b/architecture.py
-++-@@ -4,7 +4,7 @@ import torch
-++- from torch import nn
-++- import torch.nn.functional as F
-++- 
-++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-++- 
-++- from absl import flags
-++- FLAGS = flags.FLAGS
-+ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-+ -         return F.relu(x + res)
-+ - 
-+ - class Model(nn.Module):
-+---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-+--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-+ -         super().__init__()
-+ - 
-+ -         self.conv_blocks = nn.Sequential(
-+--@@ -59,9 +59,7 @@ class Model(nn.Module):
-+--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+ - 
-+---        self.has_aux_out = num_aux_outs is not None
-+---        if self.has_aux_out:
-+---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-+--+        self.has_aux_loss = has_aux_loss
-++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-+ - 
-+--     def forward(self, x_feat, x_raw, y,session_ids):
-+--         # x shape is (batch, time, electrode)
-+--@@ -82,12 +80,14 @@ class Model(nn.Module):
-++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+ - 
-+--         x = x.transpose(0,1) # put time first
-+--         tgt = tgt.transpose(0,1) # put channel after
-+---        x = self.transformerEncoder(x)
-+---        x = self.transformerDecoder(tgt, x)
-+---        x = x.transpose(0,1)
-+--+        x_encoder = self.transformerEncoder(x)
-+--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++-         self.has_aux_loss = has_aux_loss
-++--
-++--    def forward(self, x_feat, x_raw, y,session_ids):
-++-+        self.device=device
-++-+
-++-+    def create_src_padding_mask(self, src):
-++-+        # input src of shape ()
-++-+        src_padding_mask = src.transpose(1, 0) == 0
-++-+        return src_padding_mask
-++-+
-++-+    def create_tgt_padding_mask(self, tgt):
-++-+        # input tgt of shape ()
-++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-++-+        return tgt_padding_mask
-++-+    
-++-+    def forward(self, x_feat, x_raw, y, session_ids):
-++-         # x shape is (batch, time, electrode)
-++-+        # y shape is (batch, sequence_length)
-++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-++-+        memory_key_padding_mask = src_key_padding_mask
-++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-+ - 
-+---        if self.has_aux_out:
-+---            return self.w_out(x), self.w_aux(x)
-+--+        x_encoder = x_encoder.transpose(0,1)
-+--+        x_decoder = x_decoder.transpose(0,1)
-++-         if self.training:
-++-             r = random.randrange(8)
-++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-++-         x_raw = self.conv_blocks(x_raw)
-++-         x_raw = x_raw.transpose(1,2)
-++-         x_raw = self.w_raw_in(x_raw)
-++--
-++-         x = x_raw
-+ -+
-+--+        if self.has_aux_loss:
-+--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-+--         else:
-+--             return self.w_out(x)
-++-+        #Embedding and positional encoding of tgt
-++-         tgt=self.embedding_tgt(y)
-++-+        tgt=self.pos_encoder(tgt)
-+ - 
-+--diff --git a/data_utils.py b/data_utils.py
-+--index e2632e8..8b05213 100644
-+----- a/data_utils.py
-+--+++ b/data_utils.py
-+--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-++-         x = x.transpose(0,1) # put time first
-++-         tgt = tgt.transpose(0,1) # put channel after
-++--        x_encoder = self.transformerEncoder(x)
-++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-+ - 
-+-- def combine_fixed_length_tgt(tensor_list, n_batch):
-+--     total_length = sum(t.size(0) for t in tensor_list)
-+--+    tensor_list = list(tensor_list) # copy
-+--     if total_length % n_batch != 0:
-+--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-+---        tensor_list = list(tensor_list) # copy
-+--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-+--         total_length += pad_length
-+--     tensor = torch.cat(tensor_list, 0)
-++-         x_encoder = x_encoder.transpose(0,1)
-++-         x_decoder = x_decoder.transpose(0,1)
-+ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+--index b8f7791..617dd85 100644
-++-index 53839a0..1343d7d 100644
-+ ---- a/models/recognition_model/log.txt
-+ -+++ b/models/recognition_model/log.txt
-+--@@ -1,480 +1,2 @@
-+---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-+--+2fa943cd85263a152b6be80d502eda27932ebb27
-++-@@ -1,639 +1,2 @@
-++--2fa943cd85263a152b6be80d502eda27932ebb27
-++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-+ - 
-+ --diff --git a/architecture.py b/architecture.py
-+---index d6e99b4..a8c70f3 100644
-++--index a8c70f3..2413a8a 100644
-+ ----- a/architecture.py
-+ --+++ b/architecture.py
-+---@@ -54,7 +54,7 @@ class Model(nn.Module):
-+---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-++--         return F.relu(x + res)
-+ -- 
-+---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++-- class Model(nn.Module):
-++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++--         super().__init__()
-++-- 
-++--         self.conv_blocks = nn.Sequential(
-++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-+ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++-- 
-++---        self.has_aux_out = num_aux_outs is not None
-++---        if self.has_aux_out:
-++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-++--+        self.has_aux_loss = has_aux_loss
-++-- 
-++--     def forward(self, x_feat, x_raw, y,session_ids):
-++--         # x shape is (batch, time, electrode)
-++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-++-- 
-++--         x = x.transpose(0,1) # put time first
-++--         tgt = tgt.transpose(0,1) # put channel after
-++---        x = self.transformerEncoder(x)
-++---        x = self.transformerDecoder(tgt, x)
-++---        x = x.transpose(0,1)
-++--+        x_encoder = self.transformerEncoder(x)
-++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++-- 
-++---        if self.has_aux_out:
-++---            return self.w_out(x), self.w_aux(x)
-++--+        x_encoder = x_encoder.transpose(0,1)
-++--+        x_decoder = x_decoder.transpose(0,1)
-++--+
-++--+        if self.has_aux_loss:
-++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-++--         else:
-++--             return self.w_out(x)
-++-- 
-+ --diff --git a/data_utils.py b/data_utils.py
-+---index e4ac852..e2632e8 100644
-++--index e2632e8..8b05213 100644
-+ ----- a/data_utils.py
-+ --+++ b/data_utils.py
-+---@@ -1,3 +1,4 @@
-+---+import math
-+--- import string
-+--- 
-+--- import numpy as np
-+---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-+---     n = total_length // length
-+---     return tensor.view(n, length, *tensor.size()[1:])
-++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-+ -- 
-+---+def combine_fixed_length_tgt(tensor_list, n_batch):
-+---+    total_length = sum(t.size(0) for t in tensor_list)
-+---+    if total_length % n_batch != 0:
-+---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-+---+        tensor_list = list(tensor_list) # copy
-+---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-+---+        total_length += pad_length
-+---+    tensor = torch.cat(tensor_list, 0)
-+---+    length = total_length // n_batch
-+---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-+---+
-+--- def decollate_tensor(tensor, lengths):
-+---     b, s, d = tensor.size()
-+---     tensor = tensor.view(b*s, d)
-++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-++--     total_length = sum(t.size(0) for t in tensor_list)
-++--+    tensor_list = list(tensor_list) # copy
-++--     if total_length % n_batch != 0:
-++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-++---        tensor_list = list(tensor_list) # copy
-++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-++--         total_length += pad_length
-++--     tensor = torch.cat(tensor_list, 0)
-+ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+---index e890f0f..1ee3421 100644
-++--index b8f7791..617dd85 100644
-+ ----- a/models/recognition_model/log.txt
-+ --+++ b/models/recognition_model/log.txt
-+---@@ -1,265 +1,2 @@
-+----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-+---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-++--@@ -1,480 +1,2 @@
-++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-++--+2fa943cd85263a152b6be80d502eda27932ebb27
-+ -- 
-+ ---diff --git a/architecture.py b/architecture.py
-+----index b22af61..d6e99b4 100644
-++---index d6e99b4..a8c70f3 100644
-+ ------ a/architecture.py
-+ ---+++ b/architecture.py
-+----@@ -51,6 +51,8 @@ class Model(nn.Module):
-+----         )
-+----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+ --- 
-+----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+----+
-+ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+----@@ -61,7 +63,7 @@ class Model(nn.Module):
-+----         if self.has_aux_out:
-+----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-+---- 
-+-----    def forward(self, x_feat, x_raw, session_ids):
-+----+    def forward(self, x_feat, x_raw, y,session_ids):
-+----         # x shape is (batch, time, electrode)
-+---- 
-+----         if self.training:
-+----@@ -76,10 +78,12 @@ class Model(nn.Module):
-+----         x_raw = self.w_raw_in(x_raw)
-+---- 
-+----         x = x_raw
-+----+        tgt=self.embedding_tgt(y)
-+---- 
-+----         x = x.transpose(0,1) # put time first
-+----+        tgt = tgt.transpose(0,1) # put channel after
-+----         x = self.transformerEncoder(x)
-+-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-+----+        x = self.transformerDecoder(tgt, x)
-+----         x = x.transpose(0,1)
-+---- 
-+----         if self.has_aux_out:
-++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+ ---diff --git a/data_utils.py b/data_utils.py
-+----index 11d4805..e4ac852 100644
-++---index e4ac852..e2632e8 100644
-+ ------ a/data_utils.py
-+ ---+++ b/data_utils.py
-+----@@ -244,6 +244,7 @@ class TextTransform(object):
-+----     def __init__(self):
-+----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-+----         self.chars = string.ascii_lowercase+string.digits+' '
-+----+        self.vocabulary_size=len(self.chars)
-++---@@ -1,3 +1,4 @@
-++---+import math
-++--- import string
-+ --- 
-+----     def clean_text(self, text):
-+----         text = unidecode(text)
-++--- import numpy as np
-++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-++---     n = total_length // length
-++---     return tensor.view(n, length, *tensor.size()[1:])
-++--- 
-++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-++---+    total_length = sum(t.size(0) for t in tensor_list)
-++---+    if total_length % n_batch != 0:
-++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-++---+        tensor_list = list(tensor_list) # copy
-++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-++---+        total_length += pad_length
-++---+    tensor = torch.cat(tensor_list, 0)
-++---+    length = total_length // n_batch
-++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-++---+
-++--- def decollate_tensor(tensor, lengths):
-++---     b, s, d = tensor.size()
-++---     tensor = tensor.view(b*s, d)
-+ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+----index fbc0abb..400061a 100644
-++---index e890f0f..1ee3421 100644
-+ ------ a/models/recognition_model/log.txt
-+ ---+++ b/models/recognition_model/log.txt
-+----@@ -1,188 +1,2 @@
-+-----57f8139449dd9286c2203ec2eca118a550638a7c
-+----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-++---@@ -1,265 +1,2 @@
-++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-+ --- 
-+ ----diff --git a/architecture.py b/architecture.py
-+-----index 4fc3793..b22af61 100644
-++----index b22af61..d6e99b4 100644
-+ ------- a/architecture.py
-+ ----+++ b/architecture.py
-+-----@@ -4,7 +4,7 @@ import torch
-+----- from torch import nn
-+----- import torch.nn.functional as F
-+----- 
-+------from transformer import TransformerEncoderLayer
-+-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-+----- 
-+----- from absl import flags
-+----- FLAGS = flags.FLAGS
-+-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-++----         )
-+ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+ ---- 
-++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++----+
-+ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-++----         if self.has_aux_out:
-++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-++---- 
-++-----    def forward(self, x_feat, x_raw, session_ids):
-++----+    def forward(self, x_feat, x_raw, y,session_ids):
-++----         # x shape is (batch, time, electrode)
-++---- 
-++----         if self.training:
-++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-++----         x_raw = self.w_raw_in(x_raw)
-+ ---- 
-+-----         self.has_aux_out = num_aux_outs is not None
-+-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-+ ----         x = x_raw
-++----+        tgt=self.embedding_tgt(y)
-+ ---- 
-+ ----         x = x.transpose(0,1) # put time first
-+------        x = self.transformer(x)
-+-----+        x = self.transformerEncoder(x)
-+-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-++----+        tgt = tgt.transpose(0,1) # put channel after
-++----         x = self.transformerEncoder(x)
-++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-++----+        x = self.transformerDecoder(tgt, x)
-+ ----         x = x.transpose(0,1)
-+ ---- 
-+ ----         if self.has_aux_out:
-++----diff --git a/data_utils.py b/data_utils.py
-++----index 11d4805..e4ac852 100644
-++------- a/data_utils.py
-++----+++ b/data_utils.py
-++----@@ -244,6 +244,7 @@ class TextTransform(object):
-++----     def __init__(self):
-++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-++----         self.chars = string.ascii_lowercase+string.digits+' '
-++----+        self.vocabulary_size=len(self.chars)
-++---- 
-++----     def clean_text(self, text):
-++----         text = unidecode(text)
-+ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+-----index 571de9d..8563980 100644
-++----index fbc0abb..400061a 100644
-+ ------- a/models/recognition_model/log.txt
-+ ----+++ b/models/recognition_model/log.txt
-+-----@@ -1,5 +1,2 @@
-+-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++----@@ -1,188 +1,2 @@
-++-----57f8139449dd9286c2203ec2eca118a550638a7c
-++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-+ ---- 
-++-----diff --git a/architecture.py b/architecture.py
-++-----index 4fc3793..b22af61 100644
-++-------- a/architecture.py
-++-----+++ b/architecture.py
-++-----@@ -4,7 +4,7 @@ import torch
-++----- from torch import nn
-++----- import torch.nn.functional as F
-++----- 
-++------from transformer import TransformerEncoderLayer
-++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-++----- 
-++----- from absl import flags
-++----- FLAGS = flags.FLAGS
-++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-++----- 
-++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++----- 
-++-----         self.has_aux_out = num_aux_outs is not None
-++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-++-----         x = x_raw
-++----- 
-++-----         x = x.transpose(0,1) # put time first
-++------        x = self.transformer(x)
-++-----+        x = self.transformerEncoder(x)
-++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-++-----         x = x.transpose(0,1)
-++----- 
-++-----         if self.has_aux_out:
-++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-++-----index 571de9d..8563980 100644
-++-------- a/models/recognition_model/log.txt
-++-----+++ b/models/recognition_model/log.txt
-++-----@@ -1,5 +1,2 @@
-++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++----- 
-++------
-++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-++------train / dev split: 8055 200
-++-----diff --git a/output/log.txt b/output/log.txt
-++-----index ae42364..1d2cd8e 100644
-++-------- a/output/log.txt
-++-----+++ b/output/log.txt
-++-----@@ -1,3 +1,13 @@
-++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++----- 
-++-----+diff --git a/output/log.txt b/output/log.txt
-++-----+index ae42364..8563980 100644
-++-----+--- a/output/log.txt
-++-----++++ b/output/log.txt
-++-----+@@ -1,3 +1,2 @@
-++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-++-----+ 
-++-----+-
-++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++----- 
-++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++-----diff --git a/transformer.py b/transformer.py
-++-----index 6743588..ac131be 100644
-++-------- a/transformer.py
-++-----+++ b/transformer.py
-++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-++-----         Shape:
-++-----             see the docs in Transformer class.
-++-----         """
-++------        src2 = self.self_attn(src)
-++-----+        src2 = self.self_attn(src, src, src)
-++-----         src = src + self.dropout1(src2)
-++-----         src = self.norm1(src)
-++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-++-----         src = self.norm2(src)
-++-----         return src
-++----- 
-++-----+class TransformerDecoderLayer(nn.Module):
-++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-++-----+    in a different way during application.
-++-----+
-++-----+    Args:
-++-----+        d_model: the number of expected features in the input (required).
-++-----+        nhead: the number of heads in the multiheadattention models (required).
-++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-++-----+        dropout: the dropout value (default=0.1).
-++-----+        activation: the activation function of the intermediate layer, can be a string
-++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-++-----+        batch_first: If ``True``, then the input and output tensors are provided
-++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-++-----+            Default: ``False`` (after).
-++-----+
-++-----+    Examples::
-++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-++-----+        >>> memory = torch.rand(10, 32, 512)
-++-----+        >>> tgt = torch.rand(20, 32, 512)
-++-----+        >>> out = decoder_layer(tgt, memory)
-++-----+    """
-++-----+    # Adapted from pytorch source
-++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-++-----+        super(TransformerDecoderLayer, self).__init__()
-++-----+        #Attention Mechanism
-++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-++-----+        # Implementation of Feedforward model
-++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-++-----+        self.dropout = nn.Dropout(dropout)
-++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-++-----+        #Normalization Layer and Dropout Layer
-++-----+        self.norm1 = nn.LayerNorm(d_model)
-++-----+        self.norm2 = nn.LayerNorm(d_model)
-++-----+        self.norm3 = nn.LayerNorm(d_model)
-++-----+        self.dropout1 = nn.Dropout(dropout)
-++-----+        self.dropout2 = nn.Dropout(dropout)
-++-----+        self.dropout3 = nn.Dropout(dropout)
-++-----+        #Activation Function
-++-----+        self.activation = nn.ReLU()
-++-----+    
-++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-++-----+        r"""Pass the input through the encoder layer.
-++-----+
-++-----+        Args:
-++-----+            tgt: the sequence to the decoder layer (required).
-++-----+            memory: the sequence from the last layer of the encoder (required).
-++-----+            tgt_mask: the mask for the tgt sequence (optional).
-++-----+            memory_mask: the mask for the memory sequence (optional).
-++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-++-----+
-++-----+        Shape:
-++-----+            see the docs in Transformer class.
-++-----+        """
-++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-++-----+        tgt = tgt + self.dropout1(tgt2)
-++-----+        tgt = self.norm1(tgt)
-++-----+
-++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-++-----+        tgt = tgt + self.dropout1(tgt2)
-++-----+        tgt = self.norm1(tgt)
-++-----+
-++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-++-----+        tgt = tgt + self.dropout2(tgt2)
-++-----+        tgt = self.norm2(tgt)
-++-----+        return tgt
-++-----+    
-++-----+
-++----- class MultiHeadAttention(nn.Module):
-++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-++-----     super().__init__()
-++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-++-----     else:
-++-----         self.relative_positional = None
-++----- 
-++------  def forward(self, x):
-++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-++-----     """Runs the multi-head self-attention layer.
-++----- 
-++-----     Args:
-++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-++-----       A single tensor containing the output from this layer
-++-----     """
-++----- 
-++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-++----- 
-++-----     if self.relative_positional is not None:
-+ -----
-+------['recognition_model.py', '--output_directory', './models/recognition_model/']
-++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+ -----train / dev split: 8055 200
-+-----diff --git a/output/log.txt b/output/log.txt
-+-----index ae42364..1d2cd8e 100644
-+-------- a/output/log.txt
-+-----+++ b/output/log.txt
-+-----@@ -1,3 +1,13 @@
-+-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++----diff --git a/recognition_model.py b/recognition_model.py
-++----index dea6d47..a46dff0 100644
-++------- a/recognition_model.py
-++----+++ b/recognition_model.py
-++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-+ ---- 
-+-----+diff --git a/output/log.txt b/output/log.txt
-+-----+index ae42364..8563980 100644
-+-----+--- a/output/log.txt
-+-----++++ b/output/log.txt
-+-----+@@ -1,3 +1,2 @@
-+-----++57f8139449dd9286c2203ec2eca118a550638a7c
-+-----+ 
-+-----+-
-+-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++----             X = combine_fixed_length(example['emg'], 200).to(device)
-++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+ ---- 
-+----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+-----diff --git a/transformer.py b/transformer.py
-+-----index 6743588..ac131be 100644
-+-------- a/transformer.py
-+-----+++ b/transformer.py
-+-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-+-----         Shape:
-+-----             see the docs in Transformer class.
-+-----         """
-+------        src2 = self.self_attn(src)
-+-----+        src2 = self.self_attn(src, src, src)
-+-----         src = src + self.dropout1(src2)
-+-----         src = self.norm1(src)
-+-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-+-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-+-----         src = self.norm2(src)
-+-----         return src
-++-----            pred = model(X, X_raw, sess)
-++----+            pred = model(X, X_raw, tgt, sess)
-++----             pred = F.log_softmax(pred, 2)
-+ ---- 
-+-----+class TransformerDecoderLayer(nn.Module):
-+-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-+-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-+-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-+-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-+-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-+-----+    in a different way during application.
-+-----+
-+-----+    Args:
-+-----+        d_model: the number of expected features in the input (required).
-+-----+        nhead: the number of heads in the multiheadattention models (required).
-+-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-+-----+        dropout: the dropout value (default=0.1).
-+-----+        activation: the activation function of the intermediate layer, can be a string
-+-----+            ("relu" or "gelu") or a unary callable. Default: relu
-+-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-+-----+        batch_first: If ``True``, then the input and output tensors are provided
-+-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-+-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-+-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-+-----+            Default: ``False`` (after).
-+-----+
-+-----+    Examples::
-+-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-+-----+        >>> memory = torch.rand(10, 32, 512)
-+-----+        >>> tgt = torch.rand(20, 32, 512)
-+-----+        >>> out = decoder_layer(tgt, memory)
-+-----+    """
-+-----+    # Adapted from pytorch source
-+-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-+-----+        super(TransformerDecoderLayer, self).__init__()
-+-----+        #Attention Mechanism
-+-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-+-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-+-----+        # Implementation of Feedforward model
-+-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-+-----+        self.dropout = nn.Dropout(dropout)
-+-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-+-----+        #Normalization Layer and Dropout Layer
-+-----+        self.norm1 = nn.LayerNorm(d_model)
-+-----+        self.norm2 = nn.LayerNorm(d_model)
-+-----+        self.norm3 = nn.LayerNorm(d_model)
-+-----+        self.dropout1 = nn.Dropout(dropout)
-+-----+        self.dropout2 = nn.Dropout(dropout)
-+-----+        self.dropout3 = nn.Dropout(dropout)
-+-----+        #Activation Function
-+-----+        self.activation = nn.ReLU()
-+-----+    
-+-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-+-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-+-----+        r"""Pass the input through the encoder layer.
-+-----+
-+-----+        Args:
-+-----+            tgt: the sequence to the decoder layer (required).
-+-----+            memory: the sequence from the last layer of the encoder (required).
-+-----+            tgt_mask: the mask for the tgt sequence (optional).
-+-----+            memory_mask: the mask for the memory sequence (optional).
-+-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-+-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-+-----+
-+-----+        Shape:
-+-----+            see the docs in Transformer class.
-+-----+        """
-+-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-+-----+        tgt = tgt + self.dropout1(tgt2)
-+-----+        tgt = self.norm1(tgt)
-+-----+
-+-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-+-----+        tgt = tgt + self.dropout1(tgt2)
-+-----+        tgt = self.norm1(tgt)
-+-----+
-+-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-+-----+        tgt = tgt + self.dropout2(tgt2)
-+-----+        tgt = self.norm2(tgt)
-+-----+        return tgt
-+-----+    
-+-----+
-+----- class MultiHeadAttention(nn.Module):
-+-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-+-----     super().__init__()
-+-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-+-----     else:
-+-----         self.relative_positional = None
-+----- 
-+------  def forward(self, x):
-+-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-+-----     """Runs the multi-head self-attention layer.
-+----- 
-+-----     Args:
-+-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-+-----       A single tensor containing the output from this layer
-+-----     """
-+----- 
-+------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-+------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-+------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-+-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-+-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-+-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-+-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+----- 
-+-----     if self.relative_positional is not None:
-++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-+ ----
-+ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+ ----train / dev split: 8055 200
-+ ---diff --git a/recognition_model.py b/recognition_model.py
-+----index dea6d47..a46dff0 100644
-++---index a46dff0..8fd300c 100644
-+ ------ a/recognition_model.py
-+ ---+++ b/recognition_model.py
-+----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-++---@@ -6,6 +6,7 @@ import subprocess
-++--- from ctcdecode import CTCBeamDecoder
-++--- import jiwer
-++--- import random
-++---+from torch.utils.tensorboard import SummaryWriter
-++--- 
-++--- import torch
-++--- from torch import nn
-++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-++--- 
-++--- from read_emg import EMGDataset, SizeAwareSampler
-++--- from architecture import Model
-++----from data_utils import combine_fixed_length, decollate_tensor
-++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-++--- from transformer import TransformerEncoderLayer
-++--- 
-++--- from absl import flags
-++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-++---     return jiwer.wer(references, predictions)
-++--- 
-++--- 
-++----def train_model(trainset, devset, device, n_epochs=200):
-++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++----
-++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-++---+    #Define Dataloader
-++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++--- 
-++---+    #Define model and loss function
-++---     n_chars = len(devset.text_transform.chars)
-++---     model = Model(devset.num_features, n_chars+1).to(device)
-++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-++--- 
-++---     if FLAGS.start_training_from is not None:
-++---         state_dict = torch.load(FLAGS.start_training_from)
-++---         model.load_state_dict(state_dict, strict=False)
-+ --- 
-++---+    #Define optimizer and scheduler for the learning rate
-++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-++--- 
-++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-++--- 
-++---     batch_idx = 0
-++---+    train_loss= 0
-++---+    eval_loss = 0
-++---     optim.zero_grad()
-++---     for epoch_idx in range(n_epochs):
-++---+        model.train()
-++---         losses = []
-++----        for example in dataloader:
-++---+        for example in dataloader_training:
-++---             schedule_lr(batch_idx)
-++--- 
-++---+            #Preprosessing of the input and target for the model
-+ ---             X = combine_fixed_length(example['emg'], 200).to(device)
-+ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-+ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-++--- 
-++---+            #Shifting target for input decoder and loss
-++---+            tgt= y[:,:-1]
-++---+            target= y[:,1:]
-++---+
-++---+            #Prediction
-++---             pred = model(X, X_raw, tgt, sess)
-++----            pred = F.log_softmax(pred, 2)
-++--- 
-++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-++---+            #Primary Loss
-++---+            pred=pred.permute(0,2,1)
-++---+            loss = loss_fn(pred, target)
-++---+
-++---+            #Auxiliary Loss
-++---+            #pred = F.log_softmax(pred, 2)
-++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++---             losses.append(loss.item())
-++---+            train_loss += loss.item()
-++--- 
-++---             loss.backward()
-++---             if (batch_idx+1) % 2 == 0:
-++---                 optim.step()
-++---                 optim.zero_grad()
-++--- 
-++---+            #Report plots in tensorboard
-++---+            if batch_idx % report_every == report_every - 2:     
-++---+                #Evaluation
-++---+                model.eval()
-++---+                with torch.no_grad():
-++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-++---+                        X_raw = example['raw_emg'].to(device)
-++---+                        sess = example['session_ids'].to(device)
-++---+                        y = example['text_int'].to(device)
-++---+
-++---+                        #Shifting target for input decoder and loss
-++---+                        tgt= y[:,:-1]
-++---+                        target= y[:,1:]
-++---+
-++---+                        #Prediction without the 197-th batch because of missing label
-++---+                        if idx != 197:
-++---+                            pred = model(X, X_raw, tgt, sess)
-++---+                            #Primary Loss
-++---+                            pred=pred.permute(0,2,1)
-++---+                            loss = loss_fn(pred, target)
-++---+                            eval_loss += loss.item()
-++---+
-++---+                #Writing on tensorboard
-++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-++---+                train_loss= 0
-++---+                eval_loss= 0
-++---+
-++---+            #Increment counter        
-++---             batch_idx += 1
-++----        train_loss = np.mean(losses)
-++---+
-++---+        #Testing and change learning rate
-++---         val = test(model, devset, device)
-++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-++---         lr_sched.step()
-++---+    
-++---+        #Logging
-++---+        train_loss = np.mean(losses)
-++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-++--- 
-++---@@ -148,8 +201,9 @@ def main():
-++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-++--- 
-++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-++---+    writer = SummaryWriter(log_dir="./content/runs")
-++--- 
-++----    model = train_model(trainset, devset, device)
-++---+    model = train_model(trainset, devset ,device, writer)
-+ --- 
-+-----            pred = model(X, X_raw, sess)
-+----+            pred = model(X, X_raw, tgt, sess)
-+----             pred = F.log_softmax(pred, 2)
-++--- if __name__ == '__main__':
-++---     FLAGS(sys.argv)
-++---diff --git a/transformer.py b/transformer.py
-++---index ac131be..51e1f2e 100644
-++------ a/transformer.py
-++---+++ b/transformer.py
-++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-++---     self.d_qkv = d_qkv
-+ --- 
-+----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-++---+    #self.kdim = kdim if kdim is not None else embed_dim
-++---+    #self.vdim = vdim if vdim is not None else embed_dim
-++---+
-++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+ ---
-+ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+ ---train / dev split: 8055 200
-+ --diff --git a/recognition_model.py b/recognition_model.py
-+---index a46dff0..8fd300c 100644
-++--index fde5a40..6d5143b 100644
-+ ----- a/recognition_model.py
-+ --+++ b/recognition_model.py
-+---@@ -6,6 +6,7 @@ import subprocess
-+--- from ctcdecode import CTCBeamDecoder
-+--- import jiwer
-+--- import random
-+---+from torch.utils.tensorboard import SummaryWriter
-+--- 
-+--- import torch
-+--- from torch import nn
-+---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-+--- 
-+--- from read_emg import EMGDataset, SizeAwareSampler
-+--- from architecture import Model
-+----from data_utils import combine_fixed_length, decollate_tensor
-+---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-+--- from transformer import TransformerEncoderLayer
-+--- 
-+--- from absl import flags
-+---@@ -62,17 +63,21 @@ def test(model, testset, device):
-++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-+ --     return jiwer.wer(references, predictions)
-+ -- 
-+ -- 
-+----def train_model(trainset, devset, device, n_epochs=200):
-+----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+----
-+---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-+---+    #Define Dataloader
-+---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-++--     #Define Dataloader
-++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-+ -- 
-+---+    #Define model and loss function
-++--     #Define model and loss function
-+ --     n_chars = len(devset.text_transform.chars)
-+---     model = Model(devset.num_features, n_chars+1).to(device)
-+---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-++---    model = Model(devset.num_features, n_chars+1).to(device)
-++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-+ -- 
-+ --     if FLAGS.start_training_from is not None:
-+---         state_dict = torch.load(FLAGS.start_training_from)
-+---         model.load_state_dict(state_dict, strict=False)
-+--- 
-+---+    #Define optimizer and scheduler for the learning rate
-+---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-+---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-+--- 
-+---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-+---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-+--- 
-+---     batch_idx = 0
-+---+    train_loss= 0
-+---+    eval_loss = 0
-+---     optim.zero_grad()
-+---     for epoch_idx in range(n_epochs):
-+---+        model.train()
-+---         losses = []
-+----        for example in dataloader:
-+---+        for example in dataloader_training:
-+---             schedule_lr(batch_idx)
-++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-++--             target= y[:,1:]
-+ -- 
-+---+            #Preprosessing of the input and target for the model
-+---             X = combine_fixed_length(example['emg'], 200).to(device)
-+---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-+---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-++--             #Prediction
-++---            pred = model(X, X_raw, tgt, sess)
-++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-+ -- 
-+---+            #Shifting target for input decoder and loss
-+---+            tgt= y[:,:-1]
-+---+            target= y[:,1:]
-+---+
-+---+            #Prediction
-+---             pred = model(X, X_raw, tgt, sess)
-+----            pred = F.log_softmax(pred, 2)
-++--             #Primary Loss
-++---            pred=pred.permute(0,2,1)
-++---            loss = loss_fn(pred, target)
-++--+            out_dec=out_dec.permute(0,2,1)
-++--+            loss_dec = loss_fn(out_dec, target)
-+ -- 
-+----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-+----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+---+            #Primary Loss
-+---+            pred=pred.permute(0,2,1)
-+---+            loss = loss_fn(pred, target)
-++--             #Auxiliary Loss
-++---            #pred = F.log_softmax(pred, 2)
-++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++--+            out_enc = F.log_softmax(out_enc, 2)
-++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+ --+
-+---+            #Auxiliary Loss
-+---+            #pred = F.log_softmax(pred, 2)
-+---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-+---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-+ --             losses.append(loss.item())
-+---+            train_loss += loss.item()
-++--             train_loss += loss.item()
-+ -- 
-+---             loss.backward()
-++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+ --             if (batch_idx+1) % 2 == 0:
-+ --                 optim.step()
-+ --                 optim.zero_grad()
-++---
-++---            if batch_idx % report_every == report_every - 2:     
-++--+            
-++--+            if False:
-++--+            #if batch_idx % report_every == report_every - 2:     
-++--                 #Evaluation
-++--                 model.eval()
-++--                 with torch.no_grad():
-++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-++---                        X_raw = example['raw_emg'].to(device)
-++---                        sess = example['session_ids'].to(device)
-++---                        y = example['text_int'].to(device)
-++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+ -- 
-+---+            #Report plots in tensorboard
-+---+            if batch_idx % report_every == report_every - 2:     
-+---+                #Evaluation
-+---+                model.eval()
-+---+                with torch.no_grad():
-+---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-+---+                        X_raw = example['raw_emg'].to(device)
-+---+                        sess = example['session_ids'].to(device)
-+---+                        y = example['text_int'].to(device)
-+---+
-+---+                        #Shifting target for input decoder and loss
-+---+                        tgt= y[:,:-1]
-+---+                        target= y[:,1:]
-+---+
-+---+                        #Prediction without the 197-th batch because of missing label
-+---+                        if idx != 197:
-+---+                            pred = model(X, X_raw, tgt, sess)
-+---+                            #Primary Loss
-+---+                            pred=pred.permute(0,2,1)
-+---+                            loss = loss_fn(pred, target)
-+---+                            eval_loss += loss.item()
-+---+
-+---+                #Writing on tensorboard
-+---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-+---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-+---+                train_loss= 0
-+---+                eval_loss= 0
-+---+
-+---+            #Increment counter        
-+---             batch_idx += 1
-+----        train_loss = np.mean(losses)
-+---+
-+---+        #Testing and change learning rate
-+---         val = test(model, devset, device)
-+---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-+---         lr_sched.step()
-+---+    
-+---+        #Logging
-+---+        train_loss = np.mean(losses)
-+---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-+---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-+--- 
-+---@@ -148,8 +201,9 @@ def main():
-+---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-+--- 
-+---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-+---+    writer = SummaryWriter(log_dir="./content/runs")
-++--                         #Shifting target for input decoder and loss
-++--                         tgt= y[:,:-1]
-++--                         target= y[:,1:]
-+ -- 
-+----    model = train_model(trainset, devset, device)
-+---+    model = train_model(trainset, devset ,device, writer)
-++--+                        print(idx)
-++--+
-++--                         #Prediction without the 197-th batch because of missing label
-++---                        if idx != 197:
-++--+                        if idx != 181:
-++--                             pred = model(X, X_raw, tgt, sess)
-++--                             #Primary Loss
-++--                             pred=pred.permute(0,2,1)
-++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+ -- 
-+--- if __name__ == '__main__':
-+---     FLAGS(sys.argv)
-+---diff --git a/transformer.py b/transformer.py
-+---index ac131be..51e1f2e 100644
-+------ a/transformer.py
-+---+++ b/transformer.py
-+---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-+---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-+---     self.d_qkv = d_qkv
-++--             #Increment counter        
-++--             batch_idx += 1
-++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-+ -- 
-+---+    #self.kdim = kdim if kdim is not None else embed_dim
-+---+    #self.vdim = vdim if vdim is not None else embed_dim
-+---+
-+---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++--         #Testing and change learning rate
-++--         val = test(model, devset, device)
-+ --
-+ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+ --train / dev split: 8055 200
-+ -diff --git a/recognition_model.py b/recognition_model.py
-+--index fde5a40..6d5143b 100644
-++-index 30c5ff2..2672d45 100644
-+ ---- a/recognition_model.py
-+ -+++ b/recognition_model.py
-+--@@ -63,14 +63,14 @@ def test(model, testset, device):
-+--     return jiwer.wer(references, predictions)
-+-- 
-+-- 
-+---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-+--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-+--     #Define Dataloader
-+--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-+ - 
-+ -     #Define model and loss function
-+ -     n_chars = len(devset.text_transform.chars)
-+---    model = Model(devset.num_features, n_chars+1).to(device)
-+--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-+ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-+ - 
-+ -     if FLAGS.start_training_from is not None:
-+--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+--             target= y[:,1:]
-++-diff --git a/transformer.py b/transformer.py
-++-index 51e1f2e..c125841 100644
-++---- a/transformer.py
-++-+++ b/transformer.py
-++-@@ -1,3 +1,4 @@
-++-+import math
-++- from typing import Optional
-+ - 
-+--             #Prediction
-+---            pred = model(X, X_raw, tgt, sess)
-+--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-++- import torch
-++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-++-         Shape:
-++-             see the docs in Transformer class.
-++-         """
-++--        src2 = self.self_attn(src, src, src)
-++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-++-         src = src + self.dropout1(src2)
-++-         src = self.norm1(src)
-++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-++-         Shape:
-++-             see the docs in Transformer class.
-++-         """
-++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-++-         tgt = tgt + self.dropout1(tgt2)
-++-         tgt = self.norm1(tgt)
-+ - 
-+--             #Primary Loss
-+---            pred=pred.permute(0,2,1)
-+---            loss = loss_fn(pred, target)
-+--+            out_dec=out_dec.permute(0,2,1)
-+--+            loss_dec = loss_fn(out_dec, target)
-++--        tgt2=self.multihead_attn(tgt, memory, memory)
-++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-++-         tgt = tgt + self.dropout1(tgt2)
-++-         tgt = self.norm1(tgt)
-+ - 
-+--             #Auxiliary Loss
-+---            #pred = F.log_softmax(pred, 2)
-+---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-+---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-+--+            out_enc = F.log_softmax(out_enc, 2)
-+--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-+--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+--+
-+--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-+--             losses.append(loss.item())
-+--             train_loss += loss.item()
-++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-++-     self.d_qkv = d_qkv
-+ - 
-+--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+--             if (batch_idx+1) % 2 == 0:
-+--                 optim.step()
-+--                 optim.zero_grad()
-++--    #self.kdim = kdim if kdim is not None else embed_dim
-++--    #self.vdim = vdim if vdim is not None else embed_dim
-+ --
-+---            if batch_idx % report_every == report_every - 2:     
-+--+            
-+--+            if False:
-+--+            #if batch_idx % report_every == report_every - 2:     
-+--                 #Evaluation
-+--                 model.eval()
-+--                 with torch.no_grad():
-+--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-+---                        X_raw = example['raw_emg'].to(device)
-+---                        sess = example['session_ids'].to(device)
-+---                        y = example['text_int'].to(device)
-+--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+-- 
-+--                         #Shifting target for input decoder and loss
-+--                         tgt= y[:,:-1]
-+--                         target= y[:,1:]
-++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-++-     else:
-++-         self.relative_positional = None
-+ - 
-+--+                        print(idx)
-+--+
-+--                         #Prediction without the 197-th batch because of missing label
-+---                        if idx != 197:
-+--+                        if idx != 181:
-+--                             pred = model(X, X_raw, tgt, sess)
-+--                             #Primary Loss
-+--                             pred=pred.permute(0,2,1)
-+--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-++-     """Runs the multi-head self-attention layer.
-+ - 
-+--             #Increment counter        
-+--             batch_idx += 1
-+--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-++-     Args:
-++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+ - 
-+--         #Testing and change learning rate
-+--         val = test(model, devset, device)
-++-+    if attn_mask is not None:
-++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++-+
-++-     if self.relative_positional is not None:
-++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-++-         l,b,h,d = q_pos.size()
-++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-++-             x = x.transpose(0, 1)
-++-             x = x.contiguous().view(bsz_heads, length+1, length)
-++-             return x[:, 1:, :]
-++-+        
-++-+
-++-+########
-++-+# Taken from:
-++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-++-+# or also here:
-++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-++-+class PositionalEncoding(nn.Module):
-++-+
-++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-++-+        super(PositionalEncoding, self).__init__()
-++-+        self.dropout = nn.Dropout(p=dropout)
-++-+        self.max_len = max_len
-++-+
-++-+        pe = torch.zeros(max_len, d_model)
-++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-++-+                             * (-math.log(10000.0) / d_model))
-++-+        pe[:, 0::2] = torch.sin(position * div_term)
-++-+        pe[:, 1::2] = torch.cos(position * div_term)
-++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-++-+        self.register_buffer('pe', pe)  # Will not be trained.
-++-+
-++-+    def forward(self, x):
-++-+        """Inputs of forward function
-++-+        Args:
-++-+            x: the sequence fed to the positional encoder model (required).
-++-+        Shape:
-++-+            x: [sequence length, batch size, embed dim]
-++-+            output: [sequence length, batch size, embed dim]
-++-+        """
-++-+        assert x.size(0) < self.max_len, (
-++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-++-+        # shape of x (len, B, dim)
-++-+        x = x + self.pe[:x.size(0), :]
-++-+        return self.dropout(x)
-+ -
-+ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+ -train / dev split: 8055 200
-++diff --git a/output/log.txt b/output/log.txt
-++index 1d2cd8e..979357b 100644
-++--- a/output/log.txt
-+++++ b/output/log.txt
-++@@ -1,13 +1,1651 @@
-++-57f8139449dd9286c2203ec2eca118a550638a7c
-+++be71135adc89793578f304adb405cea80a5b2b9a
-++ 
-+++diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++index 2243f2d..a2c8558 100644
-+++--- a/models/recognition_model/log.txt
-++++++ b/models/recognition_model/log.txt
-+++@@ -1,844 +1,898 @@
-+++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-++++be71135adc89793578f304adb405cea80a5b2b9a
-+++ 
-+++-diff --git a/architecture.py b/architecture.py
-+++-index 2413a8a..94d0de0 100644
-+++---- a/architecture.py
-+++-+++ b/architecture.py
-+++-@@ -4,7 +4,7 @@ import torch
-+++- from torch import nn
-+++- import torch.nn.functional as F
-+++- 
-+++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-+++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-+++- 
-+++- from absl import flags
-+++- FLAGS = flags.FLAGS
-+++-@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-+++-         return F.relu(x + res)
-+++- 
-+++- class Model(nn.Module):
-+++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-+++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-+++-         super().__init__()
-+++- 
-+++-         self.conv_blocks = nn.Sequential(
-+++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-+++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+++- 
-+++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-+++- 
-+++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-+++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+++- 
-+++-         self.has_aux_loss = has_aux_loss
-+++--
-+++--    def forward(self, x_feat, x_raw, y,session_ids):
-+++-+        self.device=device
-+++-+
-+++-+    def create_src_padding_mask(self, src):
-+++-+        # input src of shape ()
-+++-+        src_padding_mask = src.transpose(1, 0) == 0
-+++-+        return src_padding_mask
-+++-+
-+++-+    def create_tgt_padding_mask(self, tgt):
-+++-+        # input tgt of shape ()
-+++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-+++-+        return tgt_padding_mask
-+++-+    
-+++-+    def forward(self, x_feat, x_raw, y, session_ids):
-+++-         # x shape is (batch, time, electrode)
-+++-+        # y shape is (batch, sequence_length)
-+++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-+++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-+++-+        memory_key_padding_mask = src_key_padding_mask
-+++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-+++- 
-+++-         if self.training:
-+++-             r = random.randrange(8)
-+++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-+++-         x_raw = self.conv_blocks(x_raw)
-+++-         x_raw = x_raw.transpose(1,2)
-+++-         x_raw = self.w_raw_in(x_raw)
-+++--
-+++-         x = x_raw
-+++-+
-+++-+        #Embedding and positional encoding of tgt
-+++-         tgt=self.embedding_tgt(y)
-+++-+        tgt=self.pos_encoder(tgt)
-+++- 
-+++-         x = x.transpose(0,1) # put time first
-+++-         tgt = tgt.transpose(0,1) # put channel after
-+++--        x_encoder = self.transformerEncoder(x)
-+++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-+++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-+++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-+++- 
-+++-         x_encoder = x_encoder.transpose(0,1)
-+++-         x_decoder = x_decoder.transpose(0,1)
-+++ diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++-index 53839a0..1343d7d 100644
-++++index 2243f2d..342fccd 100644
-+++ --- a/models/recognition_model/log.txt
-+++ +++ b/models/recognition_model/log.txt
-+++-@@ -1,639 +1,2 @@
-+++--2fa943cd85263a152b6be80d502eda27932ebb27
-+++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-++++@@ -1,845 +1,2 @@
-++++-dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-+++++be71135adc89793578f304adb405cea80a5b2b9a
-+++  
-+++ -diff --git a/architecture.py b/architecture.py
-+++--index a8c70f3..2413a8a 100644
-++++-index 2413a8a..94d0de0 100644
-+++ ---- a/architecture.py
-+++ -+++ b/architecture.py
-++++-@@ -4,7 +4,7 @@ import torch
-++++- from torch import nn
-++++- import torch.nn.functional as F
-++++- 
-++++--from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-++++-+from transformer import TransformerEncoderLayer, TransformerDecoderLayer, PositionalEncoding
-++++- 
-++++- from absl import flags
-++++- FLAGS = flags.FLAGS
-+++ -@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-+++ -         return F.relu(x + res)
-+++ - 
-+++ - class Model(nn.Module):
-+++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-+++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++++--    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++++-+    def __init__(self, num_features, num_outs, device ,has_aux_loss=False):
-+++ -         super().__init__()
-+++ - 
-+++ -         self.conv_blocks = nn.Sequential(
-+++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-+++--         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+++--         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++++-@@ -52,6 +52,7 @@ class Model(nn.Module):
-++++-         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+++ - 
-+++---        self.has_aux_out = num_aux_outs is not None
-+++---        if self.has_aux_out:
-+++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-+++--+        self.has_aux_loss = has_aux_loss
-++++-         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++++-+        self.pos_encoder = PositionalEncoding(FLAGS.model_size)
-+++ - 
-+++--     def forward(self, x_feat, x_raw, y,session_ids):
-+++--         # x shape is (batch, time, electrode)
-+++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-++++-         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++-         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++-@@ -60,9 +61,25 @@ class Model(nn.Module):
-++++-         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+++ - 
-+++--         x = x.transpose(0,1) # put time first
-+++--         tgt = tgt.transpose(0,1) # put channel after
-+++---        x = self.transformerEncoder(x)
-+++---        x = self.transformerDecoder(tgt, x)
-+++---        x = x.transpose(0,1)
-+++--+        x_encoder = self.transformerEncoder(x)
-+++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++++-         self.has_aux_loss = has_aux_loss
-++++--
-++++--    def forward(self, x_feat, x_raw, y,session_ids):
-++++-+        self.device=device
-++++-+
-++++-+    def create_src_padding_mask(self, src):
-++++-+        # input src of shape ()
-++++-+        src_padding_mask = src.transpose(1, 0) == 0
-++++-+        return src_padding_mask
-++++-+
-++++-+    def create_tgt_padding_mask(self, tgt):
-++++-+        # input tgt of shape ()
-++++-+        tgt_padding_mask = tgt.transpose(1, 0) == 0
-++++-+        return tgt_padding_mask
-++++-+    
-++++-+    def forward(self, x_feat, x_raw, y, session_ids):
-++++-         # x shape is (batch, time, electrode)
-++++-+        # y shape is (batch, sequence_length)
-++++-+        src_key_padding_mask = self.create_src_padding_mask(x_raw).to(self.device)
-++++-+        tgt_key_padding_mask = self.create_tgt_padding_mask(y).to(self.device)
-++++-+        memory_key_padding_mask = src_key_padding_mask
-++++-+        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self, y.shape[1]).to(self.device)
-+++ - 
-+++---        if self.has_aux_out:
-+++---            return self.w_out(x), self.w_aux(x)
-+++--+        x_encoder = x_encoder.transpose(0,1)
-+++--+        x_decoder = x_decoder.transpose(0,1)
-++++-         if self.training:
-++++-             r = random.randrange(8)
-++++-@@ -74,14 +91,16 @@ class Model(nn.Module):
-++++-         x_raw = self.conv_blocks(x_raw)
-++++-         x_raw = x_raw.transpose(1,2)
-++++-         x_raw = self.w_raw_in(x_raw)
-++++--
-++++-         x = x_raw
-+++ -+
-+++--+        if self.has_aux_loss:
-+++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-+++--         else:
-+++--             return self.w_out(x)
-++++-+        #Embedding and positional encoding of tgt
-++++-         tgt=self.embedding_tgt(y)
-++++-+        tgt=self.pos_encoder(tgt)
-+++ - 
-+++--diff --git a/data_utils.py b/data_utils.py
-+++--index e2632e8..8b05213 100644
-+++----- a/data_utils.py
-+++--+++ b/data_utils.py
-+++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-++++-         x = x.transpose(0,1) # put time first
-++++-         tgt = tgt.transpose(0,1) # put channel after
-++++--        x_encoder = self.transformerEncoder(x)
-++++--        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++++-+        x_encoder = self.transformerEncoder(x,src_key_padding_mask=src_key_padding_mask)
-++++-+        x_decoder = self.transformerDecoder(tgt, x_encoder,tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask=tgt_mask)
-+++ - 
-+++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-+++--     total_length = sum(t.size(0) for t in tensor_list)
-+++--+    tensor_list = list(tensor_list) # copy
-+++--     if total_length % n_batch != 0:
-+++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-+++---        tensor_list = list(tensor_list) # copy
-+++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-+++--         total_length += pad_length
-+++--     tensor = torch.cat(tensor_list, 0)
-++++-         x_encoder = x_encoder.transpose(0,1)
-++++-         x_decoder = x_decoder.transpose(0,1)
-+++ -diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++--index b8f7791..617dd85 100644
-++++-index 53839a0..1343d7d 100644
-+++ ---- a/models/recognition_model/log.txt
-+++ -+++ b/models/recognition_model/log.txt
-+++--@@ -1,480 +1,2 @@
-+++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-+++--+2fa943cd85263a152b6be80d502eda27932ebb27
-++++-@@ -1,639 +1,2 @@
-++++--2fa943cd85263a152b6be80d502eda27932ebb27
-++++-+dbd4435b81bcbaf1460328c2ba3e2638b53f2404
-+++ - 
-+++ --diff --git a/architecture.py b/architecture.py
-+++---index d6e99b4..a8c70f3 100644
-++++--index a8c70f3..2413a8a 100644
-+++ ----- a/architecture.py
-+++ --+++ b/architecture.py
-+++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-+++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++++--@@ -41,7 +41,7 @@ class ResBlock(nn.Module):
-++++--         return F.relu(x + res)
-++++-- 
-++++-- class Model(nn.Module):
-++++---    def __init__(self, num_features, num_outs, num_aux_outs=None):
-++++--+    def __init__(self, num_features, num_outs, has_aux_loss=False):
-++++--         super().__init__()
-+++ -- 
-+++---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++++--         self.conv_blocks = nn.Sequential(
-++++--@@ -59,9 +59,7 @@ class Model(nn.Module):
-+++ --         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+++ --         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++++-- 
-++++---        self.has_aux_out = num_aux_outs is not None
-++++---        if self.has_aux_out:
-++++---            self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-++++--+        self.has_aux_loss = has_aux_loss
-++++-- 
-++++--     def forward(self, x_feat, x_raw, y,session_ids):
-++++--         # x shape is (batch, time, electrode)
-++++--@@ -82,12 +80,14 @@ class Model(nn.Module):
-++++-- 
-++++--         x = x.transpose(0,1) # put time first
-++++--         tgt = tgt.transpose(0,1) # put channel after
-++++---        x = self.transformerEncoder(x)
-++++---        x = self.transformerDecoder(tgt, x)
-++++---        x = x.transpose(0,1)
-++++--+        x_encoder = self.transformerEncoder(x)
-++++--+        x_decoder = self.transformerDecoder(tgt, x_encoder)
-++++-- 
-++++---        if self.has_aux_out:
-++++---            return self.w_out(x), self.w_aux(x)
-++++--+        x_encoder = x_encoder.transpose(0,1)
-++++--+        x_decoder = x_decoder.transpose(0,1)
-++++--+
-++++--+        if self.has_aux_loss:
-++++--+            return self.w_out(x_encoder), self.w_out(x_decoder)
-++++--         else:
-++++--             return self.w_out(x)
-++++-- 
-+++ --diff --git a/data_utils.py b/data_utils.py
-+++---index e4ac852..e2632e8 100644
-++++--index e2632e8..8b05213 100644
-+++ ----- a/data_utils.py
-+++ --+++ b/data_utils.py
-+++---@@ -1,3 +1,4 @@
-+++---+import math
-+++--- import string
-++++--@@ -169,9 +169,9 @@ def combine_fixed_length(tensor_list, length):
-+++ -- 
-+++--- import numpy as np
-+++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-+++---     n = total_length // length
-+++---     return tensor.view(n, length, *tensor.size()[1:])
-+++--- 
-+++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-+++---+    total_length = sum(t.size(0) for t in tensor_list)
-+++---+    if total_length % n_batch != 0:
-+++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-+++---+        tensor_list = list(tensor_list) # copy
-+++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-+++---+        total_length += pad_length
-+++---+    tensor = torch.cat(tensor_list, 0)
-+++---+    length = total_length // n_batch
-+++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-+++---+
-+++--- def decollate_tensor(tensor, lengths):
-+++---     b, s, d = tensor.size()
-+++---     tensor = tensor.view(b*s, d)
-++++-- def combine_fixed_length_tgt(tensor_list, n_batch):
-++++--     total_length = sum(t.size(0) for t in tensor_list)
-++++--+    tensor_list = list(tensor_list) # copy
-++++--     if total_length % n_batch != 0:
-++++--         pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-++++---        tensor_list = list(tensor_list) # copy
-++++--         tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-++++--         total_length += pad_length
-++++--     tensor = torch.cat(tensor_list, 0)
-+++ --diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++---index e890f0f..1ee3421 100644
-++++--index b8f7791..617dd85 100644
-+++ ----- a/models/recognition_model/log.txt
-+++ --+++ b/models/recognition_model/log.txt
-+++---@@ -1,265 +1,2 @@
-+++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-+++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-++++--@@ -1,480 +1,2 @@
-++++---902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-++++--+2fa943cd85263a152b6be80d502eda27932ebb27
-+++ -- 
-+++ ---diff --git a/architecture.py b/architecture.py
-+++----index b22af61..d6e99b4 100644
-++++---index d6e99b4..a8c70f3 100644
-+++ ------ a/architecture.py
-+++ ---+++ b/architecture.py
-+++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-+++----         )
-+++----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-++++---@@ -54,7 +54,7 @@ class Model(nn.Module):
-++++---         self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+++ --- 
-+++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-+++----+
-+++ ---         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++----        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++---+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=False, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++ ---         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-+++----         if self.has_aux_out:
-+++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-+++---- 
-+++-----    def forward(self, x_feat, x_raw, session_ids):
-+++----+    def forward(self, x_feat, x_raw, y,session_ids):
-+++----         # x shape is (batch, time, electrode)
-+++---- 
-+++----         if self.training:
-+++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-+++----         x_raw = self.w_raw_in(x_raw)
-+++---- 
-+++----         x = x_raw
-+++----+        tgt=self.embedding_tgt(y)
-+++---- 
-+++----         x = x.transpose(0,1) # put time first
-+++----+        tgt = tgt.transpose(0,1) # put channel after
-+++----         x = self.transformerEncoder(x)
-+++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-+++----+        x = self.transformerDecoder(tgt, x)
-+++----         x = x.transpose(0,1)
-+++---- 
-+++----         if self.has_aux_out:
-++++---         self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-++++---         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-+++ ---diff --git a/data_utils.py b/data_utils.py
-+++----index 11d4805..e4ac852 100644
-++++---index e4ac852..e2632e8 100644
-+++ ------ a/data_utils.py
-+++ ---+++ b/data_utils.py
-+++----@@ -244,6 +244,7 @@ class TextTransform(object):
-+++----     def __init__(self):
-+++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-+++----         self.chars = string.ascii_lowercase+string.digits+' '
-+++----+        self.vocabulary_size=len(self.chars)
-++++---@@ -1,3 +1,4 @@
-++++---+import math
-++++--- import string
-++++--- 
-++++--- import numpy as np
-++++---@@ -166,6 +167,17 @@ def combine_fixed_length(tensor_list, length):
-++++---     n = total_length // length
-++++---     return tensor.view(n, length, *tensor.size()[1:])
-+++ --- 
-+++----     def clean_text(self, text):
-+++----         text = unidecode(text)
-++++---+def combine_fixed_length_tgt(tensor_list, n_batch):
-++++---+    total_length = sum(t.size(0) for t in tensor_list)
-++++---+    if total_length % n_batch != 0:
-++++---+        pad_length = (math.ceil(total_length / n_batch) * n_batch) - total_length
-++++---+        tensor_list = list(tensor_list) # copy
-++++---+        tensor_list.append(torch.zeros(pad_length,*tensor_list[0].size()[1:], dtype=tensor_list[0].dtype, device=tensor_list[0].device))
-++++---+        total_length += pad_length
-++++---+    tensor = torch.cat(tensor_list, 0)
-++++---+    length = total_length // n_batch
-++++---+    return tensor.view(n_batch, length, *tensor.size()[1:])
-++++---+
-++++--- def decollate_tensor(tensor, lengths):
-++++---     b, s, d = tensor.size()
-++++---     tensor = tensor.view(b*s, d)
-+++ ---diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++----index fbc0abb..400061a 100644
-++++---index e890f0f..1ee3421 100644
-+++ ------ a/models/recognition_model/log.txt
-+++ ---+++ b/models/recognition_model/log.txt
-+++----@@ -1,188 +1,2 @@
-+++-----57f8139449dd9286c2203ec2eca118a550638a7c
-+++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-++++---@@ -1,265 +1,2 @@
-++++----031b80598b18e602b7f2b8d237d6b2f8d1246c05
-++++---+902535fc5cfdd7fb2dfb7da551aae421cc4f87e8
-+++ --- 
-+++ ----diff --git a/architecture.py b/architecture.py
-+++-----index 4fc3793..b22af61 100644
-++++----index b22af61..d6e99b4 100644
-+++ ------- a/architecture.py
-+++ ----+++ b/architecture.py
-+++-----@@ -4,7 +4,7 @@ import torch
-+++----- from torch import nn
-+++----- import torch.nn.functional as F
-+++----- 
-+++------from transformer import TransformerEncoderLayer
-+++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-+++----- 
-+++----- from absl import flags
-+++----- FLAGS = flags.FLAGS
-+++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-++++----@@ -51,6 +51,8 @@ class Model(nn.Module):
-++++----         )
-+++ ----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-+++ ---- 
-++++----+        self.embedding_tgt = nn.Embedding(num_outs,FLAGS.model_size, padding_idx=0)#TODO insert the padding ID
-++++----+
-+++ ----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-+++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-+++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-+++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++++----         decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++----         self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++++----@@ -61,7 +63,7 @@ class Model(nn.Module):
-++++----         if self.has_aux_out:
-++++----             self.w_aux = nn.Linear(FLAGS.model_size, num_aux_outs)
-++++---- 
-++++-----    def forward(self, x_feat, x_raw, session_ids):
-++++----+    def forward(self, x_feat, x_raw, y,session_ids):
-++++----         # x shape is (batch, time, electrode)
-++++---- 
-++++----         if self.training:
-++++----@@ -76,10 +78,12 @@ class Model(nn.Module):
-++++----         x_raw = self.w_raw_in(x_raw)
-+++ ---- 
-+++-----         self.has_aux_out = num_aux_outs is not None
-+++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-+++ ----         x = x_raw
-++++----+        tgt=self.embedding_tgt(y)
-+++ ---- 
-+++ ----         x = x.transpose(0,1) # put time first
-+++------        x = self.transformer(x)
-+++-----+        x = self.transformerEncoder(x)
-+++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-++++----+        tgt = tgt.transpose(0,1) # put channel after
-++++----         x = self.transformerEncoder(x)
-++++-----        x = self.transformerDecoder(x) #TODO I need the target EMG
-++++----+        x = self.transformerDecoder(tgt, x)
-+++ ----         x = x.transpose(0,1)
-+++ ---- 
-+++ ----         if self.has_aux_out:
-++++----diff --git a/data_utils.py b/data_utils.py
-++++----index 11d4805..e4ac852 100644
-++++------- a/data_utils.py
-++++----+++ b/data_utils.py
-++++----@@ -244,6 +244,7 @@ class TextTransform(object):
-++++----     def __init__(self):
-++++----         self.transformation = jiwer.Compose([jiwer.RemovePunctuation(), jiwer.ToLowerCase()])
-++++----         self.chars = string.ascii_lowercase+string.digits+' '
-++++----+        self.vocabulary_size=len(self.chars)
-++++---- 
-++++----     def clean_text(self, text):
-++++----         text = unidecode(text)
-+++ ----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-+++-----index 571de9d..8563980 100644
-++++----index fbc0abb..400061a 100644
-+++ ------- a/models/recognition_model/log.txt
-+++ ----+++ b/models/recognition_model/log.txt
-+++-----@@ -1,5 +1,2 @@
-+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++++----@@ -1,188 +1,2 @@
-++++-----57f8139449dd9286c2203ec2eca118a550638a7c
-++++----+031b80598b18e602b7f2b8d237d6b2f8d1246c05
-+++ ---- 
-++++-----diff --git a/architecture.py b/architecture.py
-++++-----index 4fc3793..b22af61 100644
-++++-------- a/architecture.py
-++++-----+++ b/architecture.py
-++++-----@@ -4,7 +4,7 @@ import torch
-++++----- from torch import nn
-++++----- import torch.nn.functional as F
-++++----- 
-++++------from transformer import TransformerEncoderLayer
-++++-----+from transformer import TransformerEncoderLayer, TransformerDecoderLayer
-++++----- 
-++++----- from absl import flags
-++++----- FLAGS = flags.FLAGS
-++++-----@@ -52,7 +52,9 @@ class Model(nn.Module):
-++++-----         self.w_raw_in = nn.Linear(FLAGS.model_size, FLAGS.model_size)
-++++----- 
-++++-----         encoder_layer = TransformerEncoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++------        self.transformer = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++++-----+        decoder_layer = TransformerDecoderLayer(d_model=FLAGS.model_size, nhead=8, relative_positional=True, relative_positional_distance=100, dim_feedforward=3072, dropout=FLAGS.dropout)
-++++-----+        self.transformerEncoder = nn.TransformerEncoder(encoder_layer, FLAGS.num_layers)
-++++-----+        self.transformerDecoder = nn.TransformerDecoder(decoder_layer, FLAGS.num_layers)
-++++-----         self.w_out = nn.Linear(FLAGS.model_size, num_outs)
-++++----- 
-++++-----         self.has_aux_out = num_aux_outs is not None
-++++-----@@ -76,7 +78,8 @@ class Model(nn.Module):
-++++-----         x = x_raw
-++++----- 
-++++-----         x = x.transpose(0,1) # put time first
-++++------        x = self.transformer(x)
-++++-----+        x = self.transformerEncoder(x)
-++++-----+        x = self.transformerDecoder(x) #TODO I need the target EMG
-++++-----         x = x.transpose(0,1)
-++++----- 
-++++-----         if self.has_aux_out:
-++++-----diff --git a/models/recognition_model/log.txt b/models/recognition_model/log.txt
-++++-----index 571de9d..8563980 100644
-++++-------- a/models/recognition_model/log.txt
-++++-----+++ b/models/recognition_model/log.txt
-++++-----@@ -1,5 +1,2 @@
-++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++++----- 
-++++------
-++++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-++++------output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-++++------train / dev split: 8055 200
-++++-----diff --git a/output/log.txt b/output/log.txt
-++++-----index ae42364..1d2cd8e 100644
-++++-------- a/output/log.txt
-++++-----+++ b/output/log.txt
-++++-----@@ -1,3 +1,13 @@
-++++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++++----- 
-++++-----+diff --git a/output/log.txt b/output/log.txt
-++++-----+index ae42364..8563980 100644
-++++-----+--- a/output/log.txt
-++++-----++++ b/output/log.txt
-++++-----+@@ -1,3 +1,2 @@
-++++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-++++-----+ 
-++++-----+-
-++++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++++----- 
-++++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++++-----diff --git a/transformer.py b/transformer.py
-++++-----index 6743588..ac131be 100644
-++++-------- a/transformer.py
-++++-----+++ b/transformer.py
-++++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-++++-----         Shape:
-++++-----             see the docs in Transformer class.
-++++-----         """
-++++------        src2 = self.self_attn(src)
-++++-----+        src2 = self.self_attn(src, src, src)
-++++-----         src = src + self.dropout1(src2)
-++++-----         src = self.norm1(src)
-++++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-++++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-++++-----         src = self.norm2(src)
-++++-----         return src
-++++----- 
-++++-----+class TransformerDecoderLayer(nn.Module):
-++++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-++++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-++++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-++++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-++++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-++++-----+    in a different way during application.
-++++-----+
-++++-----+    Args:
-++++-----+        d_model: the number of expected features in the input (required).
-++++-----+        nhead: the number of heads in the multiheadattention models (required).
-++++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-++++-----+        dropout: the dropout value (default=0.1).
-++++-----+        activation: the activation function of the intermediate layer, can be a string
-++++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-++++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-++++-----+        batch_first: If ``True``, then the input and output tensors are provided
-++++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-++++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-++++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-++++-----+            Default: ``False`` (after).
-++++-----+
-++++-----+    Examples::
-++++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-++++-----+        >>> memory = torch.rand(10, 32, 512)
-++++-----+        >>> tgt = torch.rand(20, 32, 512)
-++++-----+        >>> out = decoder_layer(tgt, memory)
-++++-----+    """
-++++-----+    # Adapted from pytorch source
-++++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-++++-----+        super(TransformerDecoderLayer, self).__init__()
-++++-----+        #Attention Mechanism
-++++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-++++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-++++-----+        # Implementation of Feedforward model
-++++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-++++-----+        self.dropout = nn.Dropout(dropout)
-++++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-++++-----+        #Normalization Layer and Dropout Layer
-++++-----+        self.norm1 = nn.LayerNorm(d_model)
-++++-----+        self.norm2 = nn.LayerNorm(d_model)
-++++-----+        self.norm3 = nn.LayerNorm(d_model)
-++++-----+        self.dropout1 = nn.Dropout(dropout)
-++++-----+        self.dropout2 = nn.Dropout(dropout)
-++++-----+        self.dropout3 = nn.Dropout(dropout)
-++++-----+        #Activation Function
-++++-----+        self.activation = nn.ReLU()
-++++-----+    
-++++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-++++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-++++-----+        r"""Pass the input through the encoder layer.
-++++-----+
-++++-----+        Args:
-++++-----+            tgt: the sequence to the decoder layer (required).
-++++-----+            memory: the sequence from the last layer of the encoder (required).
-++++-----+            tgt_mask: the mask for the tgt sequence (optional).
-++++-----+            memory_mask: the mask for the memory sequence (optional).
-++++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-++++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-++++-----+
-++++-----+        Shape:
-++++-----+            see the docs in Transformer class.
-++++-----+        """
-++++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-++++-----+        tgt = tgt + self.dropout1(tgt2)
-++++-----+        tgt = self.norm1(tgt)
-++++-----+
-++++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-++++-----+        tgt = tgt + self.dropout1(tgt2)
-++++-----+        tgt = self.norm1(tgt)
-++++-----+
-++++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-++++-----+        tgt = tgt + self.dropout2(tgt2)
-++++-----+        tgt = self.norm2(tgt)
-++++-----+        return tgt
-++++-----+    
-++++-----+
-++++----- class MultiHeadAttention(nn.Module):
-++++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-++++-----     super().__init__()
-++++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-++++-----     else:
-++++-----         self.relative_positional = None
-++++----- 
-++++------  def forward(self, x):
-++++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-++++-----     """Runs the multi-head self-attention layer.
-++++----- 
-++++-----     Args:
-++++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-++++-----       A single tensor containing the output from this layer
-++++-----     """
-++++----- 
-++++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-++++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-++++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-++++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-++++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-++++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-++++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-++++----- 
-++++-----     if self.relative_positional is not None:
-+++ -----
-+++------['recognition_model.py', '--output_directory', './models/recognition_model/']
-++++-----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ -----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++ -----train / dev split: 8055 200
-+++-----diff --git a/output/log.txt b/output/log.txt
-+++-----index ae42364..1d2cd8e 100644
-+++-------- a/output/log.txt
-+++-----+++ b/output/log.txt
-+++-----@@ -1,3 +1,13 @@
-+++-----+57f8139449dd9286c2203ec2eca118a550638a7c
-++++----diff --git a/recognition_model.py b/recognition_model.py
-++++----index dea6d47..a46dff0 100644
-++++------- a/recognition_model.py
-++++----+++ b/recognition_model.py
-++++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-+++ ---- 
-+++-----+diff --git a/output/log.txt b/output/log.txt
-+++-----+index ae42364..8563980 100644
-+++-----+--- a/output/log.txt
-+++-----++++ b/output/log.txt
-+++-----+@@ -1,3 +1,2 @@
-+++-----++57f8139449dd9286c2203ec2eca118a550638a7c
-+++-----+ 
-+++-----+-
-+++-----+-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++++----             X = combine_fixed_length(example['emg'], 200).to(device)
-++++----             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-++++----             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+++ ---- 
-+++----- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+++-----diff --git a/transformer.py b/transformer.py
-+++-----index 6743588..ac131be 100644
-+++-------- a/transformer.py
-+++-----+++ b/transformer.py
-+++-----@@ -51,7 +51,7 @@ class TransformerEncoderLayer(nn.Module):
-+++-----         Shape:
-+++-----             see the docs in Transformer class.
-+++-----         """
-+++------        src2 = self.self_attn(src)
-+++-----+        src2 = self.self_attn(src, src, src)
-+++-----         src = src + self.dropout1(src2)
-+++-----         src = self.norm1(src)
-+++-----         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-+++-----@@ -59,6 +59,83 @@ class TransformerEncoderLayer(nn.Module):
-+++-----         src = self.norm2(src)
-+++-----         return src
-++++-----            pred = model(X, X_raw, sess)
-++++----+            pred = model(X, X_raw, tgt, sess)
-++++----             pred = F.log_softmax(pred, 2)
-+++ ---- 
-+++-----+class TransformerDecoderLayer(nn.Module):
-+++-----+    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
-+++-----+    This standard decoder layer is based on the paper "Attention Is All You Need".
-+++-----+    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
-+++-----+    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
-+++-----+    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
-+++-----+    in a different way during application.
-+++-----+
-+++-----+    Args:
-+++-----+        d_model: the number of expected features in the input (required).
-+++-----+        nhead: the number of heads in the multiheadattention models (required).
-+++-----+        dim_feedforward: the dimension of the feedforward network model (default=2048).
-+++-----+        dropout: the dropout value (default=0.1).
-+++-----+        activation: the activation function of the intermediate layer, can be a string
-+++-----+            ("relu" or "gelu") or a unary callable. Default: relu
-+++-----+        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
-+++-----+        batch_first: If ``True``, then the input and output tensors are provided
-+++-----+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
-+++-----+        norm_first: if ``True``, layer norm is done prior to self attention, multihead
-+++-----+            attention and feedforward operations, respectively. Otherwise it's done after.
-+++-----+            Default: ``False`` (after).
-+++-----+
-+++-----+    Examples::
-+++-----+        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
-+++-----+        >>> memory = torch.rand(10, 32, 512)
-+++-----+        >>> tgt = torch.rand(20, 32, 512)
-+++-----+        >>> out = decoder_layer(tgt, memory)
-+++-----+    """
-+++-----+    # Adapted from pytorch source
-+++-----+    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-+++-----+        super(TransformerDecoderLayer, self).__init__()
-+++-----+        #Attention Mechanism
-+++-----+        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-+++-----+        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout, relative_positional=relative_positional, relative_positional_distance=relative_positional_distance)
-+++-----+        # Implementation of Feedforward model
-+++-----+        self.linear1 = nn.Linear(d_model, dim_feedforward)
-+++-----+        self.dropout = nn.Dropout(dropout)
-+++-----+        self.linear2 = nn.Linear(dim_feedforward, d_model)
-+++-----+        #Normalization Layer and Dropout Layer
-+++-----+        self.norm1 = nn.LayerNorm(d_model)
-+++-----+        self.norm2 = nn.LayerNorm(d_model)
-+++-----+        self.norm3 = nn.LayerNorm(d_model)
-+++-----+        self.dropout1 = nn.Dropout(dropout)
-+++-----+        self.dropout2 = nn.Dropout(dropout)
-+++-----+        self.dropout3 = nn.Dropout(dropout)
-+++-----+        #Activation Function
-+++-----+        self.activation = nn.ReLU()
-+++-----+    
-+++-----+    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None,
-+++-----+                tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
-+++-----+        r"""Pass the input through the encoder layer.
-+++-----+
-+++-----+        Args:
-+++-----+            tgt: the sequence to the decoder layer (required).
-+++-----+            memory: the sequence from the last layer of the encoder (required).
-+++-----+            tgt_mask: the mask for the tgt sequence (optional).
-+++-----+            memory_mask: the mask for the memory sequence (optional).
-+++-----+            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
-+++-----+            memory_key_padding_mask: the mask for the memory keys per batch (optional).
-+++-----+
-+++-----+        Shape:
-+++-----+            see the docs in Transformer class.
-+++-----+        """
-+++-----+        tgt2 = self.self_attn(tgt, tgt, tgt)
-+++-----+        tgt = tgt + self.dropout1(tgt2)
-+++-----+        tgt = self.norm1(tgt)
-+++-----+
-+++-----+        tgt2=self.multihead_attn(tgt, memory, memory)
-+++-----+        tgt = tgt + self.dropout1(tgt2)
-+++-----+        tgt = self.norm1(tgt)
-+++-----+
-+++-----+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
-+++-----+        tgt = tgt + self.dropout2(tgt2)
-+++-----+        tgt = self.norm2(tgt)
-+++-----+        return tgt
-+++-----+    
-+++-----+
-+++----- class MultiHeadAttention(nn.Module):
-+++-----   def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional=True, relative_positional_distance=100):
-+++-----     super().__init__()
-+++-----@@ -84,7 +161,7 @@ class MultiHeadAttention(nn.Module):
-+++-----     else:
-+++-----         self.relative_positional = None
-+++----- 
-+++------  def forward(self, x):
-+++-----+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-+++-----     """Runs the multi-head self-attention layer.
-+++----- 
-+++-----     Args:
-+++-----@@ -93,9 +170,9 @@ class MultiHeadAttention(nn.Module):
-+++-----       A single tensor containing the output from this layer
-+++-----     """
-+++----- 
-+++------    q = torch.einsum('tbf,hfa->bhta', x, self.w_q)
-+++------    k = torch.einsum('tbf,hfa->bhta', x, self.w_k)
-+++------    v = torch.einsum('tbf,hfa->bhta', x, self.w_v)
-+++-----+    q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-+++-----+    k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-+++-----+    v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-+++-----     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+++----- 
-+++-----     if self.relative_positional is not None:
-++++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-+++ ----
-+++ ----['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ ----output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++ ----train / dev split: 8055 200
-+++ ---diff --git a/recognition_model.py b/recognition_model.py
-+++----index dea6d47..a46dff0 100644
-++++---index a46dff0..8fd300c 100644
-+++ ------ a/recognition_model.py
-+++ ---+++ b/recognition_model.py
-+++----@@ -95,9 +95,11 @@ def train_model(trainset, devset, device, n_epochs=200):
-++++---@@ -6,6 +6,7 @@ import subprocess
-++++--- from ctcdecode import CTCBeamDecoder
-++++--- import jiwer
-++++--- import random
-++++---+from torch.utils.tensorboard import SummaryWriter
-++++--- 
-++++--- import torch
-++++--- from torch import nn
-++++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-++++--- 
-++++--- from read_emg import EMGDataset, SizeAwareSampler
-++++--- from architecture import Model
-++++----from data_utils import combine_fixed_length, decollate_tensor
-++++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-++++--- from transformer import TransformerEncoderLayer
-++++--- 
-++++--- from absl import flags
-++++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-++++---     return jiwer.wer(references, predictions)
-++++--- 
-++++--- 
-++++----def train_model(trainset, devset, device, n_epochs=200):
-++++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++++----
-++++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-++++---+    #Define Dataloader
-++++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++++--- 
-++++---+    #Define model and loss function
-++++---     n_chars = len(devset.text_transform.chars)
-++++---     model = Model(devset.num_features, n_chars+1).to(device)
-++++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-++++--- 
-++++---     if FLAGS.start_training_from is not None:
-++++---         state_dict = torch.load(FLAGS.start_training_from)
-++++---         model.load_state_dict(state_dict, strict=False)
-++++--- 
-++++---+    #Define optimizer and scheduler for the learning rate
-++++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-++++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-++++--- 
-++++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-++++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-+++ --- 
-++++---     batch_idx = 0
-++++---+    train_loss= 0
-++++---+    eval_loss = 0
-++++---     optim.zero_grad()
-++++---     for epoch_idx in range(n_epochs):
-++++---+        model.train()
-++++---         losses = []
-++++----        for example in dataloader:
-++++---+        for example in dataloader_training:
-++++---             schedule_lr(batch_idx)
-++++--- 
-++++---+            #Preprosessing of the input and target for the model
-+++ ---             X = combine_fixed_length(example['emg'], 200).to(device)
-+++ ---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+++----+            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++----+            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-++++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-+++ ---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-++++--- 
-++++---+            #Shifting target for input decoder and loss
-++++---+            tgt= y[:,:-1]
-++++---+            target= y[:,1:]
-++++---+
-++++---+            #Prediction
-++++---             pred = model(X, X_raw, tgt, sess)
-++++----            pred = F.log_softmax(pred, 2)
-++++--- 
-++++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-++++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-++++---+            #Primary Loss
-++++---+            pred=pred.permute(0,2,1)
-++++---+            loss = loss_fn(pred, target)
-++++---+
-++++---+            #Auxiliary Loss
-++++---+            #pred = F.log_softmax(pred, 2)
-++++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-++++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++++---             losses.append(loss.item())
-++++---+            train_loss += loss.item()
-++++--- 
-++++---             loss.backward()
-++++---             if (batch_idx+1) % 2 == 0:
-++++---                 optim.step()
-++++---                 optim.zero_grad()
-++++--- 
-++++---+            #Report plots in tensorboard
-++++---+            if batch_idx % report_every == report_every - 2:     
-++++---+                #Evaluation
-++++---+                model.eval()
-++++---+                with torch.no_grad():
-++++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-++++---+                        X_raw = example['raw_emg'].to(device)
-++++---+                        sess = example['session_ids'].to(device)
-++++---+                        y = example['text_int'].to(device)
-++++---+
-++++---+                        #Shifting target for input decoder and loss
-++++---+                        tgt= y[:,:-1]
-++++---+                        target= y[:,1:]
-++++---+
-++++---+                        #Prediction without the 197-th batch because of missing label
-++++---+                        if idx != 197:
-++++---+                            pred = model(X, X_raw, tgt, sess)
-++++---+                            #Primary Loss
-++++---+                            pred=pred.permute(0,2,1)
-++++---+                            loss = loss_fn(pred, target)
-++++---+                            eval_loss += loss.item()
-++++---+
-++++---+                #Writing on tensorboard
-++++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-++++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-++++---+                train_loss= 0
-++++---+                eval_loss= 0
-++++---+
-++++---+            #Increment counter        
-++++---             batch_idx += 1
-++++----        train_loss = np.mean(losses)
-++++---+
-++++---+        #Testing and change learning rate
-++++---         val = test(model, devset, device)
-++++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-++++---         lr_sched.step()
-++++---+    
-++++---+        #Logging
-++++---+        train_loss = np.mean(losses)
-++++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-++++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-+++ --- 
-+++-----            pred = model(X, X_raw, sess)
-+++----+            pred = model(X, X_raw, tgt, sess)
-+++----             pred = F.log_softmax(pred, 2)
-++++---@@ -148,8 +201,9 @@ def main():
-++++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-+++ --- 
-+++----             pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-++++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-++++---+    writer = SummaryWriter(log_dir="./content/runs")
-++++--- 
-++++----    model = train_model(trainset, devset, device)
-++++---+    model = train_model(trainset, devset ,device, writer)
-++++--- 
-++++--- if __name__ == '__main__':
-++++---     FLAGS(sys.argv)
-++++---diff --git a/transformer.py b/transformer.py
-++++---index ac131be..51e1f2e 100644
-++++------ a/transformer.py
-++++---+++ b/transformer.py
-++++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-++++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-++++---     self.d_qkv = d_qkv
-++++--- 
-++++---+    #self.kdim = kdim if kdim is not None else embed_dim
-++++---+    #self.vdim = vdim if vdim is not None else embed_dim
-++++---+
-++++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++ ---
-+++ ---['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ ---output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++ ---train / dev split: 8055 200
-+++ --diff --git a/recognition_model.py b/recognition_model.py
-+++---index a46dff0..8fd300c 100644
-++++--index fde5a40..6d5143b 100644
-+++ ----- a/recognition_model.py
-+++ --+++ b/recognition_model.py
-+++---@@ -6,6 +6,7 @@ import subprocess
-+++--- from ctcdecode import CTCBeamDecoder
-+++--- import jiwer
-+++--- import random
-+++---+from torch.utils.tensorboard import SummaryWriter
-+++--- 
-+++--- import torch
-+++--- from torch import nn
-+++---@@ -13,7 +14,7 @@ import torch.nn.functional as F
-+++--- 
-+++--- from read_emg import EMGDataset, SizeAwareSampler
-+++--- from architecture import Model
-+++----from data_utils import combine_fixed_length, decollate_tensor
-+++---+from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-+++--- from transformer import TransformerEncoderLayer
-+++--- 
-+++--- from absl import flags
-+++---@@ -62,17 +63,21 @@ def test(model, testset, device):
-++++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-+++ --     return jiwer.wer(references, predictions)
-+++ -- 
-+++ -- 
-+++----def train_model(trainset, devset, device, n_epochs=200):
-+++----    dataloader = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+++----
-+++---+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-+++---+    #Define Dataloader
-+++---+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+++---+    dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-++++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-++++--     #Define Dataloader
-++++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-++++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-+++ -- 
-+++---+    #Define model and loss function
-++++--     #Define model and loss function
-+++ --     n_chars = len(devset.text_transform.chars)
-+++---     model = Model(devset.num_features, n_chars+1).to(device)
-+++---+    loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-++++---    model = Model(devset.num_features, n_chars+1).to(device)
-++++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-++++--     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-+++ -- 
-+++ --     if FLAGS.start_training_from is not None:
-+++---         state_dict = torch.load(FLAGS.start_training_from)
-+++---         model.load_state_dict(state_dict, strict=False)
-++++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-++++--             target= y[:,1:]
-+++ -- 
-+++---+    #Define optimizer and scheduler for the learning rate
-+++---     optim = torch.optim.AdamW(model.parameters(), lr=FLAGS.learning_rate, weight_decay=FLAGS.l2)
-+++---     lr_sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[125,150,175], gamma=.5)
-++++--             #Prediction
-++++---            pred = model(X, X_raw, tgt, sess)
-++++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-+++ -- 
-+++---@@ -87,35 +92,83 @@ def train_model(trainset, devset, device, n_epochs=200):
-+++---             set_lr(iteration*target_lr/FLAGS.learning_rate_warmup)
-++++--             #Primary Loss
-++++---            pred=pred.permute(0,2,1)
-++++---            loss = loss_fn(pred, target)
-++++--+            out_dec=out_dec.permute(0,2,1)
-++++--+            loss_dec = loss_fn(out_dec, target)
-+++ -- 
-+++---     batch_idx = 0
-+++---+    train_loss= 0
-+++---+    eval_loss = 0
-+++---     optim.zero_grad()
-+++---     for epoch_idx in range(n_epochs):
-+++---+        model.train()
-+++---         losses = []
-+++----        for example in dataloader:
-+++---+        for example in dataloader_training:
-+++---             schedule_lr(batch_idx)
-+++--- 
-+++---+            #Preprosessing of the input and target for the model
-+++---             X = combine_fixed_length(example['emg'], 200).to(device)
-+++---             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+++----            y=nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++----            tgt = combine_fixed_length(example['text_int'], 27).to(device)#TODO ???
-+++---             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+++---+            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+++--- 
-+++---+            #Shifting target for input decoder and loss
-+++---+            tgt= y[:,:-1]
-+++---+            target= y[:,1:]
-+++---+
-+++---+            #Prediction
-+++---             pred = model(X, X_raw, tgt, sess)
-+++----            pred = F.log_softmax(pred, 2)
-+++--- 
-+++----            pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['lengths']), batch_first=False) # seq first, as required by ctc
-+++----            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++----            loss = F.ctc_loss(pred, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+++---+            #Primary Loss
-+++---+            pred=pred.permute(0,2,1)
-+++---+            loss = loss_fn(pred, target)
-++++--             #Auxiliary Loss
-++++---            #pred = F.log_softmax(pred, 2)
-++++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-++++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++++--+            out_enc = F.log_softmax(out_enc, 2)
-++++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-++++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-++++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+++ --+
-+++---+            #Auxiliary Loss
-+++---+            #pred = F.log_softmax(pred, 2)
-+++---+            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-+++---+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++---+            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-++++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-+++ --             losses.append(loss.item())
-+++---+            train_loss += loss.item()
-++++--             train_loss += loss.item()
-+++ -- 
-+++---             loss.backward()
-++++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+++ --             if (batch_idx+1) % 2 == 0:
-+++ --                 optim.step()
-+++ --                 optim.zero_grad()
-++++---
-++++---            if batch_idx % report_every == report_every - 2:     
-++++--+            
-++++--+            if False:
-++++--+            #if batch_idx % report_every == report_every - 2:     
-++++--                 #Evaluation
-++++--                 model.eval()
-++++--                 with torch.no_grad():
-++++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-++++---                        X_raw = example['raw_emg'].to(device)
-++++---                        sess = example['session_ids'].to(device)
-++++---                        y = example['text_int'].to(device)
-++++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+++ -- 
-+++---+            #Report plots in tensorboard
-+++---+            if batch_idx % report_every == report_every - 2:     
-+++---+                #Evaluation
-+++---+                model.eval()
-+++---+                with torch.no_grad():
-+++---+                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-+++---+                        X_raw = example['raw_emg'].to(device)
-+++---+                        sess = example['session_ids'].to(device)
-+++---+                        y = example['text_int'].to(device)
-+++---+
-+++---+                        #Shifting target for input decoder and loss
-+++---+                        tgt= y[:,:-1]
-+++---+                        target= y[:,1:]
-+++---+
-+++---+                        #Prediction without the 197-th batch because of missing label
-+++---+                        if idx != 197:
-+++---+                            pred = model(X, X_raw, tgt, sess)
-+++---+                            #Primary Loss
-+++---+                            pred=pred.permute(0,2,1)
-+++---+                            loss = loss_fn(pred, target)
-+++---+                            eval_loss += loss.item()
-+++---+
-+++---+                #Writing on tensorboard
-+++---+                writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-+++---+                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-+++---+                train_loss= 0
-+++---+                eval_loss= 0
-+++---+
-+++---+            #Increment counter        
-+++---             batch_idx += 1
-+++----        train_loss = np.mean(losses)
-+++---+
-+++---+        #Testing and change learning rate
-+++---         val = test(model, devset, device)
-+++---+        writer.add_scalar('WER/Evaluation',val, batch_idx)
-+++---         lr_sched.step()
-+++---+    
-+++---+        #Logging
-+++---+        train_loss = np.mean(losses)
-+++---         logging.info(f'finished epoch {epoch_idx+1} - training loss: {train_loss:.4f} validation WER: {val*100:.2f}')
-+++---         torch.save(model.state_dict(), os.path.join(FLAGS.output_directory,'model.pt'))
-+++--- 
-+++---@@ -148,8 +201,9 @@ def main():
-+++---     logging.info('train / dev split: %d %d',len(trainset),len(devset))
-+++--- 
-+++---     device = 'cuda' if torch.cuda.is_available() and not FLAGS.debug else 'cpu'
-+++---+    writer = SummaryWriter(log_dir="./content/runs")
-++++--                         #Shifting target for input decoder and loss
-++++--                         tgt= y[:,:-1]
-++++--                         target= y[:,1:]
-+++ -- 
-+++----    model = train_model(trainset, devset, device)
-+++---+    model = train_model(trainset, devset ,device, writer)
-++++--+                        print(idx)
-++++--+
-++++--                         #Prediction without the 197-th batch because of missing label
-++++---                        if idx != 197:
-++++--+                        if idx != 181:
-++++--                             pred = model(X, X_raw, tgt, sess)
-++++--                             #Primary Loss
-++++--                             pred=pred.permute(0,2,1)
-++++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+++ -- 
-+++--- if __name__ == '__main__':
-+++---     FLAGS(sys.argv)
-+++---diff --git a/transformer.py b/transformer.py
-+++---index ac131be..51e1f2e 100644
-+++------ a/transformer.py
-+++---+++ b/transformer.py
-+++---@@ -145,6 +145,9 @@ class MultiHeadAttention(nn.Module):
-+++---     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-+++---     self.d_qkv = d_qkv
-++++--             #Increment counter        
-++++--             batch_idx += 1
-++++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-+++ -- 
-+++---+    #self.kdim = kdim if kdim is not None else embed_dim
-+++---+    #self.vdim = vdim if vdim is not None else embed_dim
-+++---+
-+++---     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++---     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++---     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++--         #Testing and change learning rate
-++++--         val = test(model, devset, device)
-+++ --
-+++ --['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ --output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++ --train / dev split: 8055 200
-+++ -diff --git a/recognition_model.py b/recognition_model.py
-+++--index fde5a40..6d5143b 100644
-++++-index 30c5ff2..2672d45 100644
-+++ ---- a/recognition_model.py
-+++ -+++ b/recognition_model.py
-+++--@@ -63,14 +63,14 @@ def test(model, testset, device):
-+++--     return jiwer.wer(references, predictions)
-+++-- 
-+++-- 
-+++---def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10):
-+++--+def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
-+++--     #Define Dataloader
-+++--     dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
-+++--     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
-++++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-+++ - 
-+++ -     #Define model and loss function
-+++ -     n_chars = len(devset.text_transform.chars)
-+++---    model = Model(devset.num_features, n_chars+1).to(device)
-+++--+    model = Model(devset.num_features, n_chars+1, True).to(device)
-++++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-++++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-+++ -     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-+++ - 
-+++ -     if FLAGS.start_training_from is not None:
-+++--@@ -112,17 +112,19 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+++--             target= y[:,1:]
-++++-diff --git a/transformer.py b/transformer.py
-++++-index 51e1f2e..c125841 100644
-++++---- a/transformer.py
-++++-+++ b/transformer.py
-++++-@@ -1,3 +1,4 @@
-++++-+import math
-++++- from typing import Optional
-+++ - 
-+++--             #Prediction
-+++---            pred = model(X, X_raw, tgt, sess)
-+++--+            out_enc, out_dec = model(X, X_raw, tgt, sess)
-++++- import torch
-++++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-++++-         Shape:
-++++-             see the docs in Transformer class.
-++++-         """
-++++--        src2 = self.self_attn(src, src, src)
-++++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-++++-         src = src + self.dropout1(src2)
-++++-         src = self.norm1(src)
-++++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-++++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-++++-         Shape:
-++++-             see the docs in Transformer class.
-++++-         """
-++++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-++++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-++++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-++++-         tgt = tgt + self.dropout1(tgt2)
-++++-         tgt = self.norm1(tgt)
-+++ - 
-+++--             #Primary Loss
-+++---            pred=pred.permute(0,2,1)
-+++---            loss = loss_fn(pred, target)
-+++--+            out_dec=out_dec.permute(0,2,1)
-+++--+            loss_dec = loss_fn(out_dec, target)
-++++--        tgt2=self.multihead_attn(tgt, memory, memory)
-++++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-++++-         tgt = tgt + self.dropout1(tgt2)
-++++-         tgt = self.norm1(tgt)
-+++ - 
-+++--             #Auxiliary Loss
-+++---            #pred = F.log_softmax(pred, 2)
-+++---            #pred = nn.utils.rnn.pad_sequence(decollate_tensor(pred, example['text_int_lengths']), batch_first=False) # seq first, as required by ctc
-+++---            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++---            #loss = F.ctc_loss(pred, y, example['text_int_lengths'], example['text_int_lengths'], blank=n_chars)
-+++--+            out_enc = F.log_softmax(out_enc, 2)
-+++--+            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-+++--+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-+++--+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
-+++--+
-+++--+            loss = (1 - alpha) * loss_dec + alpha * loss_enc
-+++--             losses.append(loss.item())
-+++--             train_loss += loss.item()
-++++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-++++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-++++-     self.d_qkv = d_qkv
-+++ - 
-+++--@@ -130,22 +132,25 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+++--             if (batch_idx+1) % 2 == 0:
-+++--                 optim.step()
-+++--                 optim.zero_grad()
-++++--    #self.kdim = kdim if kdim is not None else embed_dim
-++++--    #self.vdim = vdim if vdim is not None else embed_dim
-+++ --
-+++---            if batch_idx % report_every == report_every - 2:     
-+++--+            
-+++--+            if False:
-+++--+            #if batch_idx % report_every == report_every - 2:     
-+++--                 #Evaluation
-+++--                 model.eval()
-+++--                 with torch.no_grad():
-+++--                     for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-+++---                        X_raw = example['raw_emg'].to(device)
-+++---                        sess = example['session_ids'].to(device)
-+++---                        y = example['text_int'].to(device)
-+++--+                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+++--+                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+++--+                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-++++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-++++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-++++-     else:
-++++-         self.relative_positional = None
-+++ - 
-+++--                         #Shifting target for input decoder and loss
-+++--                         tgt= y[:,:-1]
-+++--                         target= y[:,1:]
-++++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-++++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-++++-     """Runs the multi-head self-attention layer.
-+++ - 
-+++--+                        print(idx)
-+++--+
-+++--                         #Prediction without the 197-th batch because of missing label
-+++---                        if idx != 197:
-+++--+                        if idx != 181:
-+++--                             pred = model(X, X_raw, tgt, sess)
-+++--                             #Primary Loss
-+++--                             pred=pred.permute(0,2,1)
-+++--@@ -160,6 +165,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=10)
-+++-- 
-+++--             #Increment counter        
-+++--             batch_idx += 1
-+++--+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-++++-     Args:
-++++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-++++-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-++++-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+++ - 
-+++--         #Testing and change learning rate
-+++--         val = test(model, devset, device)
-++++-+    if attn_mask is not None:
-++++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++++-+
-++++-     if self.relative_positional is not None:
-++++-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-++++-         l,b,h,d = q_pos.size()
-++++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-++++-             x = x.transpose(0, 1)
-++++-             x = x.contiguous().view(bsz_heads, length+1, length)
-++++-             return x[:, 1:, :]
-++++-+        
-++++-+
-++++-+########
-++++-+# Taken from:
-++++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-++++-+# or also here:
-++++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-++++-+class PositionalEncoding(nn.Module):
-++++-+
-++++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-++++-+        super(PositionalEncoding, self).__init__()
-++++-+        self.dropout = nn.Dropout(p=dropout)
-++++-+        self.max_len = max_len
-++++-+
-++++-+        pe = torch.zeros(max_len, d_model)
-++++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-++++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-++++-+                             * (-math.log(10000.0) / d_model))
-++++-+        pe[:, 0::2] = torch.sin(position * div_term)
-++++-+        pe[:, 1::2] = torch.cos(position * div_term)
-++++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-++++-+        self.register_buffer('pe', pe)  # Will not be trained.
-++++-+
-++++-+    def forward(self, x):
-++++-+        """Inputs of forward function
-++++-+        Args:
-++++-+            x: the sequence fed to the positional encoder model (required).
-++++-+        Shape:
-++++-+            x: [sequence length, batch size, embed dim]
-++++-+            output: [sequence length, batch size, embed dim]
-++++-+        """
-++++-+        assert x.size(0) < self.max_len, (
-++++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-++++-+        # shape of x (len, B, dim)
-++++-+        x = x + self.pe[:x.size(0), :]
-++++-+        return self.dropout(x)
-+++ -
-+++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ -output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++ -train / dev split: 8055 200
-+++ diff --git a/recognition_model.py b/recognition_model.py
-+++-index 30c5ff2..2672d45 100644
-++++index 2672d45..d268f26 100644
-+++ --- a/recognition_model.py
-+++ +++ b/recognition_model.py
-+++-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-+++- 
-+++-     #Define model and loss function
-+++-     n_chars = len(devset.text_transform.chars)
-+++--    model = Model(devset.num_features, n_chars+1, True).to(device)
-+++-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-+++-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-++++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-++++             X = combine_fixed_length(example['emg'], 200).to(device)
-++++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+++++            y = combine_fixed_length(example['text_int'], 200).to(device)
-+++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+++  
-+++-     if FLAGS.start_training_from is not None:
-++++             #Shifting target for input decoder and loss
-++++             tgt= y[:,:-1]
-+++ diff --git a/transformer.py b/transformer.py
-+++-index 51e1f2e..c125841 100644
-++++index c125841..73d805b 100644
-+++ --- a/transformer.py
-+++ +++ b/transformer.py
-+++-@@ -1,3 +1,4 @@
-+++-+import math
-+++- from typing import Optional
-+++- 
-+++- import torch
-+++-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-+++-         Shape:
-+++-             see the docs in Transformer class.
-+++-         """
-+++--        src2 = self.self_attn(src, src, src)
-+++-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-+++-         src = src + self.dropout1(src2)
-+++-         src = self.norm1(src)
-+++-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-+++-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-++++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
-+++          Shape:
-+++              see the docs in Transformer class.
-+++          """
-+++--        tgt2 = self.self_attn(tgt, tgt, tgt)
-+++-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-+++-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-++++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-++++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-+++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-+++++        tgt2 = self.self_attn(tgt, tgt, tgt)
-+++          tgt = tgt + self.dropout1(tgt2)
-+++          tgt = self.norm1(tgt)
-+++  
-+++--        tgt2=self.multihead_attn(tgt, memory, memory)
-+++-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-+++-         tgt = tgt + self.dropout1(tgt2)
-+++-         tgt = self.norm1(tgt)
-+++- 
-+++-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-+++-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-+++-     self.d_qkv = d_qkv
-+++- 
-+++--    #self.kdim = kdim if kdim is not None else embed_dim
-+++--    #self.vdim = vdim if vdim is not None else embed_dim
-+++--
-+++-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+++-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-+++-     else:
-+++-         self.relative_positional = None
-+++- 
-+++--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-+++-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-+++-     """Runs the multi-head self-attention layer.
-+++- 
-+++-     Args:
-+++-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-++++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
-+++      v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-+++      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+++  
-+++-+    if attn_mask is not None:
-+++-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+++-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+++-+
-++++-    if attn_mask is not None:
-++++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+++++   # if attn_mask is not None:
-+++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++++ 
-+++      if self.relative_positional is not None:
-+++          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-+++-         l,b,h,d = q_pos.size()
-+++-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-+++-             x = x.transpose(0, 1)
-+++-             x = x.contiguous().view(bsz_heads, length+1, length)
-+++-             return x[:, 1:, :]
-+++-+        
-+++-+
-+++-+########
-+++-+# Taken from:
-+++-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-+++-+# or also here:
-+++-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-+++-+class PositionalEncoding(nn.Module):
-+++-+
-+++-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-+++-+        super(PositionalEncoding, self).__init__()
-+++-+        self.dropout = nn.Dropout(p=dropout)
-+++-+        self.max_len = max_len
-+++-+
-+++-+        pe = torch.zeros(max_len, d_model)
-+++-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-+++-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-+++-+                             * (-math.log(10000.0) / d_model))
-+++-+        pe[:, 0::2] = torch.sin(position * div_term)
-+++-+        pe[:, 1::2] = torch.cos(position * div_term)
-+++-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-+++-+        self.register_buffer('pe', pe)  # Will not be trained.
-+++-+
-+++-+    def forward(self, x):
-+++-+        """Inputs of forward function
-+++-+        Args:
-+++-+            x: the sequence fed to the positional encoder model (required).
-+++-+        Shape:
-+++-+            x: [sequence length, batch size, embed dim]
-+++-+            output: [sequence length, batch size, embed dim]
-+++-+        """
-+++-+        assert x.size(0) < self.max_len, (
-+++-+            f"Too long sequence length: increase `max_len` of pos encoding")
-+++-+        # shape of x (len, B, dim)
-+++-+        x = x + self.pe[:x.size(0), :]
-+++-+        return self.dropout(x)
-+++ 
-+++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+++ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-++ diff --git a/output/log.txt b/output/log.txt
-++-index ae42364..8563980 100644
-+++index 1d2cd8e..342fccd 100644
-++ --- a/output/log.txt
-++ +++ b/output/log.txt
-++-@@ -1,3 +1,2 @@
-++-+57f8139449dd9286c2203ec2eca118a550638a7c
-+++@@ -1,13 +1,2 @@
-+++-57f8139449dd9286c2203ec2eca118a550638a7c
-++++be71135adc89793578f304adb405cea80a5b2b9a
-++  
-+++-diff --git a/output/log.txt b/output/log.txt
-+++-index ae42364..8563980 100644
-+++---- a/output/log.txt
-+++-+++ b/output/log.txt
-+++-@@ -1,3 +1,2 @@
-+++-+57f8139449dd9286c2203ec2eca118a550638a7c
-+++- 
-+++--
-+++--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-++ -
-++ -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+++diff --git a/recognition_model.py b/recognition_model.py
-+++index 2672d45..d268f26 100644
-+++--- a/recognition_model.py
-++++++ b/recognition_model.py
-+++@@ -105,7 +105,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-+++             X = combine_fixed_length(example['emg'], 200).to(device)
-+++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+++-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-++++            y = combine_fixed_length(example['text_int'], 200).to(device)
-++++            #y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+++ 
-+++             #Shifting target for input decoder and loss
-+++             tgt= y[:,:-1]
-+++diff --git a/transformer.py b/transformer.py
-+++index c125841..73d805b 100644
-+++--- a/transformer.py
-++++++ b/transformer.py
-+++@@ -123,8 +123,8 @@ class TransformerDecoderLayer(nn.Module):
-+++         Shape:
-+++             see the docs in Transformer class.
-+++         """
-+++-        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-+++-        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-++++      # self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-++++        tgt2 = self.self_attn(tgt, tgt, tgt)
-+++         tgt = tgt + self.dropout1(tgt2)
-+++         tgt = self.norm1(tgt)
-+++ 
-+++@@ -177,9 +177,9 @@ class MultiHeadAttention(nn.Module):
-+++     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-+++     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+++ 
-+++-    if attn_mask is not None:
-+++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++++   # if attn_mask is not None:
-++++      #  attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++++       # logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+++ 
-+++     if self.relative_positional is not None:
-+++         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-++ 
-++ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+++output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+++train / dev split: 8055 200
-+ diff --git a/recognition_model.py b/recognition_model.py
-+-index 30c5ff2..2672d45 100644
-++index 2672d45..517c9a3 100644
-+ --- a/recognition_model.py
-+ +++ b/recognition_model.py
-+-@@ -70,7 +70,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-++@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-++             X = combine_fixed_length(example['emg'], 200).to(device)
-++             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-++             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-+++            y = combine_fixed_length(example['text_int'], 200).to(device)
-++             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+  
-+-     #Define model and loss function
-+-     n_chars = len(devset.text_transform.chars)
-+--    model = Model(devset.num_features, n_chars+1, True).to(device)
-+-+    model = Model(devset.num_features, n_chars+1, device, True).to(device)
-+-     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
-+- 
-+-     if FLAGS.start_training_from is not None:
-++             #Shifting target for input decoder and loss
-+ diff --git a/transformer.py b/transformer.py
-+-index 51e1f2e..c125841 100644
-++index c125841..06e870b 100644
-+ --- a/transformer.py
-+ +++ b/transformer.py
-+-@@ -1,3 +1,4 @@
-+-+import math
-+- from typing import Optional
-+- 
-+- import torch
-+-@@ -51,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-+-         Shape:
-+-             see the docs in Transformer class.
-+-         """
-+--        src2 = self.self_attn(src, src, src)
-+-+        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-+-         src = src + self.dropout1(src2)
-+-         src = self.norm1(src)
-+-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-+-@@ -122,11 +123,12 @@ class TransformerDecoderLayer(nn.Module):
-+-         Shape:
-+-             see the docs in Transformer class.
-+-         """
-+--        tgt2 = self.self_attn(tgt, tgt, tgt)
-+-+        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
-+-+        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-+-         tgt = tgt + self.dropout1(tgt2)
-+-         tgt = self.norm1(tgt)
-+- 
-+--        tgt2=self.multihead_attn(tgt, memory, memory)
-+-+        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-+-         tgt = tgt + self.dropout1(tgt2)
-+-         tgt = self.norm1(tgt)
-+- 
-+-@@ -145,9 +147,6 @@ class MultiHeadAttention(nn.Module):
-+-     assert d_qkv * n_head == d_model, 'd_model must be divisible by n_head'
-+-     self.d_qkv = d_qkv
-+- 
-+--    #self.kdim = kdim if kdim is not None else embed_dim
-+--    #self.vdim = vdim if vdim is not None else embed_dim
-+--
-+-     self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+-     self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+-     self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
-+-@@ -164,7 +163,7 @@ class MultiHeadAttention(nn.Module):
-+-     else:
-+-         self.relative_positional = None
-+- 
-+--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):
-+-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-+-     """Runs the multi-head self-attention layer.
-+- 
-+-     Args:
-+-@@ -178,6 +177,10 @@ class MultiHeadAttention(nn.Module):
-+-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-++@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
-+      logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+  
-+-+    if attn_mask is not None:
-+-+        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+-+        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+-+
-++     if attn_mask is not None:
-++-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++ 
-+      if self.relative_positional is not None:
-+          q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-+-         l,b,h,d = q_pos.size()
-+-@@ -383,3 +386,39 @@ class LearnedRelativePositionalEmbedding(nn.Module):
-+-             x = x.transpose(0, 1)
-+-             x = x.contiguous().view(bsz_heads, length+1, length)
-+-             return x[:, 1:, :]
-+-+        
-+-+
-+-+########
-+-+# Taken from:
-+-+# https://pytorch.org/tutorials/beginner/transformer_tutorial.html
-+-+# or also here:
-+-+# https://github.com/pytorch/examples/blob/master/word_language_model/model.py
-+-+class PositionalEncoding(nn.Module):
-+-+
-+-+    def __init__(self, d_model, dropout=0.0, max_len=5000):
-+-+        super(PositionalEncoding, self).__init__()
-+-+        self.dropout = nn.Dropout(p=dropout)
-+-+        self.max_len = max_len
-+-+
-+-+        pe = torch.zeros(max_len, d_model)
-+-+        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-+-+        div_term = torch.exp(torch.arange(0, d_model, 2).float()
-+-+                             * (-math.log(10000.0) / d_model))
-+-+        pe[:, 0::2] = torch.sin(position * div_term)
-+-+        pe[:, 1::2] = torch.cos(position * div_term)
-+-+        pe = pe.unsqueeze(0).transpose(0, 1)  # shape (max_len, 1, dim)
-+-+        self.register_buffer('pe', pe)  # Will not be trained.
-+-+
-+-+    def forward(self, x):
-+-+        """Inputs of forward function
-+-+        Args:
-+-+            x: the sequence fed to the positional encoder model (required).
-+-+        Shape:
-+-+            x: [sequence length, batch size, embed dim]
-+-+            output: [sequence length, batch size, embed dim]
-+-+        """
-+-+        assert x.size(0) < self.max_len, (
-+-+            f"Too long sequence length: increase `max_len` of pos encoding")
-+-+        # shape of x (len, B, dim)
-+-+        x = x + self.pe[:x.size(0), :]
-+-+        return self.dropout(x)
-+ 
-+ ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-+ output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
- diff --git a/output/log.txt b/output/log.txt
--index ae42364..8563980 100644
-+index 1d2cd8e..342fccd 100644
- --- a/output/log.txt
- +++ b/output/log.txt
--@@ -1,3 +1,2 @@
--+57f8139449dd9286c2203ec2eca118a550638a7c
-+@@ -1,13 +1,2 @@
-+-57f8139449dd9286c2203ec2eca118a550638a7c
-++be71135adc89793578f304adb405cea80a5b2b9a
-  
-+-diff --git a/output/log.txt b/output/log.txt
-+-index ae42364..8563980 100644
-+---- a/output/log.txt
-+-+++ b/output/log.txt
-+-@@ -1,3 +1,2 @@
-+-+57f8139449dd9286c2203ec2eca118a550638a7c
-+- 
-+--
-+--['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
- -
- -['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+diff --git a/recognition_model.py b/recognition_model.py
-+index 2672d45..517c9a3 100644
-+--- a/recognition_model.py
-++++ b/recognition_model.py
-+@@ -105,6 +105,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-+             X = combine_fixed_length(example['emg'], 200).to(device)
-+             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-+             sess = combine_fixed_length(example['session_ids'], 200).to(device)
-++            y = combine_fixed_length(example['text_int'], 200).to(device)
-+             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+ 
-+             #Shifting target for input decoder and loss
-+diff --git a/transformer.py b/transformer.py
-+index c125841..06e870b 100644
-+--- a/transformer.py
-++++ b/transformer.py
-+@@ -178,8 +178,8 @@ class MultiHeadAttention(nn.Module):
-+     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
-+ 
-+     if attn_mask is not None:
-+-        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-+-        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-++       attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
-++       logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+ 
-+     if self.relative_positional is not None:
-+         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
- 
- ['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py']
-+output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-+train / dev split: 8055 200
-diff --git a/recognition_model.py b/recognition_model.py
-index 2672d45..506241a 100644
---- a/recognition_model.py
-+++ b/recognition_model.py
-@@ -39,23 +39,28 @@ def test(model, testset, device):
-     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
-     references = []
-     predictions = []
-+    batch_idx = 0
-     with torch.no_grad():
-         for example in dataloader:
--            X = example['emg'].to(device)
-             X_raw = example['raw_emg'].to(device)
--            sess = example['session_ids'].to(device)
-+            tgt = example['text_int'].to(device)
- 
--            pred  = F.log_softmax(model(X, X_raw, sess), -1)
-+            #Prediction without the 197-th batch because of missing label
-+            if batch_idx != 181:
-+                out_enc, out_dec = model(X_raw, tgt)
-+                pred  = F.log_softmax(out_dec, -1)
- 
--            beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
--            pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
-+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
-+                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
- 
--            pred_text = testset.text_transform.int_to_text(pred_int)
--            target_text = testset.text_transform.clean_text(example['text'][0])
-+                pred_text = testset.text_transform.int_to_text(pred_int)
-+                target_text = testset.text_transform.clean_text(example['text'][0])
- 
--            references.append(target_text)
--            predictions.append(pred_text)
-+                references.append(target_text)
-+                predictions.append(pred_text)
- 
-+        batch_idx += 1
-+        
-     model.train()
-     #remove empty strings because I had an error in the calculation of WER function
-     predictions = [predictions[i] for i in range(len(predictions)) if len(references[i]) > 0]
-@@ -102,17 +107,17 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-             schedule_lr(batch_idx)
- 
-             #Preprosessing of the input and target for the model
--            X = combine_fixed_length(example['emg'], 200).to(device)
-             X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
--            sess = combine_fixed_length(example['session_ids'], 200).to(device)
-             y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-+            #X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
-+            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
- 
-             #Shifting target for input decoder and loss
-             tgt= y[:,:-1]
-             target= y[:,1:]
- 
-             #Prediction
--            out_enc, out_dec = model(X, X_raw, tgt, sess)
-+            out_enc, out_dec = model(X_raw, tgt)
- 
-             #Primary Loss
-             out_dec=out_dec.permute(0,2,1)
-@@ -168,6 +173,7 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
-             #Increment counter        
-             batch_idx += 1
-             writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-+            val = test(model, devset, device)
- 
-         #Testing and change learning rate
-         val = test(model, devset, device)
-diff --git a/transformer.py b/transformer.py
-index c125841..0a305a4 100644
---- a/transformer.py
-+++ b/transformer.py
-@@ -52,7 +52,7 @@ class TransformerEncoderLayer(nn.Module):
-         Shape:
-             see the docs in Transformer class.
-         """
--        src2 = self.self_attn(src, src, src, src_key_padding_mask)
-+        src2 = self.self_attn(src, src, src)
-         src = src + self.dropout1(src2)
-         src = self.norm1(src)
-         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
-@@ -123,12 +123,11 @@ class TransformerDecoderLayer(nn.Module):
-         Shape:
-             see the docs in Transformer class.
-         """
--        self_att_mask = torch.logical_or(tgt_mask, tgt_key_padding_mask) 
--        tgt2 = self.self_attn(tgt, tgt, tgt, self_att_mask)
-+        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_key_padding_mask, tgt_mask)
-         tgt = tgt + self.dropout1(tgt2)
-         tgt = self.norm1(tgt)
- 
--        tgt2=self.multihead_attn(tgt, memory, memory, memory_key_padding_mask)
-+        tgt2=self.multihead_attn(tgt, memory, memory)
-         tgt = tgt + self.dropout1(tgt2)
-         tgt = self.norm1(tgt)
- 
-@@ -163,7 +162,7 @@ class MultiHeadAttention(nn.Module):
-     else:
-         self.relative_positional = None
- 
--  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
-+  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None, attn_mask: Optional[torch.Tensor] = None):
-     """Runs the multi-head self-attention layer.
- 
-     Args:
-@@ -171,15 +170,39 @@ class MultiHeadAttention(nn.Module):
-     Returns:
-       A single tensor containing the output from this layer
-     """
-+    # Apply mask to the keys if provided
-+    if attn_mask is not None:
-+        attn_mask = attn_mask.unsqueeze(2)
-+        attn_mask = attn_mask.unsqueeze(3)
-+        key = key.masked_fill(attn_mask == float('-inf'), float('-inf'))[0,:,:,:]
-+    
-+    # Apply mask to the query if provided
-+    if key_padding_mask is not None:
-+        query = query.float().masked_fill(
-+            key_padding_mask, float('-inf')).type_as(query)  
-+    
-+    # Apply mask to the values if provided
-+    if key_padding_mask is not None:
-+        value = value.float().masked_fill(
-+            key_padding_mask, float(0)).type_as(value) 
- 
-+    #Computes projections
-     q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
-     k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
-     v = torch.einsum('tbf,hfa->bhta', value, self.w_v)
-+     
-+    # Compute scaled dot-product attention
-     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
- 
-+    
-+    # Apply mask to the attention weights if provided
-     if attn_mask is not None:
--        attn_mask = torch.unsqueeze(attn_mask, 1)  # Shape: (batch_size, 1, tgt_seq_len)
--        logits = logits.masked_fill(attn_mask == 0, float('-inf'))
-+        attn_mask = attn_mask.squeeze(2)       
-+        attn_mask = attn_mask.squeeze(2)
-+        attn_mask = attn_mask.unsqueeze(0)
-+        attn_mask = attn_mask.unsqueeze(1)
-+        logits = logits.masked_fill(attn_mask == float('-inf'), float('-inf'))
-+    
- 
-     if self.relative_positional is not None:
-         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
-
-['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
-output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
-train / dev split: 8055 200
diff --git a/read_emg.py b/read_emg.py
index 664aa94..27787d5 100644
--- a/read_emg.py
+++ b/read_emg.py
@@ -72,8 +72,8 @@ def load_utterance(base_dir, index, limit_length=False, debug=False, text_align_
     x = apply_to_all(notch_harmonics, x, 60, 1000)
     x = apply_to_all(remove_drift, x, 1000)
     x = x[raw_emg_before.shape[0]:x.shape[0]-raw_emg_after.shape[0],:]
-    emg_orig = apply_to_all(subsample, x, 689.06, 1000)
-    x = apply_to_all(subsample, x, 516.79, 1000)
+    emg_orig = apply_to_all(subsample, x, 400.00, 1000)
+    x = apply_to_all(subsample, x, 300, 1000)
     emg = x
 
     for c in FLAGS.remove_channels:
@@ -240,6 +240,7 @@ class EMGDataset(torch.utils.data.Dataset):
         session_ids = np.full(emg.shape[0], directory_info.session_index, dtype=np.int64)
         audio_file = f'{directory_info.directory}/{idx}_audio_clean.flac'
 
+        #self.text_transform.add_new_words(text)
         text_int = np.array(self.text_transform.text_to_int(text), dtype=np.int64)
 
         result = {'audio_features':torch.from_numpy(mfccs).pin_memory(), 'emg':torch.from_numpy(emg).pin_memory(), 'text':text, 'text_int': torch.from_numpy(text_int).pin_memory(), 'file_label':idx, 'session_ids':torch.from_numpy(session_ids).pin_memory(), 'book_location':book_location, 'silent':directory_info.silent, 'raw_emg':torch.from_numpy(raw_emg).pin_memory()}
diff --git a/recognition_model.py b/recognition_model.py
index 506241a..547fde4 100644
--- a/recognition_model.py
+++ b/recognition_model.py
@@ -3,7 +3,7 @@ import sys
 import numpy as np
 import logging
 import subprocess
-from ctcdecode import CTCBeamDecoder
+from ctcdecode import OnlineCTCBeamDecoder, DecoderState
 import jiwer
 import random
 from torch.utils.tensorboard import SummaryWriter
@@ -14,8 +14,7 @@ import torch.nn.functional as F
 
 from read_emg import EMGDataset, SizeAwareSampler
 from architecture import Model
-from data_utils import combine_fixed_length, decollate_tensor, combine_fixed_length_tgt
-from transformer import TransformerEncoderLayer
+from data_utils import combine_fixed_length, decollate_tensor
 
 from absl import flags
 FLAGS = flags.FLAGS
@@ -32,32 +31,32 @@ flags.DEFINE_string('evaluate_saved', None, 'run evaluation on given model file'
 def test(model, testset, device):
     model.eval()
 
-    blank_id = len(testset.text_transform.chars)
-    decoder = CTCBeamDecoder(testset.text_transform.chars+'_', blank_id=blank_id, log_probs_input=True,
+    blank_id = 1
+    decoder = OnlineCTCBeamDecoder(testset.text_transform.chars, blank_id=blank_id, log_probs_input=True,
             model_path='lm.binary', alpha=1.5, beta=1.85)
-
+    state = DecoderState(decoder)
     dataloader = torch.utils.data.DataLoader(testset, batch_size=1)
     references = []
     predictions = []
     batch_idx = 0
     with torch.no_grad():
         for example in dataloader:
-            X_raw = example['raw_emg'].to(device)
-            tgt = example['text_int'].to(device)
+            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
+            tgt = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
 
             #Prediction without the 197-th batch because of missing label
             if batch_idx != 181:
                 out_enc, out_dec = model(X_raw, tgt)
                 pred  = F.log_softmax(out_dec, -1)
 
-                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred)
-                pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
+                beam_results, beam_scores, timesteps, out_lens = decoder.decode(pred, [state], [False])
+                #pred_int = beam_results[0,0,:out_lens[0,0]].tolist()
 
-                pred_text = testset.text_transform.int_to_text(pred_int)
-                target_text = testset.text_transform.clean_text(example['text'][0])
+                #pred_text = testset.text_transform.int_to_text(pred_int)
+                #target_text = testset.text_transform.clean_text(example['text'][0])
 
-                references.append(target_text)
-                predictions.append(pred_text)
+                #references.append(target_text)
+                #predictions.append(pred_text)
 
         batch_idx += 1
         
@@ -70,12 +69,12 @@ def test(model, testset, device):
 
 def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1, alpha=0.7):
     #Define Dataloader
-    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_sampler=SizeAwareSampler(trainset, 128000))
+    dataloader_training = torch.utils.data.DataLoader(trainset, pin_memory=(device=='cuda'), num_workers=0, collate_fn=EMGDataset.collate_raw, batch_size=2)
     dataloader_evaluation = torch.utils.data.DataLoader(devset, batch_size=1)
 
     #Define model and loss function
     n_chars = len(devset.text_transform.chars)
-    model = Model(devset.num_features, n_chars+1, device, True).to(device)
+    model = Model(devset.num_features, n_chars, device, True).to(device)
     loss_fn=nn.CrossEntropyLoss(ignore_index=0)
 
     if FLAGS.start_training_from is not None:
@@ -107,10 +106,8 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
             schedule_lr(batch_idx)
 
             #Preprosessing of the input and target for the model
-            X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-            y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-            #X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
-            #y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
+            X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
+            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
 
             #Shifting target for input decoder and loss
             tgt= y[:,:-1]
@@ -119,15 +116,14 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
             #Prediction
             out_enc, out_dec = model(X_raw, tgt)
 
-            #Primary Loss
+            #Decoder Loss
             out_dec=out_dec.permute(0,2,1)
             loss_dec = loss_fn(out_dec, target)
 
-            #Auxiliary Loss
+            #Encoder Loss
             out_enc = F.log_softmax(out_enc, 2)
-            out_enc = nn.utils.rnn.pad_sequence(decollate_tensor(out_enc, example['lengths']), batch_first=False) # seq first, as required by ctc
-            y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
-            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank=n_chars)
+            out_enc = out_enc.transpose(1,0)
+            loss_enc = F.ctc_loss(out_enc, y, example['lengths'], example['text_int_lengths'], blank = 1) # 1 is the blank token according to TextTransform
 
             #Combination the two losses
             loss = (1 - alpha) * loss_dec + alpha * loss_enc
@@ -140,41 +136,43 @@ def train_model(trainset, devset, device, writer, n_epochs=200, report_every=1,
                 optim.step()
                 optim.zero_grad()
             
-            if False:
-            #if batch_idx % report_every == report_every - 2:     
+            
+            #Increment counter and print the loss training       
+            batch_idx += 1
+            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
+            train_loss= 0
+
+
+            #Debug
+            val = test(model, devset, device)
+
+            if batch_idx % report_every == 0:     
                 #Evaluation
                 model.eval()
                 with torch.no_grad():
-                    for example, idx in zip(dataloader_evaluation, range(len(dataloader_evaluation))):
-                        X_raw = combine_fixed_length(example['raw_emg'], 200*8).to(device)
-                        sess = combine_fixed_length(example['session_ids'], 200).to(device)
-                        y = combine_fixed_length_tgt(example['text_int'], X_raw.shape[0]).to(device)
-
+                    for idx, example in enumerate(dataloader_evaluation):
+                        X_raw = nn.utils.rnn.pad_sequence(example['raw_emg'], batch_first=True).to(device)
+                        y = nn.utils.rnn.pad_sequence(example['text_int'], batch_first=True).to(device)
+                    
                         #Shifting target for input decoder and loss
                         tgt= y[:,:-1]
                         target= y[:,1:]
 
-                        print(idx)
-
                         #Prediction without the 197-th batch because of missing label
-                        if idx != 181:
-                            pred = model(X, X_raw, tgt, sess)
-                            #Primary Loss
-                            pred=pred.permute(0,2,1)
-                            loss = loss_fn(pred, target)
-                            eval_loss += loss.item()
+                        out_enc, out_dec = model(X_raw, tgt)
+                        #Decoder Loss
+                        out_dec=out_dec.permute(0,2,1)
+                        loss = loss_fn(out_dec, target)
+                        eval_loss += loss.item()
+                        
+                        #just for now
+                        if idx == 10:
+                            break
 
                 #Writing on tensorboard
                 writer.add_scalar('Loss/Evaluation', eval_loss / batch_idx, batch_idx)
-                writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx) 
-                train_loss= 0
                 eval_loss= 0
 
-            #Increment counter        
-            batch_idx += 1
-            writer.add_scalar('Loss/Training', train_loss / batch_idx, batch_idx)
-            val = test(model, devset, device)
-
         #Testing and change learning rate
         val = test(model, devset, device)
         writer.add_scalar('WER/Evaluation',val, batch_idx)
diff --git a/transformer.py b/transformer.py
index 0a305a4..47a7ec7 100644
--- a/transformer.py
+++ b/transformer.py
@@ -170,22 +170,6 @@ class MultiHeadAttention(nn.Module):
     Returns:
       A single tensor containing the output from this layer
     """
-    # Apply mask to the keys if provided
-    if attn_mask is not None:
-        attn_mask = attn_mask.unsqueeze(2)
-        attn_mask = attn_mask.unsqueeze(3)
-        key = key.masked_fill(attn_mask == float('-inf'), float('-inf'))[0,:,:,:]
-    
-    # Apply mask to the query if provided
-    if key_padding_mask is not None:
-        query = query.float().masked_fill(
-            key_padding_mask, float('-inf')).type_as(query)  
-    
-    # Apply mask to the values if provided
-    if key_padding_mask is not None:
-        value = value.float().masked_fill(
-            key_padding_mask, float(0)).type_as(value) 
-
     #Computes projections
     q = torch.einsum('tbf,hfa->bhta', query, self.w_q)
     k = torch.einsum('tbf,hfa->bhta', key, self.w_k)
@@ -194,16 +178,18 @@ class MultiHeadAttention(nn.Module):
     # Compute scaled dot-product attention
     logits = torch.einsum('bhqa,bhka->bhqk', q, k) / (self.d_qkv ** 0.5)
 
-    
-    # Apply mask to the attention weights if provided
+    # Apply att_mask to the attention weights if provided
     if attn_mask is not None:
-        attn_mask = attn_mask.squeeze(2)       
-        attn_mask = attn_mask.squeeze(2)
-        attn_mask = attn_mask.unsqueeze(0)
-        attn_mask = attn_mask.unsqueeze(1)
+        attn_mask=attn_mask.unsqueeze(0)
+        attn_mask=attn_mask.unsqueeze(1)
         logits = logits.masked_fill(attn_mask == float('-inf'), float('-inf'))
     
-
+    
+    # Apply padding_mask to the attention weights if provided
+    if key_padding_mask is not None:
+        logits = logits.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
+    
+    
     if self.relative_positional is not None:
         q_pos = q.permute(2,0,1,3) #bhqd->qbhd
         l,b,h,d = q_pos.size()

['/home/christian/storage/EMG-christian/silent_speech-main/recognition_model.py', '--output_directory', './models/recognition_model/']
output example: (./emg_data/silent_parallel_data/5-6_silent, 208)
train / dev split: 8055 200
